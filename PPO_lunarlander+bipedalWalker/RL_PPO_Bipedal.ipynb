{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1iddZ6AWmNK4X0z-caI1cxEdDa47VFjbh","authorship_tag":"ABX9TyNIdAbqhLOjaB1vZ5hU3/h3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install stable_baselines3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdVfkug-xKjR","executionInfo":{"status":"ok","timestamp":1740510853731,"user_tz":-120,"elapsed":3952,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}},"outputId":"8117839f-933a-48b6-bca0-93d56a48200d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting stable_baselines3\n","  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n","Collecting gymnasium<1.1.0,>=0.29.1 (from stable_baselines3)\n","  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.26.4)\n","Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.5.1+cu121)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (4.12.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium<1.1.0,>=0.29.1->stable_baselines3)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.17.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.1.105)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0,>=2.3->stable_baselines3) (12.8.61)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.55.6)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n","Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium, stable_baselines3\n","Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 stable_baselines3-2.5.0\n"]}]},{"cell_type":"code","source":["!pip install box2d"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QBcvAg-9xPgq","executionInfo":{"status":"ok","timestamp":1740510865654,"user_tz":-120,"elapsed":3174,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}},"outputId":"5de206d9-2f69-4020-b79c-ed2d2f74f3a1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d\n","  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n","Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/3.7 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/3.7 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: box2d\n","Successfully installed box2d-2.3.10\n"]}]},{"cell_type":"code","source":["def evaluate_model(model, env, n_eval_episodes=10):\n","    \"\"\"Evaluate the model on a given environment and return mean and std rewards.\"\"\"\n","    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes)\n","    return mean_reward, std_reward"],"metadata":{"id":"TpebfblMMKj7","executionInfo":{"status":"ok","timestamp":1740510874365,"user_tz":-120,"elapsed":22,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import stable_baselines3\n","import os\n","import gym\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.evaluation import evaluate_policy"],"metadata":{"id":"LBPWItmlxVlu","executionInfo":{"status":"ok","timestamp":1740510904391,"user_tz":-120,"elapsed":9444,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["#Perfect Model"],"metadata":{"id":"4_CKvO8FyIO8"}},{"cell_type":"code","execution_count":21,"metadata":{"id":"roo9jdAIwmba","executionInfo":{"status":"ok","timestamp":1740519823499,"user_tz":-120,"elapsed":43,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}}},"outputs":[],"source":["def train_bipedalwalker():\n","    # Create environment and model for BipedalWalker\n","    env_bipedal = make_vec_env(\"BipedalWalker-v3\", n_envs=1)\n","    model_b = PPO(\"MlpPolicy\", env_bipedal, verbose=1, tensorboard_log=\"/content/drive/MyDrive/RL_models/ppo_bipedalwalker/\",\n","                n_steps=2048, batch_size=64, gae_lambda=0.95, gamma=0.999,\n","                n_epochs=10, ent_coef=0.0, learning_rate=3e-4, clip_range=0.18)\n","\n","    eval_freq = 10000  # Evaluate every 10,000 steps\n","    n_eval_episodes = 10  # Number of episodes per evaluation\n","\n","    for step in range(1, 200001, eval_freq):\n","        # Train the model\n","        model_b.learn(total_timesteps=eval_freq, reset_num_timesteps=False)\n","\n","        # Evaluate the model on BipedalWalker\n","        mean_reward, std_reward = evaluate_model(model_b, env_bipedal, n_eval_episodes)\n","\n","        # Print evaluation results for BipedalWalker\n","        print(f\"BipedalWalker - Step: {step} | Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n","\n","    # Save the trained model for BipedalWalker\n","    model_b.save(\"ppo_bipedalwalker\")"]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    train_bipedalwalker()"],"metadata":{"id":"BNKE7WVGxUyg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gym\n","import imageio\n","import numpy as np\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.env_util import make_vec_env\n","from IPython.display import HTML\n","from base64 import b64encode\n","\n","\n","def record_video(model_path, video_path=\"bipedal_walker.mp4\", n_episodes=1):\n","    env = gym.make(\"BipedalWalker-v3\")  # Create a fresh environment (NO NORMALIZATION)\n","\n","    model = PPO.load(model_path)  # Load trained model\n","    images = []\n","\n","    for _ in range(n_episodes):\n","        obs = env.reset()\n","        done = False\n","        while not done:\n","            action, _ = model.predict(obs, deterministic=True)\n","            obs, reward, done, _ = env.step(action)\n","            frame = env.render(mode=\"rgb_array\")  # Capture frame\n","            images.append(frame)\n","\n","    env.close()\n","    imageio.mimsave(video_path, images, fps=30)  # Save as video\n","\n","\n","def show_video(video_path):\n","    video_file = open(video_path, \"rb\").read()\n","    video_encoded = b64encode(video_file).decode(\"ascii\")\n","    return HTML(f'<video width=\"640\" height=\"480\" controls><source src=\"data:video/mp4;base64,{video_encoded}\" type=\"video/mp4\"></video>')\n","\n","# Run the recording after training\n","record_video(\"ppo_bipedalwalker.zip\")\n","show_video(\"bipedal_walker.mp4\")\n"],"metadata":{"id":"gJTkH7Ulzrk7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. PPO Bipedal\n","env = 1, batch size= 64, lr = 1e-4"],"metadata":{"id":"eH6k8GzjzQqN"}},{"cell_type":"code","source":["def train_bipedalwalker1():\n","    # Create environment and model for BipedalWalker\n","    env_bipedal = make_vec_env(\"BipedalWalker-v3\", n_envs=1)\n","    model_b = PPO(\"MlpPolicy\", env_bipedal, verbose=1, tensorboard_log=\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/\",\n","                n_steps=2048, batch_size=64, gae_lambda=0.95, gamma=0.99,\n","                n_epochs=10, ent_coef=0.0, learning_rate=1e-4, clip_range=0.18)\n","\n","    eval_freq = 10000  # Evaluate every 10,000 steps\n","    n_eval_episodes = 10  # Number of episodes per evaluation\n","\n","    for step in range(1, 200001, eval_freq):\n","        # Train the model\n","        model_b.learn(total_timesteps=eval_freq, reset_num_timesteps=False)\n","\n","        # Evaluate the model on BipedalWalker\n","        mean_reward, std_reward = evaluate_model(model_b, env_bipedal, n_eval_episodes)\n","\n","        # Print evaluation results for BipedalWalker\n","        print(f\"BipedalWalker - Step: {step} | Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n","\n","    # Save the trained model for BipedalWalker\n","    model_b.save(\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4\")"],"metadata":{"id":"mtAUL_p6zQVu","executionInfo":{"status":"ok","timestamp":1740510917567,"user_tz":-120,"elapsed":10,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    train_bipedalwalker1()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKZzni831O9A","executionInfo":{"status":"ok","timestamp":1740511478601,"user_tz":-120,"elapsed":557762,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}},"outputId":"ede65e70-e483-44c7-c381-857e43772646"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n","<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n","<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n","/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n","  from pkg_resources import resource_stream, resource_exists\n","/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n","Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n","  declare_namespace(pkg)\n","/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n","Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n","  declare_namespace(pkg)\n","/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n","Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n","  declare_namespace(pkg)\n"]},{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.6e+03  |\n","|    ep_rew_mean     | -123     |\n","| time/              |          |\n","|    fps             | 973      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 2048     |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 584         |\n","|    ep_rew_mean          | -109        |\n","| time/                   |             |\n","|    fps                  | 712         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.007636534 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.68       |\n","|    explained_variance   | -0.0774     |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.273       |\n","|    n_updates            | 10          |\n","|    policy_gradient_loss | -0.014      |\n","|    std                  | 1           |\n","|    value_loss           | 0.736       |\n","-----------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 729           |\n","|    ep_rew_mean          | -107          |\n","| time/                   |               |\n","|    fps                  | 637           |\n","|    iterations           | 3             |\n","|    time_elapsed         | 9             |\n","|    total_timesteps      | 6144          |\n","| train/                  |               |\n","|    approx_kl            | 0.00018087283 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.18          |\n","|    entropy_loss         | -5.68         |\n","|    explained_variance   | -0.00526      |\n","|    learning_rate        | 0.0001        |\n","|    loss                 | 5.39          |\n","|    n_updates            | 20            |\n","|    policy_gradient_loss | -0.000513     |\n","|    std                  | 1             |\n","|    value_loss           | 154           |\n","-------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 838          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 638          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 8192         |\n","| train/                  |              |\n","|    approx_kl            | 0.0077933427 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.67        |\n","|    explained_variance   | 0.193        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.212        |\n","|    n_updates            | 30           |\n","|    policy_gradient_loss | -0.0137      |\n","|    std                  | 0.999        |\n","|    value_loss           | 0.355        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 990         |\n","|    ep_rew_mean          | -106        |\n","| time/                   |             |\n","|    fps                  | 635         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 16          |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.007013748 |\n","|    clip_fraction        | 0.0837      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.67       |\n","|    explained_variance   | 0.516       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0617      |\n","|    n_updates            | 40          |\n","|    policy_gradient_loss | -0.00895    |\n","|    std                  | 0.997       |\n","|    value_loss           | 0.178       |\n","-----------------------------------------\n","BipedalWalker - Step: 1 | Mean Reward: -104.11 ± 0.18\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.05e+03 |\n","|    ep_rew_mean     | -105     |\n","| time/              |          |\n","|    fps             | 712      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 12288    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 839         |\n","|    ep_rew_mean          | -106        |\n","| time/                   |             |\n","|    fps                  | 644         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 14336       |\n","| train/                  |             |\n","|    approx_kl            | 0.005355832 |\n","|    clip_fraction        | 0.0417      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.62       |\n","|    explained_variance   | 0.673       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0562      |\n","|    n_updates            | 60          |\n","|    policy_gradient_loss | -0.00597    |\n","|    std                  | 0.982       |\n","|    value_loss           | 0.122       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 884          |\n","|    ep_rew_mean          | -105         |\n","| time/                   |              |\n","|    fps                  | 637          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 9            |\n","|    total_timesteps      | 16384        |\n","| train/                  |              |\n","|    approx_kl            | 0.0005849281 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.6         |\n","|    explained_variance   | 0.0619       |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 51.7         |\n","|    n_updates            | 70           |\n","|    policy_gradient_loss | -0.00124     |\n","|    std                  | 0.982        |\n","|    value_loss           | 141          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 643         |\n","|    ep_rew_mean          | -108        |\n","| time/                   |             |\n","|    fps                  | 640         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 18432       |\n","| train/                  |             |\n","|    approx_kl            | 0.004315517 |\n","|    clip_fraction        | 0.0571      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.59       |\n","|    explained_variance   | 0.327       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.154       |\n","|    n_updates            | 80          |\n","|    policy_gradient_loss | -0.00706    |\n","|    std                  | 0.977       |\n","|    value_loss           | 0.263       |\n","-----------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 656           |\n","|    ep_rew_mean          | -107          |\n","| time/                   |               |\n","|    fps                  | 605           |\n","|    iterations           | 5             |\n","|    time_elapsed         | 16            |\n","|    total_timesteps      | 20480         |\n","| train/                  |               |\n","|    approx_kl            | 0.00018236987 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.18          |\n","|    entropy_loss         | -5.58         |\n","|    explained_variance   | 0.0549        |\n","|    learning_rate        | 0.0001        |\n","|    loss                 | 95.6          |\n","|    n_updates            | 90            |\n","|    policy_gradient_loss | -0.000496     |\n","|    std                  | 0.976         |\n","|    value_loss           | 305           |\n","-------------------------------------------\n","BipedalWalker - Step: 10001 | Mean Reward: -104.77 ± 0.09\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 637      |\n","|    ep_rew_mean     | -108     |\n","| time/              |          |\n","|    fps             | 984      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 22528    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 616          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 757          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 24576        |\n","| train/                  |              |\n","|    approx_kl            | 0.0037231396 |\n","|    clip_fraction        | 0.00981      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.57        |\n","|    explained_variance   | -0.0443      |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 19.5         |\n","|    n_updates            | 110          |\n","|    policy_gradient_loss | -0.00231     |\n","|    std                  | 0.975        |\n","|    value_loss           | 92.7         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 642         |\n","|    ep_rew_mean          | -107        |\n","| time/                   |             |\n","|    fps                  | 692         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 26624       |\n","| train/                  |             |\n","|    approx_kl            | 0.002645338 |\n","|    clip_fraction        | 0.00176     |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.57       |\n","|    explained_variance   | 0.0799      |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 8.48        |\n","|    n_updates            | 120         |\n","|    policy_gradient_loss | -0.00233    |\n","|    std                  | 0.974       |\n","|    value_loss           | 85.7        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 624          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 636          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 28672        |\n","| train/                  |              |\n","|    approx_kl            | 0.0043436177 |\n","|    clip_fraction        | 0.0397       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.56        |\n","|    explained_variance   | -0.0264      |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0707       |\n","|    n_updates            | 130          |\n","|    policy_gradient_loss | -0.00602     |\n","|    std                  | 0.967        |\n","|    value_loss           | 0.368        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 646          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 635          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 16           |\n","|    total_timesteps      | 30720        |\n","| train/                  |              |\n","|    approx_kl            | 0.0036380626 |\n","|    clip_fraction        | 0.0156       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.54        |\n","|    explained_variance   | 0.303        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 27.2         |\n","|    n_updates            | 140          |\n","|    policy_gradient_loss | -0.00316     |\n","|    std                  | 0.966        |\n","|    value_loss           | 80.4         |\n","------------------------------------------\n","BipedalWalker - Step: 20001 | Mean Reward: -103.49 ± 0.37\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 609      |\n","|    ep_rew_mean     | -107     |\n","| time/              |          |\n","|    fps             | 988      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 32768    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 579          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 675          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 6            |\n","|    total_timesteps      | 34816        |\n","| train/                  |              |\n","|    approx_kl            | 0.0013556454 |\n","|    clip_fraction        | 0.000586     |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.5         |\n","|    explained_variance   | 0.359        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 55.9         |\n","|    n_updates            | 160          |\n","|    policy_gradient_loss | -0.000967    |\n","|    std                  | 0.958        |\n","|    value_loss           | 128          |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 597          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 649          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 9            |\n","|    total_timesteps      | 36864        |\n","| train/                  |              |\n","|    approx_kl            | 0.0007624023 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.5         |\n","|    explained_variance   | 0.428        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 132          |\n","|    n_updates            | 170          |\n","|    policy_gradient_loss | -0.00117     |\n","|    std                  | 0.958        |\n","|    value_loss           | 123          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 614         |\n","|    ep_rew_mean          | -107        |\n","| time/                   |             |\n","|    fps                  | 651         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 38912       |\n","| train/                  |             |\n","|    approx_kl            | 0.005583451 |\n","|    clip_fraction        | 0.0241      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.5        |\n","|    explained_variance   | 0.593       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.366       |\n","|    n_updates            | 180         |\n","|    policy_gradient_loss | -0.00268    |\n","|    std                  | 0.955       |\n","|    value_loss           | 1.11        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 631         |\n","|    ep_rew_mean          | -107        |\n","| time/                   |             |\n","|    fps                  | 647         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 40960       |\n","| train/                  |             |\n","|    approx_kl            | 0.004120901 |\n","|    clip_fraction        | 0.0522      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.48       |\n","|    explained_variance   | 0.379       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0295      |\n","|    n_updates            | 190         |\n","|    policy_gradient_loss | -0.00482    |\n","|    std                  | 0.951       |\n","|    value_loss           | 0.397       |\n","-----------------------------------------\n","BipedalWalker - Step: 30001 | Mean Reward: -69.10 ± 1.77\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 648      |\n","|    ep_rew_mean     | -106     |\n","| time/              |          |\n","|    fps             | 790      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 43008    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 664          |\n","|    ep_rew_mean          | -106         |\n","| time/                   |              |\n","|    fps                  | 696          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 45056        |\n","| train/                  |              |\n","|    approx_kl            | 0.0061693345 |\n","|    clip_fraction        | 0.0853       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.43        |\n","|    explained_variance   | 0.265        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0186       |\n","|    n_updates            | 210          |\n","|    policy_gradient_loss | -0.00871     |\n","|    std                  | 0.936        |\n","|    value_loss           | 0.144        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 679         |\n","|    ep_rew_mean          | -106        |\n","| time/                   |             |\n","|    fps                  | 675         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 47104       |\n","| train/                  |             |\n","|    approx_kl            | 0.007915978 |\n","|    clip_fraction        | 0.109       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.39       |\n","|    explained_variance   | 0.0418      |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0372      |\n","|    n_updates            | 220         |\n","|    policy_gradient_loss | -0.0123     |\n","|    std                  | 0.929       |\n","|    value_loss           | 0.0955      |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 699          |\n","|    ep_rew_mean          | -105         |\n","| time/                   |              |\n","|    fps                  | 659          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 49152        |\n","| train/                  |              |\n","|    approx_kl            | 0.0044058356 |\n","|    clip_fraction        | 0.0355       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.37        |\n","|    explained_variance   | 0.103        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0515       |\n","|    n_updates            | 230          |\n","|    policy_gradient_loss | -0.00421     |\n","|    std                  | 0.926        |\n","|    value_loss           | 0.179        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 703          |\n","|    ep_rew_mean          | -105         |\n","| time/                   |              |\n","|    fps                  | 618          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 16           |\n","|    total_timesteps      | 51200        |\n","| train/                  |              |\n","|    approx_kl            | 0.0037365416 |\n","|    clip_fraction        | 0.0153       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.37        |\n","|    explained_variance   | 0.209        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.441        |\n","|    n_updates            | 240          |\n","|    policy_gradient_loss | -0.00131     |\n","|    std                  | 0.925        |\n","|    value_loss           | 22.5         |\n","------------------------------------------\n","BipedalWalker - Step: 40001 | Mean Reward: -73.22 ± 0.10\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 717      |\n","|    ep_rew_mean     | -104     |\n","| time/              |          |\n","|    fps             | 1018     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 53248    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 730          |\n","|    ep_rew_mean          | -104         |\n","| time/                   |              |\n","|    fps                  | 796          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 55296        |\n","| train/                  |              |\n","|    approx_kl            | 0.0039522285 |\n","|    clip_fraction        | 0.0403       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.36        |\n","|    explained_variance   | 0.68         |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0902       |\n","|    n_updates            | 260          |\n","|    policy_gradient_loss | -0.00427     |\n","|    std                  | 0.921        |\n","|    value_loss           | 0.862        |\n","------------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 742        |\n","|    ep_rew_mean          | -104       |\n","| time/                   |            |\n","|    fps                  | 720        |\n","|    iterations           | 3          |\n","|    time_elapsed         | 8          |\n","|    total_timesteps      | 57344      |\n","| train/                  |            |\n","|    approx_kl            | 0.00433202 |\n","|    clip_fraction        | 0.041      |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -5.33      |\n","|    explained_variance   | 0.353      |\n","|    learning_rate        | 0.0001     |\n","|    loss                 | 0.0601     |\n","|    n_updates            | 270        |\n","|    policy_gradient_loss | -0.00484   |\n","|    std                  | 0.914      |\n","|    value_loss           | 0.0945     |\n","----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 757          |\n","|    ep_rew_mean          | -104         |\n","| time/                   |              |\n","|    fps                  | 643          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 59392        |\n","| train/                  |              |\n","|    approx_kl            | 0.0031105168 |\n","|    clip_fraction        | 0.018        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.31        |\n","|    explained_variance   | 0.0632       |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0378       |\n","|    n_updates            | 280          |\n","|    policy_gradient_loss | -0.00451     |\n","|    std                  | 0.91         |\n","|    value_loss           | 0.166        |\n","------------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 768        |\n","|    ep_rew_mean          | -103       |\n","| time/                   |            |\n","|    fps                  | 643        |\n","|    iterations           | 5          |\n","|    time_elapsed         | 15         |\n","|    total_timesteps      | 61440      |\n","| train/                  |            |\n","|    approx_kl            | 0.00406593 |\n","|    clip_fraction        | 0.0104     |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -5.3       |\n","|    explained_variance   | 0.305      |\n","|    learning_rate        | 0.0001     |\n","|    loss                 | 24.9       |\n","|    n_updates            | 290        |\n","|    policy_gradient_loss | -0.00384   |\n","|    std                  | 0.909      |\n","|    value_loss           | 26.1       |\n","----------------------------------------\n","BipedalWalker - Step: 50001 | Mean Reward: -65.71 ± 4.27\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 779      |\n","|    ep_rew_mean     | -103     |\n","| time/              |          |\n","|    fps             | 953      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 63488    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 790          |\n","|    ep_rew_mean          | -103         |\n","| time/                   |              |\n","|    fps                  | 699          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 65536        |\n","| train/                  |              |\n","|    approx_kl            | 0.0034485303 |\n","|    clip_fraction        | 0.0246       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.28        |\n","|    explained_variance   | -0.433       |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0772       |\n","|    n_updates            | 310          |\n","|    policy_gradient_loss | -0.00505     |\n","|    std                  | 0.902        |\n","|    value_loss           | 0.109        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 801         |\n","|    ep_rew_mean          | -102        |\n","| time/                   |             |\n","|    fps                  | 632         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 67584       |\n","| train/                  |             |\n","|    approx_kl            | 0.003799856 |\n","|    clip_fraction        | 0.0331      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.25       |\n","|    explained_variance   | 0.551       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0356      |\n","|    n_updates            | 320         |\n","|    policy_gradient_loss | -0.00638    |\n","|    std                  | 0.896       |\n","|    value_loss           | 0.0534      |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 821         |\n","|    ep_rew_mean          | -102        |\n","| time/                   |             |\n","|    fps                  | 633         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 69632       |\n","| train/                  |             |\n","|    approx_kl            | 0.005288616 |\n","|    clip_fraction        | 0.0685      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.22       |\n","|    explained_variance   | 0.581       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0178      |\n","|    n_updates            | 330         |\n","|    policy_gradient_loss | -0.00744    |\n","|    std                  | 0.889       |\n","|    value_loss           | 0.0878      |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 831         |\n","|    ep_rew_mean          | -102        |\n","| time/                   |             |\n","|    fps                  | 638         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 16          |\n","|    total_timesteps      | 71680       |\n","| train/                  |             |\n","|    approx_kl            | 0.005294599 |\n","|    clip_fraction        | 0.0614      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.19       |\n","|    explained_variance   | 0.562       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0354      |\n","|    n_updates            | 340         |\n","|    policy_gradient_loss | -0.00746    |\n","|    std                  | 0.885       |\n","|    value_loss           | 0.117       |\n","-----------------------------------------\n","BipedalWalker - Step: 60001 | Mean Reward: -48.76 ± 2.81\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 841      |\n","|    ep_rew_mean     | -101     |\n","| time/              |          |\n","|    fps             | 807      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 73728    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 850         |\n","|    ep_rew_mean          | -101        |\n","| time/                   |             |\n","|    fps                  | 645         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 75776       |\n","| train/                  |             |\n","|    approx_kl            | 0.004765129 |\n","|    clip_fraction        | 0.0483      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.17       |\n","|    explained_variance   | 0.645       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0347      |\n","|    n_updates            | 360         |\n","|    policy_gradient_loss | -0.00692    |\n","|    std                  | 0.88        |\n","|    value_loss           | 0.0921      |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 859         |\n","|    ep_rew_mean          | -100        |\n","| time/                   |             |\n","|    fps                  | 644         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 77824       |\n","| train/                  |             |\n","|    approx_kl            | 0.005793722 |\n","|    clip_fraction        | 0.06        |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.15       |\n","|    explained_variance   | 0.431       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0175      |\n","|    n_updates            | 370         |\n","|    policy_gradient_loss | -0.00752    |\n","|    std                  | 0.875       |\n","|    value_loss           | 0.0953      |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 877          |\n","|    ep_rew_mean          | -99.7        |\n","| time/                   |              |\n","|    fps                  | 649          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 79872        |\n","| train/                  |              |\n","|    approx_kl            | 0.0047355965 |\n","|    clip_fraction        | 0.0521       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.13        |\n","|    explained_variance   | 0.558        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0502       |\n","|    n_updates            | 380          |\n","|    policy_gradient_loss | -0.00746     |\n","|    std                  | 0.869        |\n","|    value_loss           | 0.108        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 878          |\n","|    ep_rew_mean          | -100         |\n","| time/                   |              |\n","|    fps                  | 631          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 16           |\n","|    total_timesteps      | 81920        |\n","| train/                  |              |\n","|    approx_kl            | 0.0059877504 |\n","|    clip_fraction        | 0.0654       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.1         |\n","|    explained_variance   | 0.724        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0549       |\n","|    n_updates            | 390          |\n","|    policy_gradient_loss | -0.009       |\n","|    std                  | 0.863        |\n","|    value_loss           | 0.153        |\n","------------------------------------------\n","BipedalWalker - Step: 70001 | Mean Reward: -96.99 ± 0.38\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 877      |\n","|    ep_rew_mean     | -99.8    |\n","| time/              |          |\n","|    fps             | 953      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 83968    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 885          |\n","|    ep_rew_mean          | -99.4        |\n","| time/                   |              |\n","|    fps                  | 772          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 86016        |\n","| train/                  |              |\n","|    approx_kl            | 0.0007617359 |\n","|    clip_fraction        | 0.00259      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.09        |\n","|    explained_variance   | 0.463        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 25.3         |\n","|    n_updates            | 410          |\n","|    policy_gradient_loss | -0.00191     |\n","|    std                  | 0.863        |\n","|    value_loss           | 23.9         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 893         |\n","|    ep_rew_mean          | -98.9       |\n","| time/                   |             |\n","|    fps                  | 731         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 88064       |\n","| train/                  |             |\n","|    approx_kl            | 0.004671056 |\n","|    clip_fraction        | 0.0486      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.08       |\n","|    explained_variance   | 0.64        |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.286       |\n","|    n_updates            | 420         |\n","|    policy_gradient_loss | -0.00601    |\n","|    std                  | 0.859       |\n","|    value_loss           | 0.354       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 900          |\n","|    ep_rew_mean          | -97.9        |\n","| time/                   |              |\n","|    fps                  | 654          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 90112        |\n","| train/                  |              |\n","|    approx_kl            | 0.0057512387 |\n","|    clip_fraction        | 0.0603       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.06        |\n","|    explained_variance   | 0.533        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.042        |\n","|    n_updates            | 430          |\n","|    policy_gradient_loss | -0.00892     |\n","|    std                  | 0.854        |\n","|    value_loss           | 0.196        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 901          |\n","|    ep_rew_mean          | -97.6        |\n","| time/                   |              |\n","|    fps                  | 645          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 15           |\n","|    total_timesteps      | 92160        |\n","| train/                  |              |\n","|    approx_kl            | 0.0027886485 |\n","|    clip_fraction        | 0.00806      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.04        |\n","|    explained_variance   | 0.54         |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 9.92         |\n","|    n_updates            | 440          |\n","|    policy_gradient_loss | -0.00236     |\n","|    std                  | 0.854        |\n","|    value_loss           | 23.2         |\n","------------------------------------------\n","BipedalWalker - Step: 80001 | Mean Reward: -28.89 ± 1.78\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 909      |\n","|    ep_rew_mean     | -97      |\n","| time/              |          |\n","|    fps             | 1000     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 94208    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 907          |\n","|    ep_rew_mean          | -96.7        |\n","| time/                   |              |\n","|    fps                  | 780          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 96256        |\n","| train/                  |              |\n","|    approx_kl            | 0.0069787446 |\n","|    clip_fraction        | 0.0886       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.05        |\n","|    explained_variance   | 0.668        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.289        |\n","|    n_updates            | 460          |\n","|    policy_gradient_loss | -0.0103      |\n","|    std                  | 0.855        |\n","|    value_loss           | 0.434        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 905         |\n","|    ep_rew_mean          | -96.3       |\n","| time/                   |             |\n","|    fps                  | 651         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 98304       |\n","| train/                  |             |\n","|    approx_kl            | 0.003719691 |\n","|    clip_fraction        | 0.0361      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.04       |\n","|    explained_variance   | 0.603       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.499       |\n","|    n_updates            | 470         |\n","|    policy_gradient_loss | -0.00374    |\n","|    std                  | 0.854       |\n","|    value_loss           | 18.7        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 911          |\n","|    ep_rew_mean          | -95.8        |\n","| time/                   |              |\n","|    fps                  | 652          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 100352       |\n","| train/                  |              |\n","|    approx_kl            | 0.0034745748 |\n","|    clip_fraction        | 0.0177       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.04        |\n","|    explained_variance   | 0.747        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 4.67         |\n","|    n_updates            | 480          |\n","|    policy_gradient_loss | -0.0033      |\n","|    std                  | 0.854        |\n","|    value_loss           | 16.8         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 911          |\n","|    ep_rew_mean          | -95.2        |\n","| time/                   |              |\n","|    fps                  | 652          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 15           |\n","|    total_timesteps      | 102400       |\n","| train/                  |              |\n","|    approx_kl            | 0.0063357423 |\n","|    clip_fraction        | 0.0596       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.04        |\n","|    explained_variance   | 0.793        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 24.6         |\n","|    n_updates            | 490          |\n","|    policy_gradient_loss | -0.00735     |\n","|    std                  | 0.854        |\n","|    value_loss           | 16.1         |\n","------------------------------------------\n","BipedalWalker - Step: 90001 | Mean Reward: -12.56 ± 8.37\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 926      |\n","|    ep_rew_mean     | -94.5    |\n","| time/              |          |\n","|    fps             | 902      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 104448   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 941          |\n","|    ep_rew_mean          | -93.8        |\n","| time/                   |              |\n","|    fps                  | 520          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 7            |\n","|    total_timesteps      | 106496       |\n","| train/                  |              |\n","|    approx_kl            | 0.0071633086 |\n","|    clip_fraction        | 0.0441       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5           |\n","|    explained_variance   | 0.762        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 4.86         |\n","|    n_updates            | 510          |\n","|    policy_gradient_loss | -0.00438     |\n","|    std                  | 0.845        |\n","|    value_loss           | 15           |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 957          |\n","|    ep_rew_mean          | -93.2        |\n","| time/                   |              |\n","|    fps                  | 532          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 108544       |\n","| train/                  |              |\n","|    approx_kl            | 0.0065644723 |\n","|    clip_fraction        | 0.0721       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.98        |\n","|    explained_variance   | 0.67         |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0666       |\n","|    n_updates            | 520          |\n","|    policy_gradient_loss | -0.0091      |\n","|    std                  | 0.836        |\n","|    value_loss           | 0.146        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 957          |\n","|    ep_rew_mean          | -91.6        |\n","| time/                   |              |\n","|    fps                  | 556          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 14           |\n","|    total_timesteps      | 110592       |\n","| train/                  |              |\n","|    approx_kl            | 0.0061673704 |\n","|    clip_fraction        | 0.0479       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.95        |\n","|    explained_variance   | 0.775        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.055        |\n","|    n_updates            | 530          |\n","|    policy_gradient_loss | -0.00555     |\n","|    std                  | 0.831        |\n","|    value_loss           | 0.204        |\n","------------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 957        |\n","|    ep_rew_mean          | -90.8      |\n","| time/                   |            |\n","|    fps                  | 561        |\n","|    iterations           | 5          |\n","|    time_elapsed         | 18         |\n","|    total_timesteps      | 112640     |\n","| train/                  |            |\n","|    approx_kl            | 0.00736079 |\n","|    clip_fraction        | 0.102      |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -4.92      |\n","|    explained_variance   | 0.803      |\n","|    learning_rate        | 0.0001     |\n","|    loss                 | 0.0932     |\n","|    n_updates            | 540        |\n","|    policy_gradient_loss | -0.0111    |\n","|    std                  | 0.825      |\n","|    value_loss           | 0.207      |\n","----------------------------------------\n","BipedalWalker - Step: 100001 | Mean Reward: 104.27 ± 9.09\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 957      |\n","|    ep_rew_mean     | -89.9    |\n","| time/              |          |\n","|    fps             | 866      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 114688   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 941          |\n","|    ep_rew_mean          | -89.2        |\n","| time/                   |              |\n","|    fps                  | 751          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 116736       |\n","| train/                  |              |\n","|    approx_kl            | 0.0064102504 |\n","|    clip_fraction        | 0.0665       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.87        |\n","|    explained_variance   | 0.744        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0829       |\n","|    n_updates            | 560          |\n","|    policy_gradient_loss | -0.00989     |\n","|    std                  | 0.816        |\n","|    value_loss           | 0.263        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 957          |\n","|    ep_rew_mean          | -88.1        |\n","| time/                   |              |\n","|    fps                  | 721          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 118784       |\n","| train/                  |              |\n","|    approx_kl            | 0.0049989223 |\n","|    clip_fraction        | 0.0478       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.86        |\n","|    explained_variance   | 0.702        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 9.16         |\n","|    n_updates            | 570          |\n","|    policy_gradient_loss | -0.0048      |\n","|    std                  | 0.815        |\n","|    value_loss           | 15.8         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 987         |\n","|    ep_rew_mean          | -86         |\n","| time/                   |             |\n","|    fps                  | 687         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 120832      |\n","| train/                  |             |\n","|    approx_kl            | 0.005612068 |\n","|    clip_fraction        | 0.0541      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.84       |\n","|    explained_variance   | 0.615       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.157       |\n","|    n_updates            | 580         |\n","|    policy_gradient_loss | -0.00762    |\n","|    std                  | 0.809       |\n","|    value_loss           | 0.326       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 987         |\n","|    ep_rew_mean          | -85.1       |\n","| time/                   |             |\n","|    fps                  | 640         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 122880      |\n","| train/                  |             |\n","|    approx_kl            | 0.003466466 |\n","|    clip_fraction        | 0.027       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.82       |\n","|    explained_variance   | 0.5         |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.069       |\n","|    n_updates            | 590         |\n","|    policy_gradient_loss | -0.00362    |\n","|    std                  | 0.808       |\n","|    value_loss           | 0.768       |\n","-----------------------------------------\n","BipedalWalker - Step: 110001 | Mean Reward: 104.84 ± 2.88\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 987      |\n","|    ep_rew_mean     | -84      |\n","| time/              |          |\n","|    fps             | 993      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 124928   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1e+03       |\n","|    ep_rew_mean          | -82.8       |\n","| time/                   |             |\n","|    fps                  | 779         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 126976      |\n","| train/                  |             |\n","|    approx_kl            | 0.006383878 |\n","|    clip_fraction        | 0.079       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.82       |\n","|    explained_variance   | 0.687       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0945      |\n","|    n_updates            | 610         |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.808       |\n","|    value_loss           | 0.304       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.02e+03     |\n","|    ep_rew_mean          | -81.6        |\n","| time/                   |              |\n","|    fps                  | 694          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 129024       |\n","| train/                  |              |\n","|    approx_kl            | 0.0063640946 |\n","|    clip_fraction        | 0.0603       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.81        |\n","|    explained_variance   | 0.829        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.106        |\n","|    n_updates            | 620          |\n","|    policy_gradient_loss | -0.00861     |\n","|    std                  | 0.804        |\n","|    value_loss           | 0.212        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.05e+03    |\n","|    ep_rew_mean          | -78.9       |\n","| time/                   |             |\n","|    fps                  | 646         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 131072      |\n","| train/                  |             |\n","|    approx_kl            | 0.004702615 |\n","|    clip_fraction        | 0.0508      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.79       |\n","|    explained_variance   | 0.793       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0676      |\n","|    n_updates            | 630         |\n","|    policy_gradient_loss | -0.00613    |\n","|    std                  | 0.799       |\n","|    value_loss           | 0.169       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.06e+03     |\n","|    ep_rew_mean          | -78.5        |\n","| time/                   |              |\n","|    fps                  | 650          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 15           |\n","|    total_timesteps      | 133120       |\n","| train/                  |              |\n","|    approx_kl            | 0.0056624357 |\n","|    clip_fraction        | 0.0591       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.76        |\n","|    explained_variance   | 0.828        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.108        |\n","|    n_updates            | 640          |\n","|    policy_gradient_loss | -0.00713     |\n","|    std                  | 0.795        |\n","|    value_loss           | 0.246        |\n","------------------------------------------\n","BipedalWalker - Step: 120001 | Mean Reward: 113.46 ± 2.75\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.08e+03 |\n","|    ep_rew_mean     | -76.7    |\n","| time/              |          |\n","|    fps             | 970      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 135168   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.08e+03     |\n","|    ep_rew_mean          | -75.3        |\n","| time/                   |              |\n","|    fps                  | 734          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 137216       |\n","| train/                  |              |\n","|    approx_kl            | 0.0024437634 |\n","|    clip_fraction        | 0.00806      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.76        |\n","|    explained_variance   | 0.814        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 9.69         |\n","|    n_updates            | 660          |\n","|    policy_gradient_loss | -0.00274     |\n","|    std                  | 0.795        |\n","|    value_loss           | 38.2         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.1e+03      |\n","|    ep_rew_mean          | -73.6        |\n","| time/                   |              |\n","|    fps                  | 652          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 9            |\n","|    total_timesteps      | 139264       |\n","| train/                  |              |\n","|    approx_kl            | 0.0029345965 |\n","|    clip_fraction        | 0.0113       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.75        |\n","|    explained_variance   | 0.947        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.159        |\n","|    n_updates            | 670          |\n","|    policy_gradient_loss | -0.00367     |\n","|    std                  | 0.794        |\n","|    value_loss           | 2.69         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.1e+03      |\n","|    ep_rew_mean          | -72.2        |\n","| time/                   |              |\n","|    fps                  | 650          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 141312       |\n","| train/                  |              |\n","|    approx_kl            | 0.0045627495 |\n","|    clip_fraction        | 0.0453       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.74        |\n","|    explained_variance   | 0.758        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.117        |\n","|    n_updates            | 680          |\n","|    policy_gradient_loss | -0.00566     |\n","|    std                  | 0.789        |\n","|    value_loss           | 7.02         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.1e+03      |\n","|    ep_rew_mean          | -70.7        |\n","| time/                   |              |\n","|    fps                  | 648          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 15           |\n","|    total_timesteps      | 143360       |\n","| train/                  |              |\n","|    approx_kl            | 0.0023304746 |\n","|    clip_fraction        | 0.0101       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.72        |\n","|    explained_variance   | 0.472        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 5.71         |\n","|    n_updates            | 690          |\n","|    policy_gradient_loss | -0.00306     |\n","|    std                  | 0.788        |\n","|    value_loss           | 59.8         |\n","------------------------------------------\n","BipedalWalker - Step: 130001 | Mean Reward: 114.09 ± 2.20\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.11e+03 |\n","|    ep_rew_mean     | -69.3    |\n","| time/              |          |\n","|    fps             | 803      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 145408   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.13e+03     |\n","|    ep_rew_mean          | -68          |\n","| time/                   |              |\n","|    fps                  | 656          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 6            |\n","|    total_timesteps      | 147456       |\n","| train/                  |              |\n","|    approx_kl            | 0.0062834565 |\n","|    clip_fraction        | 0.0668       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.69        |\n","|    explained_variance   | 0.82         |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0933       |\n","|    n_updates            | 710          |\n","|    policy_gradient_loss | -0.00805     |\n","|    std                  | 0.779        |\n","|    value_loss           | 0.23         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.12e+03     |\n","|    ep_rew_mean          | -66.9        |\n","| time/                   |              |\n","|    fps                  | 654          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 9            |\n","|    total_timesteps      | 149504       |\n","| train/                  |              |\n","|    approx_kl            | 0.0062864115 |\n","|    clip_fraction        | 0.0958       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.66        |\n","|    explained_variance   | 0.845        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0876       |\n","|    n_updates            | 720          |\n","|    policy_gradient_loss | -0.0107      |\n","|    std                  | 0.775        |\n","|    value_loss           | 0.252        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.14e+03     |\n","|    ep_rew_mean          | -65.5        |\n","| time/                   |              |\n","|    fps                  | 658          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 151552       |\n","| train/                  |              |\n","|    approx_kl            | 0.0040863864 |\n","|    clip_fraction        | 0.0198       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.65        |\n","|    explained_variance   | 0.835        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 21.6         |\n","|    n_updates            | 730          |\n","|    policy_gradient_loss | -0.00515     |\n","|    std                  | 0.775        |\n","|    value_loss           | 24.6         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.15e+03     |\n","|    ep_rew_mean          | -64          |\n","| time/                   |              |\n","|    fps                  | 637          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 16           |\n","|    total_timesteps      | 153600       |\n","| train/                  |              |\n","|    approx_kl            | 0.0058451714 |\n","|    clip_fraction        | 0.0865       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.65        |\n","|    explained_variance   | 0.708        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0653       |\n","|    n_updates            | 740          |\n","|    policy_gradient_loss | -0.0078      |\n","|    std                  | 0.773        |\n","|    value_loss           | 0.192        |\n","------------------------------------------\n","BipedalWalker - Step: 140001 | Mean Reward: 106.25 ± 1.32\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.15e+03 |\n","|    ep_rew_mean     | -62.8    |\n","| time/              |          |\n","|    fps             | 983      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 155648   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.17e+03    |\n","|    ep_rew_mean          | -61.1       |\n","| time/                   |             |\n","|    fps                  | 790         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 157696      |\n","| train/                  |             |\n","|    approx_kl            | 0.005976433 |\n","|    clip_fraction        | 0.0875      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.63       |\n","|    explained_variance   | 0.814       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0662      |\n","|    n_updates            | 760         |\n","|    policy_gradient_loss | -0.00886    |\n","|    std                  | 0.768       |\n","|    value_loss           | 0.212       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.18e+03    |\n","|    ep_rew_mean          | -59.4       |\n","| time/                   |             |\n","|    fps                  | 740         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 159744      |\n","| train/                  |             |\n","|    approx_kl            | 0.006435856 |\n","|    clip_fraction        | 0.108       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.61       |\n","|    explained_variance   | 0.87        |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0642      |\n","|    n_updates            | 770         |\n","|    policy_gradient_loss | -0.01       |\n","|    std                  | 0.764       |\n","|    value_loss           | 0.216       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.22e+03    |\n","|    ep_rew_mean          | -56.2       |\n","| time/                   |             |\n","|    fps                  | 680         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 161792      |\n","| train/                  |             |\n","|    approx_kl            | 0.009357792 |\n","|    clip_fraction        | 0.108       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.58       |\n","|    explained_variance   | 0.86        |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0825      |\n","|    n_updates            | 780         |\n","|    policy_gradient_loss | -0.0112     |\n","|    std                  | 0.758       |\n","|    value_loss           | 0.21        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.22e+03     |\n","|    ep_rew_mean          | -54.7        |\n","| time/                   |              |\n","|    fps                  | 652          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 15           |\n","|    total_timesteps      | 163840       |\n","| train/                  |              |\n","|    approx_kl            | 0.0072803777 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.57        |\n","|    explained_variance   | 0.889        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0638       |\n","|    n_updates            | 790          |\n","|    policy_gradient_loss | -0.0111      |\n","|    std                  | 0.759        |\n","|    value_loss           | 0.191        |\n","------------------------------------------\n","BipedalWalker - Step: 150001 | Mean Reward: 126.33 ± 1.89\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.23e+03 |\n","|    ep_rew_mean     | -53      |\n","| time/              |          |\n","|    fps             | 994      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 165888   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.25e+03     |\n","|    ep_rew_mean          | -51.2        |\n","| time/                   |              |\n","|    fps                  | 768          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 167936       |\n","| train/                  |              |\n","|    approx_kl            | 0.0034045223 |\n","|    clip_fraction        | 0.0346       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.56        |\n","|    explained_variance   | 0.893        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.233        |\n","|    n_updates            | 810          |\n","|    policy_gradient_loss | -0.00349     |\n","|    std                  | 0.757        |\n","|    value_loss           | 9.5          |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.26e+03     |\n","|    ep_rew_mean          | -49.2        |\n","| time/                   |              |\n","|    fps                  | 667          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 9            |\n","|    total_timesteps      | 169984       |\n","| train/                  |              |\n","|    approx_kl            | 0.0059903227 |\n","|    clip_fraction        | 0.0788       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.55        |\n","|    explained_variance   | 0.769        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.105        |\n","|    n_updates            | 820          |\n","|    policy_gradient_loss | -0.00887     |\n","|    std                  | 0.753        |\n","|    value_loss           | 0.258        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.26e+03    |\n","|    ep_rew_mean          | -45.9       |\n","| time/                   |             |\n","|    fps                  | 646         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 172032      |\n","| train/                  |             |\n","|    approx_kl            | 0.003167565 |\n","|    clip_fraction        | 0.0211      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.54       |\n","|    explained_variance   | -0.133      |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 81.9        |\n","|    n_updates            | 830         |\n","|    policy_gradient_loss | -0.00289    |\n","|    std                  | 0.753       |\n","|    value_loss           | 48.6        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.24e+03    |\n","|    ep_rew_mean          | -45.9       |\n","| time/                   |             |\n","|    fps                  | 643         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 174080      |\n","| train/                  |             |\n","|    approx_kl            | 0.005089348 |\n","|    clip_fraction        | 0.0578      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.54       |\n","|    explained_variance   | 0.804       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.124       |\n","|    n_updates            | 840         |\n","|    policy_gradient_loss | -0.00653    |\n","|    std                  | 0.755       |\n","|    value_loss           | 0.353       |\n","-----------------------------------------\n","BipedalWalker - Step: 160001 | Mean Reward: 132.52 ± 2.23\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.24e+03 |\n","|    ep_rew_mean     | -44.3    |\n","| time/              |          |\n","|    fps             | 1019     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 176128   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.24e+03    |\n","|    ep_rew_mean          | -42.7       |\n","| time/                   |             |\n","|    fps                  | 651         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 178176      |\n","| train/                  |             |\n","|    approx_kl            | 0.006058776 |\n","|    clip_fraction        | 0.0822      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.54       |\n","|    explained_variance   | 0.657       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.114       |\n","|    n_updates            | 860         |\n","|    policy_gradient_loss | -0.0071     |\n","|    std                  | 0.751       |\n","|    value_loss           | 0.444       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.24e+03    |\n","|    ep_rew_mean          | -41.3       |\n","| time/                   |             |\n","|    fps                  | 657         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 180224      |\n","| train/                  |             |\n","|    approx_kl            | 0.005352338 |\n","|    clip_fraction        | 0.0584      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.53       |\n","|    explained_variance   | 0.814       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.234       |\n","|    n_updates            | 870         |\n","|    policy_gradient_loss | -0.00589    |\n","|    std                  | 0.749       |\n","|    value_loss           | 0.621       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.25e+03     |\n","|    ep_rew_mean          | -37.8        |\n","| time/                   |              |\n","|    fps                  | 657          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 182272       |\n","| train/                  |              |\n","|    approx_kl            | 0.0064660017 |\n","|    clip_fraction        | 0.0745       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.5         |\n","|    explained_variance   | 0.845        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0852       |\n","|    n_updates            | 880          |\n","|    policy_gradient_loss | -0.00846     |\n","|    std                  | 0.744        |\n","|    value_loss           | 0.311        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.25e+03     |\n","|    ep_rew_mean          | -37.3        |\n","| time/                   |              |\n","|    fps                  | 658          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 15           |\n","|    total_timesteps      | 184320       |\n","| train/                  |              |\n","|    approx_kl            | 0.0075945873 |\n","|    clip_fraction        | 0.0614       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.48        |\n","|    explained_variance   | 0.813        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.192        |\n","|    n_updates            | 890          |\n","|    policy_gradient_loss | -0.00658     |\n","|    std                  | 0.742        |\n","|    value_loss           | 0.583        |\n","------------------------------------------\n","BipedalWalker - Step: 170001 | Mean Reward: -97.73 ± 0.31\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.25e+03 |\n","|    ep_rew_mean     | -35.6    |\n","| time/              |          |\n","|    fps             | 818      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 186368   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.24e+03     |\n","|    ep_rew_mean          | -34.3        |\n","| time/                   |              |\n","|    fps                  | 726          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 188416       |\n","| train/                  |              |\n","|    approx_kl            | 0.0068921847 |\n","|    clip_fraction        | 0.0268       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.48        |\n","|    explained_variance   | -0.0482      |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 74.5         |\n","|    n_updates            | 910          |\n","|    policy_gradient_loss | -0.00208     |\n","|    std                  | 0.742        |\n","|    value_loss           | 47.4         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.24e+03     |\n","|    ep_rew_mean          | -32.6        |\n","| time/                   |              |\n","|    fps                  | 706          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 190464       |\n","| train/                  |              |\n","|    approx_kl            | 0.0051056626 |\n","|    clip_fraction        | 0.0619       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.48        |\n","|    explained_variance   | 0.0369       |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 74.1         |\n","|    n_updates            | 920          |\n","|    policy_gradient_loss | -0.00802     |\n","|    std                  | 0.743        |\n","|    value_loss           | 47.3         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.25e+03    |\n","|    ep_rew_mean          | -28.9       |\n","| time/                   |             |\n","|    fps                  | 695         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 192512      |\n","| train/                  |             |\n","|    approx_kl            | 0.006399822 |\n","|    clip_fraction        | 0.0686      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.48       |\n","|    explained_variance   | 0.646       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.309       |\n","|    n_updates            | 930         |\n","|    policy_gradient_loss | -0.00612    |\n","|    std                  | 0.74        |\n","|    value_loss           | 0.514       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.25e+03     |\n","|    ep_rew_mean          | -27.3        |\n","| time/                   |              |\n","|    fps                  | 643          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 15           |\n","|    total_timesteps      | 194560       |\n","| train/                  |              |\n","|    approx_kl            | 0.0074723633 |\n","|    clip_fraction        | 0.0786       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.44        |\n","|    explained_variance   | 0.768        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.176        |\n","|    n_updates            | 940          |\n","|    policy_gradient_loss | -0.0102      |\n","|    std                  | 0.731        |\n","|    value_loss           | 0.505        |\n","------------------------------------------\n","BipedalWalker - Step: 180001 | Mean Reward: -47.48 ± 96.57\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.25e+03 |\n","|    ep_rew_mean     | -25.5    |\n","| time/              |          |\n","|    fps             | 999      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 196608   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.25e+03     |\n","|    ep_rew_mean          | -23.8        |\n","| time/                   |              |\n","|    fps                  | 754          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 198656       |\n","| train/                  |              |\n","|    approx_kl            | 0.0057148607 |\n","|    clip_fraction        | 0.0722       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.38        |\n","|    explained_variance   | 0.832        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0787       |\n","|    n_updates            | 960          |\n","|    policy_gradient_loss | -0.00808     |\n","|    std                  | 0.72         |\n","|    value_loss           | 0.292        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.25e+03    |\n","|    ep_rew_mean          | -22.1       |\n","| time/                   |             |\n","|    fps                  | 649         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 200704      |\n","| train/                  |             |\n","|    approx_kl            | 0.007362795 |\n","|    clip_fraction        | 0.0975      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.35       |\n","|    explained_variance   | 0.867       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0897      |\n","|    n_updates            | 970         |\n","|    policy_gradient_loss | -0.0114     |\n","|    std                  | 0.715       |\n","|    value_loss           | 0.341       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.25e+03     |\n","|    ep_rew_mean          | -18.7        |\n","| time/                   |              |\n","|    fps                  | 650          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 202752       |\n","| train/                  |              |\n","|    approx_kl            | 0.0063875597 |\n","|    clip_fraction        | 0.0878       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.32        |\n","|    explained_variance   | 0.871        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.147        |\n","|    n_updates            | 980          |\n","|    policy_gradient_loss | -0.0093      |\n","|    std                  | 0.711        |\n","|    value_loss           | 0.358        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.25e+03    |\n","|    ep_rew_mean          | -16.7       |\n","| time/                   |             |\n","|    fps                  | 645         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 204800      |\n","| train/                  |             |\n","|    approx_kl            | 0.006022602 |\n","|    clip_fraction        | 0.0817      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.29       |\n","|    explained_variance   | 0.883       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.205       |\n","|    n_updates            | 990         |\n","|    policy_gradient_loss | -0.0108     |\n","|    std                  | 0.705       |\n","|    value_loss           | 0.349       |\n","-----------------------------------------\n","BipedalWalker - Step: 190001 | Mean Reward: 187.46 ± 3.81\n"]}]},{"cell_type":"markdown","source":["# 2. PPO Bipedal\n","env = 1, batch size= 64, lr = 3e-4"],"metadata":{"id":"DmoCdDbbBMMP"}},{"cell_type":"code","source":["def train_bipedalwalker2():\n","    # Create environment and model for BipedalWalker\n","    env_bipedal = make_vec_env(\"BipedalWalker-v3\", n_envs=1)\n","    model_b = PPO(\"MlpPolicy\", env_bipedal, verbose=1, tensorboard_log=\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/\",\n","                n_steps=2048, batch_size=64, gae_lambda=0.95, gamma=0.99,\n","                n_epochs=10, ent_coef=0.0, learning_rate=3e-4, clip_range=0.18)\n","\n","    eval_freq = 10000  # Evaluate every 10,000 steps\n","    n_eval_episodes = 10  # Number of episodes per evaluation\n","\n","    for step in range(1, 200001, eval_freq):\n","        # Train the model\n","        model_b.learn(total_timesteps=eval_freq, reset_num_timesteps=False)\n","\n","        # Evaluate the model on BipedalWalker\n","        mean_reward, std_reward = evaluate_model(model_b, env_bipedal, n_eval_episodes)\n","\n","        # Print evaluation results for BipedalWalker\n","        print(f\"BipedalWalker - Step: {step} | Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n","\n","    # Save the trained model for BipedalWalker\n","    model_b.save(\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4\")"],"metadata":{"id":"YXIFaPLDBYr0","executionInfo":{"status":"ok","timestamp":1740511552322,"user_tz":-120,"elapsed":20,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    train_bipedalwalker2()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7XpAdDMBfGZ","executionInfo":{"status":"ok","timestamp":1740512163155,"user_tz":-120,"elapsed":610828,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}},"outputId":"0a5c4d2f-7d3f-466d-f5d7-e4a758bfcd40"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 328      |\n","|    ep_rew_mean     | -109     |\n","| time/              |          |\n","|    fps             | 859      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 2048     |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 509          |\n","|    ep_rew_mean          | -109         |\n","| time/                   |              |\n","|    fps                  | 627          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 6            |\n","|    total_timesteps      | 4096         |\n","| train/                  |              |\n","|    approx_kl            | 0.0074911728 |\n","|    clip_fraction        | 0.0679       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.67        |\n","|    explained_variance   | 0.00526      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 48.9         |\n","|    n_updates            | 10           |\n","|    policy_gradient_loss | -0.00754     |\n","|    std                  | 1            |\n","|    value_loss           | 187          |\n","------------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 493        |\n","|    ep_rew_mean          | -108       |\n","| time/                   |            |\n","|    fps                  | 632        |\n","|    iterations           | 3          |\n","|    time_elapsed         | 9          |\n","|    total_timesteps      | 6144       |\n","| train/                  |            |\n","|    approx_kl            | 0.01067275 |\n","|    clip_fraction        | 0.186      |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -5.67      |\n","|    explained_variance   | 0.109      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.078      |\n","|    n_updates            | 20         |\n","|    policy_gradient_loss | -0.0172    |\n","|    std                  | 0.995      |\n","|    value_loss           | 0.342      |\n","----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 585          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 630          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 8192         |\n","| train/                  |              |\n","|    approx_kl            | 0.0040211435 |\n","|    clip_fraction        | 0.0109       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.66        |\n","|    explained_variance   | -0.0617      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 65.1         |\n","|    n_updates            | 30           |\n","|    policy_gradient_loss | -0.00393     |\n","|    std                  | 0.995        |\n","|    value_loss           | 92.6         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 553         |\n","|    ep_rew_mean          | -108        |\n","| time/                   |             |\n","|    fps                  | 611         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 16          |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.009628107 |\n","|    clip_fraction        | 0.138       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.63       |\n","|    explained_variance   | 0.185       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0744      |\n","|    n_updates            | 40          |\n","|    policy_gradient_loss | -0.0133     |\n","|    std                  | 0.983       |\n","|    value_loss           | 0.214       |\n","-----------------------------------------\n","BipedalWalker - Step: 1 | Mean Reward: -24.33 ± 2.27\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 583      |\n","|    ep_rew_mean     | -106     |\n","| time/              |          |\n","|    fps             | 939      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 12288    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 604         |\n","|    ep_rew_mean          | -111        |\n","| time/                   |             |\n","|    fps                  | 777         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 14336       |\n","| train/                  |             |\n","|    approx_kl            | 0.007880599 |\n","|    clip_fraction        | 0.0766      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.6        |\n","|    explained_variance   | 0.615       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 50.1        |\n","|    n_updates            | 60          |\n","|    policy_gradient_loss | -0.00571    |\n","|    std                  | 0.981       |\n","|    value_loss           | 24.9        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 670         |\n","|    ep_rew_mean          | -108        |\n","| time/                   |             |\n","|    fps                  | 732         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 16384       |\n","| train/                  |             |\n","|    approx_kl            | 0.006139959 |\n","|    clip_fraction        | 0.0448      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.59       |\n","|    explained_variance   | 0.568       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.06        |\n","|    n_updates            | 70          |\n","|    policy_gradient_loss | -0.00455    |\n","|    std                  | 0.975       |\n","|    value_loss           | 27.8        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 711         |\n","|    ep_rew_mean          | -107        |\n","| time/                   |             |\n","|    fps                  | 657         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 18432       |\n","| train/                  |             |\n","|    approx_kl            | 0.005518608 |\n","|    clip_fraction        | 0.0655      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.58       |\n","|    explained_variance   | 0.507       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.15        |\n","|    n_updates            | 80          |\n","|    policy_gradient_loss | -0.00682    |\n","|    std                  | 0.977       |\n","|    value_loss           | 19.3        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 721         |\n","|    ep_rew_mean          | -106        |\n","| time/                   |             |\n","|    fps                  | 642         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 20480       |\n","| train/                  |             |\n","|    approx_kl            | 0.009052245 |\n","|    clip_fraction        | 0.124       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.53       |\n","|    explained_variance   | 0.174       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.049       |\n","|    n_updates            | 90          |\n","|    policy_gradient_loss | -0.00947    |\n","|    std                  | 0.958       |\n","|    value_loss           | 0.123       |\n","-----------------------------------------\n","BipedalWalker - Step: 10001 | Mean Reward: -60.78 ± 3.26\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 755      |\n","|    ep_rew_mean     | -105     |\n","| time/              |          |\n","|    fps             | 993      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 22528    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 786          |\n","|    ep_rew_mean          | -104         |\n","| time/                   |              |\n","|    fps                  | 786          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 24576        |\n","| train/                  |              |\n","|    approx_kl            | 0.0069985446 |\n","|    clip_fraction        | 0.0905       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.46        |\n","|    explained_variance   | 0.547        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0779       |\n","|    n_updates            | 110          |\n","|    policy_gradient_loss | -0.00845     |\n","|    std                  | 0.945        |\n","|    value_loss           | 0.209        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 815         |\n","|    ep_rew_mean          | -103        |\n","| time/                   |             |\n","|    fps                  | 651         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 26624       |\n","| train/                  |             |\n","|    approx_kl            | 0.009760002 |\n","|    clip_fraction        | 0.116       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.39       |\n","|    explained_variance   | 0.478       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0682      |\n","|    n_updates            | 120         |\n","|    policy_gradient_loss | -0.0125     |\n","|    std                  | 0.923       |\n","|    value_loss           | 0.136       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 868         |\n","|    ep_rew_mean          | -101        |\n","| time/                   |             |\n","|    fps                  | 652         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 28672       |\n","| train/                  |             |\n","|    approx_kl            | 0.008276108 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.33       |\n","|    explained_variance   | 0.697       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0296      |\n","|    n_updates            | 130         |\n","|    policy_gradient_loss | -0.0098     |\n","|    std                  | 0.916       |\n","|    value_loss           | 0.103       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 891         |\n","|    ep_rew_mean          | -99.8       |\n","| time/                   |             |\n","|    fps                  | 649         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 30720       |\n","| train/                  |             |\n","|    approx_kl            | 0.007374393 |\n","|    clip_fraction        | 0.0945      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.3        |\n","|    explained_variance   | 0.48        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0361      |\n","|    n_updates            | 140         |\n","|    policy_gradient_loss | -0.0108     |\n","|    std                  | 0.908       |\n","|    value_loss           | 0.126       |\n","-----------------------------------------\n","BipedalWalker - Step: 20001 | Mean Reward: -40.69 ± 4.78\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 888      |\n","|    ep_rew_mean     | -98.5    |\n","| time/              |          |\n","|    fps             | 1022     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 32768    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 909          |\n","|    ep_rew_mean          | -97.5        |\n","| time/                   |              |\n","|    fps                  | 667          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 6            |\n","|    total_timesteps      | 34816        |\n","| train/                  |              |\n","|    approx_kl            | 0.0060782474 |\n","|    clip_fraction        | 0.0598       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.23        |\n","|    explained_variance   | 0.465        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.619        |\n","|    n_updates            | 160          |\n","|    policy_gradient_loss | -0.00617     |\n","|    std                  | 0.895        |\n","|    value_loss           | 17.8         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 904         |\n","|    ep_rew_mean          | -96.3       |\n","| time/                   |             |\n","|    fps                  | 659         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 36864       |\n","| train/                  |             |\n","|    approx_kl            | 0.008547794 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.21       |\n","|    explained_variance   | 0.491       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0429      |\n","|    n_updates            | 170         |\n","|    policy_gradient_loss | -0.011      |\n","|    std                  | 0.885       |\n","|    value_loss           | 0.141       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 908          |\n","|    ep_rew_mean          | -95.8        |\n","| time/                   |              |\n","|    fps                  | 652          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 38912        |\n","| train/                  |              |\n","|    approx_kl            | 0.0066083856 |\n","|    clip_fraction        | 0.0613       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.18        |\n","|    explained_variance   | 0.596        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.07         |\n","|    n_updates            | 180          |\n","|    policy_gradient_loss | -0.00834     |\n","|    std                  | 0.882        |\n","|    value_loss           | 14.7         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 943          |\n","|    ep_rew_mean          | -93.7        |\n","| time/                   |              |\n","|    fps                  | 648          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 15           |\n","|    total_timesteps      | 40960        |\n","| train/                  |              |\n","|    approx_kl            | 0.0047463896 |\n","|    clip_fraction        | 0.0341       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.17        |\n","|    explained_variance   | 0.761        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 2.21         |\n","|    n_updates            | 190          |\n","|    policy_gradient_loss | -0.00689     |\n","|    std                  | 0.882        |\n","|    value_loss           | 15.3         |\n","------------------------------------------\n","BipedalWalker - Step: 30001 | Mean Reward: -24.49 ± 5.25\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 899      |\n","|    ep_rew_mean     | -93.3    |\n","| time/              |          |\n","|    fps             | 741      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 43008    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 914          |\n","|    ep_rew_mean          | -92.6        |\n","| time/                   |              |\n","|    fps                  | 694          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 45056        |\n","| train/                  |              |\n","|    approx_kl            | 0.0023907083 |\n","|    clip_fraction        | 0.00879      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.1         |\n","|    explained_variance   | 0.775        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 12.6         |\n","|    n_updates            | 210          |\n","|    policy_gradient_loss | -0.00251     |\n","|    std                  | 0.867        |\n","|    value_loss           | 32.6         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 929         |\n","|    ep_rew_mean          | -91.7       |\n","| time/                   |             |\n","|    fps                  | 683         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 47104       |\n","| train/                  |             |\n","|    approx_kl            | 0.009365766 |\n","|    clip_fraction        | 0.111       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.06       |\n","|    explained_variance   | 0.36        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0332      |\n","|    n_updates            | 220         |\n","|    policy_gradient_loss | -0.0105     |\n","|    std                  | 0.852       |\n","|    value_loss           | 0.104       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 944         |\n","|    ep_rew_mean          | -90.7       |\n","| time/                   |             |\n","|    fps                  | 668         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 49152       |\n","| train/                  |             |\n","|    approx_kl            | 0.008616367 |\n","|    clip_fraction        | 0.144       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.03       |\n","|    explained_variance   | 0.567       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0109      |\n","|    n_updates            | 230         |\n","|    policy_gradient_loss | -0.0108     |\n","|    std                  | 0.849       |\n","|    value_loss           | 0.0937      |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 952         |\n","|    ep_rew_mean          | -89.1       |\n","| time/                   |             |\n","|    fps                  | 625         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 16          |\n","|    total_timesteps      | 51200       |\n","| train/                  |             |\n","|    approx_kl            | 0.007838013 |\n","|    clip_fraction        | 0.132       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5          |\n","|    explained_variance   | 0.738       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.043       |\n","|    n_updates            | 240         |\n","|    policy_gradient_loss | -0.0111     |\n","|    std                  | 0.841       |\n","|    value_loss           | 0.119       |\n","-----------------------------------------\n","BipedalWalker - Step: 40001 | Mean Reward: -25.29 ± 8.31\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 965      |\n","|    ep_rew_mean     | -88.2    |\n","| time/              |          |\n","|    fps             | 999      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 53248    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 977          |\n","|    ep_rew_mean          | -87.1        |\n","| time/                   |              |\n","|    fps                  | 785          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 55296        |\n","| train/                  |              |\n","|    approx_kl            | 0.0076210406 |\n","|    clip_fraction        | 0.123        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.95        |\n","|    explained_variance   | 0.746        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0331       |\n","|    n_updates            | 260          |\n","|    policy_gradient_loss | -0.012       |\n","|    std                  | 0.831        |\n","|    value_loss           | 0.111        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 989          |\n","|    ep_rew_mean          | -85.9        |\n","| time/                   |              |\n","|    fps                  | 713          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 57344        |\n","| train/                  |              |\n","|    approx_kl            | 0.0085129915 |\n","|    clip_fraction        | 0.106        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.9         |\n","|    explained_variance   | 0.743        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0322       |\n","|    n_updates            | 270          |\n","|    policy_gradient_loss | -0.00928     |\n","|    std                  | 0.819        |\n","|    value_loss           | 0.137        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.01e+03    |\n","|    ep_rew_mean          | -83.6       |\n","| time/                   |             |\n","|    fps                  | 650         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 59392       |\n","| train/                  |             |\n","|    approx_kl            | 0.008893903 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.86       |\n","|    explained_variance   | 0.756       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0467      |\n","|    n_updates            | 280         |\n","|    policy_gradient_loss | -0.011      |\n","|    std                  | 0.814       |\n","|    value_loss           | 0.124       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.02e+03    |\n","|    ep_rew_mean          | -82.4       |\n","| time/                   |             |\n","|    fps                  | 651         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 61440       |\n","| train/                  |             |\n","|    approx_kl            | 0.008678272 |\n","|    clip_fraction        | 0.133       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.83       |\n","|    explained_variance   | 0.764       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0668      |\n","|    n_updates            | 290         |\n","|    policy_gradient_loss | -0.0105     |\n","|    std                  | 0.808       |\n","|    value_loss           | 0.184       |\n","-----------------------------------------\n","BipedalWalker - Step: 50001 | Mean Reward: 98.08 ± 2.08\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.02e+03 |\n","|    ep_rew_mean     | -81.8    |\n","| time/              |          |\n","|    fps             | 977      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 63488    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.03e+03    |\n","|    ep_rew_mean          | -80.4       |\n","| time/                   |             |\n","|    fps                  | 761         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 65536       |\n","| train/                  |             |\n","|    approx_kl            | 0.006975518 |\n","|    clip_fraction        | 0.0757      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.83       |\n","|    explained_variance   | -0.327      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.52        |\n","|    n_updates            | 310         |\n","|    policy_gradient_loss | -0.00478    |\n","|    std                  | 0.81        |\n","|    value_loss           | 42.3        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.03e+03    |\n","|    ep_rew_mean          | -79         |\n","| time/                   |             |\n","|    fps                  | 655         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 67584       |\n","| train/                  |             |\n","|    approx_kl            | 0.009431392 |\n","|    clip_fraction        | 0.124       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.82       |\n","|    explained_variance   | 0.37        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0751      |\n","|    n_updates            | 320         |\n","|    policy_gradient_loss | -0.00832    |\n","|    std                  | 0.804       |\n","|    value_loss           | 0.313       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.05e+03    |\n","|    ep_rew_mean          | -75.9       |\n","| time/                   |             |\n","|    fps                  | 643         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 69632       |\n","| train/                  |             |\n","|    approx_kl            | 0.007839723 |\n","|    clip_fraction        | 0.128       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.77       |\n","|    explained_variance   | 0.765       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.107       |\n","|    n_updates            | 330         |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.795       |\n","|    value_loss           | 0.233       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.06e+03     |\n","|    ep_rew_mean          | -74.5        |\n","| time/                   |              |\n","|    fps                  | 647          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 15           |\n","|    total_timesteps      | 71680        |\n","| train/                  |              |\n","|    approx_kl            | 0.0100989975 |\n","|    clip_fraction        | 0.135        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.73        |\n","|    explained_variance   | 0.43         |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.142        |\n","|    n_updates            | 340          |\n","|    policy_gradient_loss | -0.0098      |\n","|    std                  | 0.785        |\n","|    value_loss           | 0.284        |\n","------------------------------------------\n","BipedalWalker - Step: 60001 | Mean Reward: 98.19 ± 5.54\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.07e+03 |\n","|    ep_rew_mean     | -72.6    |\n","| time/              |          |\n","|    fps             | 822      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 73728    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.08e+03    |\n","|    ep_rew_mean          | -70.7       |\n","| time/                   |             |\n","|    fps                  | 650         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 75776       |\n","| train/                  |             |\n","|    approx_kl            | 0.008865774 |\n","|    clip_fraction        | 0.142       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.62       |\n","|    explained_variance   | 0.847       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.113       |\n","|    n_updates            | 360         |\n","|    policy_gradient_loss | -0.012      |\n","|    std                  | 0.766       |\n","|    value_loss           | 0.317       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.09e+03    |\n","|    ep_rew_mean          | -69.1       |\n","| time/                   |             |\n","|    fps                  | 652         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 77824       |\n","| train/                  |             |\n","|    approx_kl            | 0.007860766 |\n","|    clip_fraction        | 0.119       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.58       |\n","|    explained_variance   | 0.87        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.117       |\n","|    n_updates            | 370         |\n","|    policy_gradient_loss | -0.0104     |\n","|    std                  | 0.759       |\n","|    value_loss           | 0.291       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.1e+03     |\n","|    ep_rew_mean          | -65.1       |\n","| time/                   |             |\n","|    fps                  | 642         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 79872       |\n","| train/                  |             |\n","|    approx_kl            | 0.012118908 |\n","|    clip_fraction        | 0.147       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.54       |\n","|    explained_variance   | 0.833       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.105       |\n","|    n_updates            | 380         |\n","|    policy_gradient_loss | -0.0142     |\n","|    std                  | 0.75        |\n","|    value_loss           | 0.463       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.09e+03    |\n","|    ep_rew_mean          | -65.7       |\n","| time/                   |             |\n","|    fps                  | 626         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 16          |\n","|    total_timesteps      | 81920       |\n","| train/                  |             |\n","|    approx_kl            | 0.009768564 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.49       |\n","|    explained_variance   | 0.865       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.115       |\n","|    n_updates            | 390         |\n","|    policy_gradient_loss | -0.0127     |\n","|    std                  | 0.74        |\n","|    value_loss           | 0.302       |\n","-----------------------------------------\n","BipedalWalker - Step: 70001 | Mean Reward: 103.95 ± 4.75\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.09e+03 |\n","|    ep_rew_mean     | -63.6    |\n","| time/              |          |\n","|    fps             | 763      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 83968    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.1e+03     |\n","|    ep_rew_mean          | -61.7       |\n","| time/                   |             |\n","|    fps                  | 595         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 86016       |\n","| train/                  |             |\n","|    approx_kl            | 0.009973652 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.44       |\n","|    explained_variance   | 0.483       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.14        |\n","|    n_updates            | 410         |\n","|    policy_gradient_loss | -0.00843    |\n","|    std                  | 0.731       |\n","|    value_loss           | 0.649       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.11e+03     |\n","|    ep_rew_mean          | -59.9        |\n","| time/                   |              |\n","|    fps                  | 614          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 10           |\n","|    total_timesteps      | 88064        |\n","| train/                  |              |\n","|    approx_kl            | 0.0073018973 |\n","|    clip_fraction        | 0.12         |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.4         |\n","|    explained_variance   | 0.855        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.257        |\n","|    n_updates            | 420          |\n","|    policy_gradient_loss | -0.0068      |\n","|    std                  | 0.726        |\n","|    value_loss           | 0.38         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.11e+03    |\n","|    ep_rew_mean          | -56.7       |\n","| time/                   |             |\n","|    fps                  | 577         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 90112       |\n","| train/                  |             |\n","|    approx_kl            | 0.008480888 |\n","|    clip_fraction        | 0.138       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.36       |\n","|    explained_variance   | 0.878       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0892      |\n","|    n_updates            | 430         |\n","|    policy_gradient_loss | -0.0106     |\n","|    std                  | 0.716       |\n","|    value_loss           | 0.313       |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.11e+03   |\n","|    ep_rew_mean          | -54.4      |\n","| time/                   |            |\n","|    fps                  | 593        |\n","|    iterations           | 5          |\n","|    time_elapsed         | 17         |\n","|    total_timesteps      | 92160      |\n","| train/                  |            |\n","|    approx_kl            | 0.00752843 |\n","|    clip_fraction        | 0.11       |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -4.33      |\n","|    explained_variance   | 0.104      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.417      |\n","|    n_updates            | 440        |\n","|    policy_gradient_loss | -0.00506   |\n","|    std                  | 0.716      |\n","|    value_loss           | 36.5       |\n","----------------------------------------\n","BipedalWalker - Step: 80001 | Mean Reward: 137.56 ± 75.28\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.12e+03 |\n","|    ep_rew_mean     | -52.3    |\n","| time/              |          |\n","|    fps             | 963      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 94208    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.12e+03    |\n","|    ep_rew_mean          | -49.8       |\n","| time/                   |             |\n","|    fps                  | 780         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 96256       |\n","| train/                  |             |\n","|    approx_kl            | 0.012363572 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.31       |\n","|    explained_variance   | 0.885       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.176       |\n","|    n_updates            | 460         |\n","|    policy_gradient_loss | -0.0163     |\n","|    std                  | 0.713       |\n","|    value_loss           | 0.504       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.11e+03     |\n","|    ep_rew_mean          | -50          |\n","| time/                   |              |\n","|    fps                  | 681          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 9            |\n","|    total_timesteps      | 98304        |\n","| train/                  |              |\n","|    approx_kl            | 0.0074714604 |\n","|    clip_fraction        | 0.0888       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.31        |\n","|    explained_variance   | 0.808        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.99         |\n","|    n_updates            | 470          |\n","|    policy_gradient_loss | -0.00916     |\n","|    std                  | 0.713        |\n","|    value_loss           | 10.1         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.09e+03     |\n","|    ep_rew_mean          | -50.4        |\n","| time/                   |              |\n","|    fps                  | 645          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 100352       |\n","| train/                  |              |\n","|    approx_kl            | 0.0044258507 |\n","|    clip_fraction        | 0.057        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.31        |\n","|    explained_variance   | 0.533        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 15.1         |\n","|    n_updates            | 480          |\n","|    policy_gradient_loss | -0.00647     |\n","|    std                  | 0.713        |\n","|    value_loss           | 56           |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.08e+03     |\n","|    ep_rew_mean          | -51.1        |\n","| time/                   |              |\n","|    fps                  | 647          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 15           |\n","|    total_timesteps      | 102400       |\n","| train/                  |              |\n","|    approx_kl            | 0.0044107316 |\n","|    clip_fraction        | 0.0342       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.32        |\n","|    explained_variance   | 0.658        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 24.3         |\n","|    n_updates            | 490          |\n","|    policy_gradient_loss | -0.00801     |\n","|    std                  | 0.713        |\n","|    value_loss           | 91.8         |\n","------------------------------------------\n","BipedalWalker - Step: 90001 | Mean Reward: 53.92 ± 110.28\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.06e+03 |\n","|    ep_rew_mean     | -52.1    |\n","| time/              |          |\n","|    fps             | 1003     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 104448   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.05e+03     |\n","|    ep_rew_mean          | -52          |\n","| time/                   |              |\n","|    fps                  | 785          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 106496       |\n","| train/                  |              |\n","|    approx_kl            | 0.0034728898 |\n","|    clip_fraction        | 0.0332       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.31        |\n","|    explained_variance   | 0.664        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 64.3         |\n","|    n_updates            | 510          |\n","|    policy_gradient_loss | -0.0045      |\n","|    std                  | 0.712        |\n","|    value_loss           | 76.1         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.05e+03     |\n","|    ep_rew_mean          | -51.2        |\n","| time/                   |              |\n","|    fps                  | 712          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 108544       |\n","| train/                  |              |\n","|    approx_kl            | 0.0062382882 |\n","|    clip_fraction        | 0.0718       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.31        |\n","|    explained_variance   | 0.873        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 13.4         |\n","|    n_updates            | 520          |\n","|    policy_gradient_loss | -0.00826     |\n","|    std                  | 0.713        |\n","|    value_loss           | 28.1         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.04e+03    |\n","|    ep_rew_mean          | -50         |\n","| time/                   |             |\n","|    fps                  | 642         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 110592      |\n","| train/                  |             |\n","|    approx_kl            | 0.005352404 |\n","|    clip_fraction        | 0.0645      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.31       |\n","|    explained_variance   | 0.916       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.03        |\n","|    n_updates            | 530         |\n","|    policy_gradient_loss | -0.0091     |\n","|    std                  | 0.713       |\n","|    value_loss           | 22.4        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.05e+03    |\n","|    ep_rew_mean          | -48.8       |\n","| time/                   |             |\n","|    fps                  | 634         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 16          |\n","|    total_timesteps      | 112640      |\n","| train/                  |             |\n","|    approx_kl            | 0.007322883 |\n","|    clip_fraction        | 0.075       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.3        |\n","|    explained_variance   | 0.882       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 25.6        |\n","|    n_updates            | 540         |\n","|    policy_gradient_loss | -0.00793    |\n","|    std                  | 0.71        |\n","|    value_loss           | 30.5        |\n","-----------------------------------------\n","BipedalWalker - Step: 100001 | Mean Reward: 120.34 ± 103.88\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.06e+03 |\n","|    ep_rew_mean     | -46.3    |\n","| time/              |          |\n","|    fps             | 1030     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 1        |\n","|    total_timesteps | 114688   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.06e+03    |\n","|    ep_rew_mean          | -45.1       |\n","| time/                   |             |\n","|    fps                  | 795         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 116736      |\n","| train/                  |             |\n","|    approx_kl            | 0.015505556 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.27       |\n","|    explained_variance   | -0.969      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.848       |\n","|    n_updates            | 560         |\n","|    policy_gradient_loss | -0.00919    |\n","|    std                  | 0.7         |\n","|    value_loss           | 3.35        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.08e+03    |\n","|    ep_rew_mean          | -43         |\n","| time/                   |             |\n","|    fps                  | 673         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 118784      |\n","| train/                  |             |\n","|    approx_kl            | 0.008194756 |\n","|    clip_fraction        | 0.103       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.24       |\n","|    explained_variance   | 0.784       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.89        |\n","|    n_updates            | 570         |\n","|    policy_gradient_loss | -0.00672    |\n","|    std                  | 0.701       |\n","|    value_loss           | 39.6        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.09e+03    |\n","|    ep_rew_mean          | -40         |\n","| time/                   |             |\n","|    fps                  | 646         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 120832      |\n","| train/                  |             |\n","|    approx_kl            | 0.013092913 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.23       |\n","|    explained_variance   | 0.51        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.49        |\n","|    n_updates            | 580         |\n","|    policy_gradient_loss | -0.00538    |\n","|    std                  | 0.695       |\n","|    value_loss           | 3.89        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.09e+03   |\n","|    ep_rew_mean          | -37.8      |\n","| time/                   |            |\n","|    fps                  | 644        |\n","|    iterations           | 5          |\n","|    time_elapsed         | 15         |\n","|    total_timesteps      | 122880     |\n","| train/                  |            |\n","|    approx_kl            | 0.01353801 |\n","|    clip_fraction        | 0.184      |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -4.2       |\n","|    explained_variance   | 0.882      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 13         |\n","|    n_updates            | 590        |\n","|    policy_gradient_loss | -0.00858   |\n","|    std                  | 0.692      |\n","|    value_loss           | 14.6       |\n","----------------------------------------\n","BipedalWalker - Step: 110001 | Mean Reward: 141.83 ± 4.83\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.1e+03  |\n","|    ep_rew_mean     | -35.1    |\n","| time/              |          |\n","|    fps             | 1021     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 124928   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.12e+03    |\n","|    ep_rew_mean          | -33.9       |\n","| time/                   |             |\n","|    fps                  | 657         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 126976      |\n","| train/                  |             |\n","|    approx_kl            | 0.007468201 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.14       |\n","|    explained_variance   | 0.909       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.02        |\n","|    n_updates            | 610         |\n","|    policy_gradient_loss | -0.00587    |\n","|    std                  | 0.682       |\n","|    value_loss           | 7.85        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.12e+03     |\n","|    ep_rew_mean          | -33          |\n","| time/                   |              |\n","|    fps                  | 642          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 9            |\n","|    total_timesteps      | 129024       |\n","| train/                  |              |\n","|    approx_kl            | 0.0069129523 |\n","|    clip_fraction        | 0.113        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.13        |\n","|    explained_variance   | 0.912        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.955        |\n","|    n_updates            | 620          |\n","|    policy_gradient_loss | -0.00609     |\n","|    std                  | 0.682        |\n","|    value_loss           | 7.3          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.13e+03    |\n","|    ep_rew_mean          | -28.3       |\n","| time/                   |             |\n","|    fps                  | 643         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 131072      |\n","| train/                  |             |\n","|    approx_kl            | 0.008063659 |\n","|    clip_fraction        | 0.133       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.13       |\n","|    explained_variance   | 0.928       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 15.8        |\n","|    n_updates            | 630         |\n","|    policy_gradient_loss | -0.0123     |\n","|    std                  | 0.681       |\n","|    value_loss           | 11.5        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.1e+03     |\n","|    ep_rew_mean          | -26.8       |\n","| time/                   |             |\n","|    fps                  | 647         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 133120      |\n","| train/                  |             |\n","|    approx_kl            | 0.016945597 |\n","|    clip_fraction        | 0.211       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.13       |\n","|    explained_variance   | 0.517       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.29        |\n","|    n_updates            | 640         |\n","|    policy_gradient_loss | -0.00652    |\n","|    std                  | 0.681       |\n","|    value_loss           | 2.82        |\n","-----------------------------------------\n","BipedalWalker - Step: 120001 | Mean Reward: 158.32 ± 5.57\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.11e+03 |\n","|    ep_rew_mean     | -24.6    |\n","| time/              |          |\n","|    fps             | 849      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 135168   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.11e+03    |\n","|    ep_rew_mean          | -22.4       |\n","| time/                   |             |\n","|    fps                  | 735         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 137216      |\n","| train/                  |             |\n","|    approx_kl            | 0.007733414 |\n","|    clip_fraction        | 0.106       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.13       |\n","|    explained_variance   | 0.421       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 33.4        |\n","|    n_updates            | 660         |\n","|    policy_gradient_loss | -0.0107     |\n","|    std                  | 0.681       |\n","|    value_loss           | 35.7        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.1e+03     |\n","|    ep_rew_mean          | -21.1       |\n","| time/                   |             |\n","|    fps                  | 708         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 139264      |\n","| train/                  |             |\n","|    approx_kl            | 0.009617369 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.12       |\n","|    explained_variance   | 0.54        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.83        |\n","|    n_updates            | 670         |\n","|    policy_gradient_loss | -0.00982    |\n","|    std                  | 0.68        |\n","|    value_loss           | 15.7        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.1e+03      |\n","|    ep_rew_mean          | -19.1        |\n","| time/                   |              |\n","|    fps                  | 685          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 141312       |\n","| train/                  |              |\n","|    approx_kl            | 0.0097835865 |\n","|    clip_fraction        | 0.133        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.12        |\n","|    explained_variance   | 0.933        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 5.34         |\n","|    n_updates            | 680          |\n","|    policy_gradient_loss | -0.0105      |\n","|    std                  | 0.683        |\n","|    value_loss           | 11.9         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.1e+03     |\n","|    ep_rew_mean          | -17.1       |\n","| time/                   |             |\n","|    fps                  | 641         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 143360      |\n","| train/                  |             |\n","|    approx_kl            | 0.012711648 |\n","|    clip_fraction        | 0.199       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.13       |\n","|    explained_variance   | 0.208       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.214       |\n","|    n_updates            | 690         |\n","|    policy_gradient_loss | -0.00473    |\n","|    std                  | 0.681       |\n","|    value_loss           | 0.738       |\n","-----------------------------------------\n","BipedalWalker - Step: 130001 | Mean Reward: 156.67 ± 11.70\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.08e+03 |\n","|    ep_rew_mean     | -16.8    |\n","| time/              |          |\n","|    fps             | 1023     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 145408   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.08e+03    |\n","|    ep_rew_mean          | -14.6       |\n","| time/                   |             |\n","|    fps                  | 801         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 147456      |\n","| train/                  |             |\n","|    approx_kl            | 0.005862286 |\n","|    clip_fraction        | 0.053       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.1        |\n","|    explained_variance   | 0.825       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.36        |\n","|    n_updates            | 710         |\n","|    policy_gradient_loss | -0.00319    |\n","|    std                  | 0.677       |\n","|    value_loss           | 22.6        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.09e+03   |\n","|    ep_rew_mean          | -11.7      |\n","| time/                   |            |\n","|    fps                  | 707        |\n","|    iterations           | 3          |\n","|    time_elapsed         | 8          |\n","|    total_timesteps      | 149504     |\n","| train/                  |            |\n","|    approx_kl            | 0.01039545 |\n","|    clip_fraction        | 0.122      |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -4.08      |\n","|    explained_variance   | 0.73       |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.242      |\n","|    n_updates            | 720        |\n","|    policy_gradient_loss | -0.00704   |\n","|    std                  | 0.67       |\n","|    value_loss           | 0.89       |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.09e+03    |\n","|    ep_rew_mean          | -9.8        |\n","| time/                   |             |\n","|    fps                  | 639         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 151552      |\n","| train/                  |             |\n","|    approx_kl            | 0.008157533 |\n","|    clip_fraction        | 0.0832      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.06       |\n","|    explained_variance   | 0.787       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.32        |\n","|    n_updates            | 730         |\n","|    policy_gradient_loss | -0.00785    |\n","|    std                  | 0.671       |\n","|    value_loss           | 20.8        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.09e+03    |\n","|    ep_rew_mean          | -8.03       |\n","| time/                   |             |\n","|    fps                  | 644         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 153600      |\n","| train/                  |             |\n","|    approx_kl            | 0.011040639 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.04       |\n","|    explained_variance   | 0.779       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.407       |\n","|    n_updates            | 740         |\n","|    policy_gradient_loss | -0.00847    |\n","|    std                  | 0.664       |\n","|    value_loss           | 0.773       |\n","-----------------------------------------\n","BipedalWalker - Step: 140001 | Mean Reward: 199.51 ± 7.52\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.09e+03 |\n","|    ep_rew_mean     | -6.8     |\n","| time/              |          |\n","|    fps             | 981      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 155648   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.11e+03    |\n","|    ep_rew_mean          | -4.1        |\n","| time/                   |             |\n","|    fps                  | 686         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 157696      |\n","| train/                  |             |\n","|    approx_kl            | 0.005754018 |\n","|    clip_fraction        | 0.0755      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.99       |\n","|    explained_variance   | 0.846       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 18.9        |\n","|    n_updates            | 760         |\n","|    policy_gradient_loss | -0.00554    |\n","|    std                  | 0.657       |\n","|    value_loss           | 21.6        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.11e+03    |\n","|    ep_rew_mean          | -2.23       |\n","| time/                   |             |\n","|    fps                  | 644         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 159744      |\n","| train/                  |             |\n","|    approx_kl            | 0.008956382 |\n","|    clip_fraction        | 0.128       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.99       |\n","|    explained_variance   | 0.692       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.692       |\n","|    n_updates            | 770         |\n","|    policy_gradient_loss | -0.00838    |\n","|    std                  | 0.659       |\n","|    value_loss           | 1.52        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.12e+03    |\n","|    ep_rew_mean          | -0.233      |\n","| time/                   |             |\n","|    fps                  | 648         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 161792      |\n","| train/                  |             |\n","|    approx_kl            | 0.014241472 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.96       |\n","|    explained_variance   | 0.783       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.204       |\n","|    n_updates            | 780         |\n","|    policy_gradient_loss | -0.0107     |\n","|    std                  | 0.65        |\n","|    value_loss           | 0.374       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.12e+03     |\n","|    ep_rew_mean          | 1.72         |\n","| time/                   |              |\n","|    fps                  | 648          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 15           |\n","|    total_timesteps      | 163840       |\n","| train/                  |              |\n","|    approx_kl            | 0.0035217067 |\n","|    clip_fraction        | 0.0433       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -3.94        |\n","|    explained_variance   | 0.864        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 7.32         |\n","|    n_updates            | 790          |\n","|    policy_gradient_loss | -0.00398     |\n","|    std                  | 0.649        |\n","|    value_loss           | 31.5         |\n","------------------------------------------\n","BipedalWalker - Step: 150001 | Mean Reward: 185.19 ± 5.68\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.12e+03 |\n","|    ep_rew_mean     | 3.85     |\n","| time/              |          |\n","|    fps             | 736      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 165888   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.11e+03     |\n","|    ep_rew_mean          | 5.59         |\n","| time/                   |              |\n","|    fps                  | 621          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 6            |\n","|    total_timesteps      | 167936       |\n","| train/                  |              |\n","|    approx_kl            | 0.0140539985 |\n","|    clip_fraction        | 0.185        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -3.9         |\n","|    explained_variance   | 0.182        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.348        |\n","|    n_updates            | 810          |\n","|    policy_gradient_loss | -0.00865     |\n","|    std                  | 0.638        |\n","|    value_loss           | 1.1          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.1e+03     |\n","|    ep_rew_mean          | 6.02        |\n","| time/                   |             |\n","|    fps                  | 635         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 169984      |\n","| train/                  |             |\n","|    approx_kl            | 0.010179529 |\n","|    clip_fraction        | 0.137       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.88       |\n","|    explained_variance   | 0.891       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.752       |\n","|    n_updates            | 820         |\n","|    policy_gradient_loss | -0.0056     |\n","|    std                  | 0.642       |\n","|    value_loss           | 3.13        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.12e+03     |\n","|    ep_rew_mean          | 10.3         |\n","| time/                   |              |\n","|    fps                  | 637          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 172032       |\n","| train/                  |              |\n","|    approx_kl            | 0.0062909974 |\n","|    clip_fraction        | 0.0851       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -3.89        |\n","|    explained_variance   | 0.856        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 4.8          |\n","|    n_updates            | 830          |\n","|    policy_gradient_loss | -0.00678     |\n","|    std                  | 0.641        |\n","|    value_loss           | 13.8         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.12e+03    |\n","|    ep_rew_mean          | 12          |\n","| time/                   |             |\n","|    fps                  | 607         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 16          |\n","|    total_timesteps      | 174080      |\n","| train/                  |             |\n","|    approx_kl            | 0.013745001 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.88       |\n","|    explained_variance   | 0.459       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.137       |\n","|    n_updates            | 840         |\n","|    policy_gradient_loss | -0.00646    |\n","|    std                  | 0.639       |\n","|    value_loss           | 0.458       |\n","-----------------------------------------\n","BipedalWalker - Step: 160001 | Mean Reward: 193.99 ± 8.44\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.12e+03 |\n","|    ep_rew_mean     | 13.9     |\n","| time/              |          |\n","|    fps             | 1020     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 176128   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.12e+03    |\n","|    ep_rew_mean          | 15.8        |\n","| time/                   |             |\n","|    fps                  | 791         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 178176      |\n","| train/                  |             |\n","|    approx_kl            | 0.012525799 |\n","|    clip_fraction        | 0.174       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.83       |\n","|    explained_variance   | 0.903       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.373       |\n","|    n_updates            | 860         |\n","|    policy_gradient_loss | -0.0114     |\n","|    std                  | 0.63        |\n","|    value_loss           | 0.288       |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.12e+03   |\n","|    ep_rew_mean          | 17.7       |\n","| time/                   |            |\n","|    fps                  | 735        |\n","|    iterations           | 3          |\n","|    time_elapsed         | 8          |\n","|    total_timesteps      | 180224     |\n","| train/                  |            |\n","|    approx_kl            | 0.01185041 |\n","|    clip_fraction        | 0.137      |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -3.78      |\n","|    explained_variance   | 0.873      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.0838     |\n","|    n_updates            | 870        |\n","|    policy_gradient_loss | -0.00703   |\n","|    std                  | 0.62       |\n","|    value_loss           | 0.259      |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.12e+03    |\n","|    ep_rew_mean          | 20.9        |\n","| time/                   |             |\n","|    fps                  | 646         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 182272      |\n","| train/                  |             |\n","|    approx_kl            | 0.008882396 |\n","|    clip_fraction        | 0.152       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.73       |\n","|    explained_variance   | 0.925       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0686      |\n","|    n_updates            | 880         |\n","|    policy_gradient_loss | -0.0107     |\n","|    std                  | 0.615       |\n","|    value_loss           | 0.221       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.11e+03    |\n","|    ep_rew_mean          | 20.8        |\n","| time/                   |             |\n","|    fps                  | 647         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 184320      |\n","| train/                  |             |\n","|    approx_kl            | 0.007598784 |\n","|    clip_fraction        | 0.103       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.71       |\n","|    explained_variance   | 0.736       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.285       |\n","|    n_updates            | 890         |\n","|    policy_gradient_loss | -0.00846    |\n","|    std                  | 0.614       |\n","|    value_loss           | 0.838       |\n","-----------------------------------------\n","BipedalWalker - Step: 170001 | Mean Reward: 129.99 ± 130.46\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.12e+03 |\n","|    ep_rew_mean     | 21.7     |\n","| time/              |          |\n","|    fps             | 1003     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 186368   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.08e+03    |\n","|    ep_rew_mean          | 21.3        |\n","| time/                   |             |\n","|    fps                  | 792         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 188416      |\n","| train/                  |             |\n","|    approx_kl            | 0.005992286 |\n","|    clip_fraction        | 0.0693      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.71       |\n","|    explained_variance   | 0.815       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 33.9        |\n","|    n_updates            | 910         |\n","|    policy_gradient_loss | -0.00364    |\n","|    std                  | 0.613       |\n","|    value_loss           | 22.3        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.08e+03    |\n","|    ep_rew_mean          | 22.8        |\n","| time/                   |             |\n","|    fps                  | 737         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 190464      |\n","| train/                  |             |\n","|    approx_kl            | 0.004159895 |\n","|    clip_fraction        | 0.03        |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.7        |\n","|    explained_variance   | 0.947       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 10.7        |\n","|    n_updates            | 920         |\n","|    policy_gradient_loss | -0.00396    |\n","|    std                  | 0.612       |\n","|    value_loss           | 22.1        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.08e+03    |\n","|    ep_rew_mean          | 23.8        |\n","| time/                   |             |\n","|    fps                  | 657         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 192512      |\n","| train/                  |             |\n","|    approx_kl            | 0.011952846 |\n","|    clip_fraction        | 0.144       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.69       |\n","|    explained_variance   | 0.404       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.295       |\n","|    n_updates            | 930         |\n","|    policy_gradient_loss | -0.00605    |\n","|    std                  | 0.608       |\n","|    value_loss           | 0.755       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.08e+03    |\n","|    ep_rew_mean          | 24.8        |\n","| time/                   |             |\n","|    fps                  | 650         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 194560      |\n","| train/                  |             |\n","|    approx_kl            | 0.009376079 |\n","|    clip_fraction        | 0.151       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.66       |\n","|    explained_variance   | 0.675       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.228       |\n","|    n_updates            | 940         |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.603       |\n","|    value_loss           | 0.661       |\n","-----------------------------------------\n","BipedalWalker - Step: 180001 | Mean Reward: 140.52 ± 101.76\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.05e+03 |\n","|    ep_rew_mean     | 23.8     |\n","| time/              |          |\n","|    fps             | 1025     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 1        |\n","|    total_timesteps | 196608   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.07e+03    |\n","|    ep_rew_mean          | 26.7        |\n","| time/                   |             |\n","|    fps                  | 778         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 198656      |\n","| train/                  |             |\n","|    approx_kl            | 0.006078697 |\n","|    clip_fraction        | 0.0938      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.63       |\n","|    explained_variance   | 0.916       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.14        |\n","|    n_updates            | 960         |\n","|    policy_gradient_loss | -0.00867    |\n","|    std                  | 0.601       |\n","|    value_loss           | 13.5        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.07e+03   |\n","|    ep_rew_mean          | 27.7       |\n","| time/                   |            |\n","|    fps                  | 668        |\n","|    iterations           | 3          |\n","|    time_elapsed         | 9          |\n","|    total_timesteps      | 200704     |\n","| train/                  |            |\n","|    approx_kl            | 0.01182442 |\n","|    clip_fraction        | 0.174      |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -3.63      |\n","|    explained_variance   | 0.594      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.233      |\n","|    n_updates            | 970        |\n","|    policy_gradient_loss | -0.00892   |\n","|    std                  | 0.6        |\n","|    value_loss           | 0.452      |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.06e+03    |\n","|    ep_rew_mean          | 27.4        |\n","| time/                   |             |\n","|    fps                  | 641         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 202752      |\n","| train/                  |             |\n","|    approx_kl            | 0.011828963 |\n","|    clip_fraction        | 0.169       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.6        |\n","|    explained_variance   | 0.876       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.108       |\n","|    n_updates            | 980         |\n","|    policy_gradient_loss | -0.0108     |\n","|    std                  | 0.594       |\n","|    value_loss           | 0.279       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.03e+03    |\n","|    ep_rew_mean          | 24.7        |\n","| time/                   |             |\n","|    fps                  | 645         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 204800      |\n","| train/                  |             |\n","|    approx_kl            | 0.005857898 |\n","|    clip_fraction        | 0.0887      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.58       |\n","|    explained_variance   | 0.781       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.79        |\n","|    n_updates            | 990         |\n","|    policy_gradient_loss | -0.00411    |\n","|    std                  | 0.593       |\n","|    value_loss           | 19.4        |\n","-----------------------------------------\n","BipedalWalker - Step: 190001 | Mean Reward: 152.71 ± 114.14\n"]}]},{"cell_type":"markdown","source":["# 3. PPO Bipedal\n","env = 1, batch size= 128, lr = 1e-4"],"metadata":{"id":"5qIYkdH8Bk8l"}},{"cell_type":"code","source":["def train_bipedalwalker3():\n","    # Create environment and model for BipedalWalker\n","    env_bipedal = make_vec_env(\"BipedalWalker-v3\", n_envs=1)\n","    model_b = PPO(\"MlpPolicy\", env_bipedal, verbose=1, tensorboard_log=\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/\",\n","                n_steps=2048, batch_size=128, gae_lambda=0.95, gamma=0.99,\n","                n_epochs=10, ent_coef=0.0, learning_rate=1e-4, clip_range=0.18)\n","\n","    eval_freq = 10000  # Evaluate every 10,000 steps\n","    n_eval_episodes = 10  # Number of episodes per evaluation\n","\n","    for step in range(1, 200001, eval_freq):\n","        # Train the model\n","        model_b.learn(total_timesteps=eval_freq, reset_num_timesteps=False)\n","\n","        # Evaluate the model on BipedalWalker\n","        mean_reward, std_reward = evaluate_model(model_b, env_bipedal, n_eval_episodes)\n","\n","        # Print evaluation results for BipedalWalker\n","        print(f\"BipedalWalker - Step: {step} | Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n","\n","    # Save the trained model for BipedalWalker\n","    model_b.save(\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4\")"],"metadata":{"id":"FxZMFEmrBwjx","executionInfo":{"status":"ok","timestamp":1740512163172,"user_tz":-120,"elapsed":42,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    train_bipedalwalker3()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGIrxCFlCCNg","executionInfo":{"status":"ok","timestamp":1740512678592,"user_tz":-120,"elapsed":515414,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}},"outputId":"d45bd0b5-7acc-4e06-e03a-a3be9f0c6a0e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 74.3     |\n","|    ep_rew_mean     | -108     |\n","| time/              |          |\n","|    fps             | 969      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 2048     |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 200          |\n","|    ep_rew_mean          | -110         |\n","| time/                   |              |\n","|    fps                  | 861          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 4            |\n","|    total_timesteps      | 4096         |\n","| train/                  |              |\n","|    approx_kl            | 0.0053641996 |\n","|    clip_fraction        | 0.0362       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.68        |\n","|    explained_variance   | 0.00271      |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 170          |\n","|    n_updates            | 10           |\n","|    policy_gradient_loss | -0.00618     |\n","|    std                  | 1            |\n","|    value_loss           | 470          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 255         |\n","|    ep_rew_mean          | -110        |\n","| time/                   |             |\n","|    fps                  | 752         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.003195322 |\n","|    clip_fraction        | 0.0227      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.68       |\n","|    explained_variance   | 0.00618     |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 53.1        |\n","|    n_updates            | 20          |\n","|    policy_gradient_loss | -0.00656    |\n","|    std                  | 1           |\n","|    value_loss           | 122         |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 368          |\n","|    ep_rew_mean          | -110         |\n","| time/                   |              |\n","|    fps                  | 735          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 8192         |\n","| train/                  |              |\n","|    approx_kl            | 0.0030444546 |\n","|    clip_fraction        | 0.00693      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.69        |\n","|    explained_variance   | 0.0278       |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 76.9         |\n","|    n_updates            | 30           |\n","|    policy_gradient_loss | -0.00397     |\n","|    std                  | 1            |\n","|    value_loss           | 117          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 422         |\n","|    ep_rew_mean          | -110        |\n","| time/                   |             |\n","|    fps                  | 746         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.004775885 |\n","|    clip_fraction        | 0.0282      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.68       |\n","|    explained_variance   | 0.0606      |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 7.43        |\n","|    n_updates            | 40          |\n","|    policy_gradient_loss | -0.0057     |\n","|    std                  | 0.999       |\n","|    value_loss           | 37.2        |\n","-----------------------------------------\n","BipedalWalker - Step: 1 | Mean Reward: -101.89 ± 1.19\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 402      |\n","|    ep_rew_mean     | -109     |\n","| time/              |          |\n","|    fps             | 988      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 12288    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 442          |\n","|    ep_rew_mean          | -109         |\n","| time/                   |              |\n","|    fps                  | 801          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 14336        |\n","| train/                  |              |\n","|    approx_kl            | 8.652752e-05 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.66        |\n","|    explained_variance   | 0.0543       |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 53.6         |\n","|    n_updates            | 60           |\n","|    policy_gradient_loss | -0.000559    |\n","|    std                  | 0.996        |\n","|    value_loss           | 183          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 453         |\n","|    ep_rew_mean          | -109        |\n","| time/                   |             |\n","|    fps                  | 733         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 16384       |\n","| train/                  |             |\n","|    approx_kl            | 0.006992444 |\n","|    clip_fraction        | 0.111       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.65       |\n","|    explained_variance   | -0.104      |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.193       |\n","|    n_updates            | 70          |\n","|    policy_gradient_loss | -0.0124     |\n","|    std                  | 0.991       |\n","|    value_loss           | 0.469       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 475         |\n","|    ep_rew_mean          | -109        |\n","| time/                   |             |\n","|    fps                  | 745         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 18432       |\n","| train/                  |             |\n","|    approx_kl            | 0.000525609 |\n","|    clip_fraction        | 0           |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.64       |\n","|    explained_variance   | 0.0698      |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 38.5        |\n","|    n_updates            | 80          |\n","|    policy_gradient_loss | -0.0014     |\n","|    std                  | 0.991       |\n","|    value_loss           | 73.8        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 524          |\n","|    ep_rew_mean          | -109         |\n","| time/                   |              |\n","|    fps                  | 746          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 20480        |\n","| train/                  |              |\n","|    approx_kl            | 0.0016665439 |\n","|    clip_fraction        | 0.00083      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.64        |\n","|    explained_variance   | 0.149        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 7.27         |\n","|    n_updates            | 90           |\n","|    policy_gradient_loss | -0.00257     |\n","|    std                  | 0.99         |\n","|    value_loss           | 37.9         |\n","------------------------------------------\n","BipedalWalker - Step: 10001 | Mean Reward: -104.85 ± 5.58\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 552      |\n","|    ep_rew_mean     | -109     |\n","| time/              |          |\n","|    fps             | 867      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 22528    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 553          |\n","|    ep_rew_mean          | -109         |\n","| time/                   |              |\n","|    fps                  | 708          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 24576        |\n","| train/                  |              |\n","|    approx_kl            | 0.0048685144 |\n","|    clip_fraction        | 0.0563       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.63        |\n","|    explained_variance   | 0.109        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.316        |\n","|    n_updates            | 110          |\n","|    policy_gradient_loss | -0.00641     |\n","|    std                  | 0.986        |\n","|    value_loss           | 0.824        |\n","------------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 578           |\n","|    ep_rew_mean          | -109          |\n","| time/                   |               |\n","|    fps                  | 722           |\n","|    iterations           | 3             |\n","|    time_elapsed         | 8             |\n","|    total_timesteps      | 26624         |\n","| train/                  |               |\n","|    approx_kl            | 0.00065424386 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.18          |\n","|    entropy_loss         | -5.61         |\n","|    explained_variance   | 0.117         |\n","|    learning_rate        | 0.0001        |\n","|    loss                 | 18.8          |\n","|    n_updates            | 120           |\n","|    policy_gradient_loss | -0.0012       |\n","|    std                  | 0.984         |\n","|    value_loss           | 68.5          |\n","-------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 578          |\n","|    ep_rew_mean          | -109         |\n","| time/                   |              |\n","|    fps                  | 736          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 28672        |\n","| train/                  |              |\n","|    approx_kl            | 0.0073734415 |\n","|    clip_fraction        | 0.081        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.6         |\n","|    explained_variance   | -0.126       |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.343        |\n","|    n_updates            | 130          |\n","|    policy_gradient_loss | -0.0105      |\n","|    std                  | 0.977        |\n","|    value_loss           | 0.805        |\n","------------------------------------------\n","--------------------------------------------\n","| rollout/                |                |\n","|    ep_len_mean          | 621            |\n","|    ep_rew_mean          | -108           |\n","| time/                   |                |\n","|    fps                  | 743            |\n","|    iterations           | 5              |\n","|    time_elapsed         | 13             |\n","|    total_timesteps      | 30720          |\n","| train/                  |                |\n","|    approx_kl            | 0.000113972084 |\n","|    clip_fraction        | 0              |\n","|    clip_range           | 0.18           |\n","|    entropy_loss         | -5.58          |\n","|    explained_variance   | -0.0585        |\n","|    learning_rate        | 0.0001         |\n","|    loss                 | 44             |\n","|    n_updates            | 140            |\n","|    policy_gradient_loss | -0.000924      |\n","|    std                  | 0.976          |\n","|    value_loss           | 67.8           |\n","--------------------------------------------\n","BipedalWalker - Step: 20001 | Mean Reward: -61.71 ± 21.80\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 608      |\n","|    ep_rew_mean     | -108     |\n","| time/              |          |\n","|    fps             | 985      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 32768    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 627          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 751          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 34816        |\n","| train/                  |              |\n","|    approx_kl            | 0.0002442226 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.56        |\n","|    explained_variance   | 0.094        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 86.3         |\n","|    n_updates            | 160          |\n","|    policy_gradient_loss | -0.000636    |\n","|    std                  | 0.973        |\n","|    value_loss           | 98.1         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 634         |\n","|    ep_rew_mean          | -108        |\n","| time/                   |             |\n","|    fps                  | 714         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 36864       |\n","| train/                  |             |\n","|    approx_kl            | 0.005726463 |\n","|    clip_fraction        | 0.0447      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.56       |\n","|    explained_variance   | 0.779       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.076       |\n","|    n_updates            | 170         |\n","|    policy_gradient_loss | -0.00616    |\n","|    std                  | 0.972       |\n","|    value_loss           | 0.279       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 651          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 725          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 38912        |\n","| train/                  |              |\n","|    approx_kl            | 0.0026177047 |\n","|    clip_fraction        | 0.00239      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.56        |\n","|    explained_variance   | 0.132        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 23.4         |\n","|    n_updates            | 180          |\n","|    policy_gradient_loss | -0.00162     |\n","|    std                  | 0.972        |\n","|    value_loss           | 33.8         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 673          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 736          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 40960        |\n","| train/                  |              |\n","|    approx_kl            | 0.0053442474 |\n","|    clip_fraction        | 0.0494       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.55        |\n","|    explained_variance   | -0.507       |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.074        |\n","|    n_updates            | 190          |\n","|    policy_gradient_loss | -0.0067      |\n","|    std                  | 0.966        |\n","|    value_loss           | 0.378        |\n","------------------------------------------\n","BipedalWalker - Step: 30001 | Mean Reward: -105.72 ± 0.03\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 642      |\n","|    ep_rew_mean     | -107     |\n","| time/              |          |\n","|    fps             | 854      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 43008    |\n","---------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 574           |\n","|    ep_rew_mean          | -108          |\n","| time/                   |               |\n","|    fps                  | 704           |\n","|    iterations           | 2             |\n","|    time_elapsed         | 5             |\n","|    total_timesteps      | 45056         |\n","| train/                  |               |\n","|    approx_kl            | 0.00029289516 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.18          |\n","|    entropy_loss         | -5.53         |\n","|    explained_variance   | 0.186         |\n","|    learning_rate        | 0.0001        |\n","|    loss                 | 44.2          |\n","|    n_updates            | 210           |\n","|    policy_gradient_loss | -0.00138      |\n","|    std                  | 0.965         |\n","|    value_loss           | 158           |\n","-------------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 575           |\n","|    ep_rew_mean          | -108          |\n","| time/                   |               |\n","|    fps                  | 731           |\n","|    iterations           | 3             |\n","|    time_elapsed         | 8             |\n","|    total_timesteps      | 47104         |\n","| train/                  |               |\n","|    approx_kl            | 0.00012459583 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.18          |\n","|    entropy_loss         | -5.53         |\n","|    explained_variance   | 0.205         |\n","|    learning_rate        | 0.0001        |\n","|    loss                 | 85.5          |\n","|    n_updates            | 220           |\n","|    policy_gradient_loss | -0.000775     |\n","|    std                  | 0.965         |\n","|    value_loss           | 268           |\n","-------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 576          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 737          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 49152        |\n","| train/                  |              |\n","|    approx_kl            | 0.0011348234 |\n","|    clip_fraction        | 0.000195     |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.53        |\n","|    explained_variance   | 0.29         |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 12.7         |\n","|    n_updates            | 230          |\n","|    policy_gradient_loss | -0.002       |\n","|    std                  | 0.965        |\n","|    value_loss           | 59.4         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 565          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 748          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 51200        |\n","| train/                  |              |\n","|    approx_kl            | 0.0006714858 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.53        |\n","|    explained_variance   | 0.324        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 75.2         |\n","|    n_updates            | 240          |\n","|    policy_gradient_loss | -0.00137     |\n","|    std                  | 0.965        |\n","|    value_loss           | 111          |\n","------------------------------------------\n","BipedalWalker - Step: 40001 | Mean Reward: -106.62 ± 0.11\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 577      |\n","|    ep_rew_mean     | -108     |\n","| time/              |          |\n","|    fps             | 712      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 53248    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 588          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 714          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 55296        |\n","| train/                  |              |\n","|    approx_kl            | 0.0050762612 |\n","|    clip_fraction        | 0.0347       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.53        |\n","|    explained_variance   | 0.897        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.123        |\n","|    n_updates            | 260          |\n","|    policy_gradient_loss | -0.00503     |\n","|    std                  | 0.962        |\n","|    value_loss           | 0.693        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 588         |\n","|    ep_rew_mean          | -108        |\n","| time/                   |             |\n","|    fps                  | 729         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 57344       |\n","| train/                  |             |\n","|    approx_kl            | 0.003917495 |\n","|    clip_fraction        | 0.0272      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.51       |\n","|    explained_variance   | 0.62        |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0527      |\n","|    n_updates            | 270         |\n","|    policy_gradient_loss | -0.0081     |\n","|    std                  | 0.956       |\n","|    value_loss           | 0.232       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 579          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 734          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 59392        |\n","| train/                  |              |\n","|    approx_kl            | 0.0018392553 |\n","|    clip_fraction        | 0.000391     |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.49        |\n","|    explained_variance   | 0.395        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 26.4         |\n","|    n_updates            | 280          |\n","|    policy_gradient_loss | -0.00325     |\n","|    std                  | 0.955        |\n","|    value_loss           | 54.7         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 585          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 722          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 14           |\n","|    total_timesteps      | 61440        |\n","| train/                  |              |\n","|    approx_kl            | 0.0018860946 |\n","|    clip_fraction        | 0.000635     |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.49        |\n","|    explained_variance   | 0.346        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 65.1         |\n","|    n_updates            | 290          |\n","|    policy_gradient_loss | -0.00307     |\n","|    std                  | 0.955        |\n","|    value_loss           | 102          |\n","------------------------------------------\n","BipedalWalker - Step: 50001 | Mean Reward: -64.26 ± 0.18\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 601      |\n","|    ep_rew_mean     | -108     |\n","| time/              |          |\n","|    fps             | 975      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 63488    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 616          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 865          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 4            |\n","|    total_timesteps      | 65536        |\n","| train/                  |              |\n","|    approx_kl            | 0.0032321345 |\n","|    clip_fraction        | 0.0105       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.49        |\n","|    explained_variance   | 0.526        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.359        |\n","|    n_updates            | 310          |\n","|    policy_gradient_loss | -0.00188     |\n","|    std                  | 0.955        |\n","|    value_loss           | 23.5         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 631          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 832          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 7            |\n","|    total_timesteps      | 67584        |\n","| train/                  |              |\n","|    approx_kl            | 0.0029556104 |\n","|    clip_fraction        | 0.0137       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.48        |\n","|    explained_variance   | 0.524        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.129        |\n","|    n_updates            | 320          |\n","|    policy_gradient_loss | -0.00305     |\n","|    std                  | 0.95         |\n","|    value_loss           | 0.277        |\n","------------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 661        |\n","|    ep_rew_mean          | -108       |\n","| time/                   |            |\n","|    fps                  | 802        |\n","|    iterations           | 4          |\n","|    time_elapsed         | 10         |\n","|    total_timesteps      | 69632      |\n","| train/                  |            |\n","|    approx_kl            | 0.00525764 |\n","|    clip_fraction        | 0.0588     |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -5.46      |\n","|    explained_variance   | -0.964     |\n","|    learning_rate        | 0.0001     |\n","|    loss                 | 0.0294     |\n","|    n_updates            | 330        |\n","|    policy_gradient_loss | -0.00723   |\n","|    std                  | 0.943      |\n","|    value_loss           | 0.145      |\n","----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 658          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 745          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 71680        |\n","| train/                  |              |\n","|    approx_kl            | 0.0032028623 |\n","|    clip_fraction        | 0.0062       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.44        |\n","|    explained_variance   | 0.451        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 19.6         |\n","|    n_updates            | 340          |\n","|    policy_gradient_loss | -0.00303     |\n","|    std                  | 0.943        |\n","|    value_loss           | 26.9         |\n","------------------------------------------\n","BipedalWalker - Step: 60001 | Mean Reward: -68.84 ± 0.74\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 673      |\n","|    ep_rew_mean     | -107     |\n","| time/              |          |\n","|    fps             | 976      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 73728    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 674         |\n","|    ep_rew_mean          | -107        |\n","| time/                   |             |\n","|    fps                  | 869         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 75776       |\n","| train/                  |             |\n","|    approx_kl            | 0.004791362 |\n","|    clip_fraction        | 0.034       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.44       |\n","|    explained_variance   | 0.427       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.209       |\n","|    n_updates            | 360         |\n","|    policy_gradient_loss | -0.00626    |\n","|    std                  | 0.941       |\n","|    value_loss           | 0.562       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 689          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 829          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 7            |\n","|    total_timesteps      | 77824        |\n","| train/                  |              |\n","|    approx_kl            | 0.0032212064 |\n","|    clip_fraction        | 0.00493      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.43        |\n","|    explained_variance   | 0.298        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 29.5         |\n","|    n_updates            | 370          |\n","|    policy_gradient_loss | -0.0029      |\n","|    std                  | 0.94         |\n","|    value_loss           | 61.2         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 704          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 750          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 10           |\n","|    total_timesteps      | 79872        |\n","| train/                  |              |\n","|    approx_kl            | 0.0028469325 |\n","|    clip_fraction        | 0.0113       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.42        |\n","|    explained_variance   | 0.429        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.137        |\n","|    n_updates            | 380          |\n","|    policy_gradient_loss | -0.00345     |\n","|    std                  | 0.938        |\n","|    value_loss           | 0.524        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 704          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 736          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 81920        |\n","| train/                  |              |\n","|    approx_kl            | 0.0038061005 |\n","|    clip_fraction        | 0.0344       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.41        |\n","|    explained_variance   | 0.669        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0358       |\n","|    n_updates            | 390          |\n","|    policy_gradient_loss | -0.00462     |\n","|    std                  | 0.934        |\n","|    value_loss           | 0.267        |\n","------------------------------------------\n","BipedalWalker - Step: 70001 | Mean Reward: -65.50 ± 1.51\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 719      |\n","|    ep_rew_mean     | -107     |\n","| time/              |          |\n","|    fps             | 995      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 83968    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 719          |\n","|    ep_rew_mean          | -106         |\n","| time/                   |              |\n","|    fps                  | 872          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 4            |\n","|    total_timesteps      | 86016        |\n","| train/                  |              |\n","|    approx_kl            | 0.0045589423 |\n","|    clip_fraction        | 0.0243       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.35        |\n","|    explained_variance   | -0.519       |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.107        |\n","|    n_updates            | 410          |\n","|    policy_gradient_loss | -0.00457     |\n","|    std                  | 0.92         |\n","|    value_loss           | 0.217        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 720         |\n","|    ep_rew_mean          | -106        |\n","| time/                   |             |\n","|    fps                  | 780         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 88064       |\n","| train/                  |             |\n","|    approx_kl            | 0.005196451 |\n","|    clip_fraction        | 0.0464      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.34       |\n","|    explained_variance   | 0.945       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.284       |\n","|    n_updates            | 420         |\n","|    policy_gradient_loss | -0.00562    |\n","|    std                  | 0.923       |\n","|    value_loss           | 0.725       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 750          |\n","|    ep_rew_mean          | -106         |\n","| time/                   |              |\n","|    fps                  | 740          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 90112        |\n","| train/                  |              |\n","|    approx_kl            | 0.0017152401 |\n","|    clip_fraction        | 0.000635     |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.36        |\n","|    explained_variance   | 0.32         |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 38.7         |\n","|    n_updates            | 430          |\n","|    policy_gradient_loss | -0.00186     |\n","|    std                  | 0.923        |\n","|    value_loss           | 36.9         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 735         |\n","|    ep_rew_mean          | -106        |\n","| time/                   |             |\n","|    fps                  | 741         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 92160       |\n","| train/                  |             |\n","|    approx_kl            | 0.003086744 |\n","|    clip_fraction        | 0.00337     |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.36       |\n","|    explained_variance   | 0.226       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 10.1        |\n","|    n_updates            | 440         |\n","|    policy_gradient_loss | -0.00171    |\n","|    std                  | 0.924       |\n","|    value_loss           | 29.7        |\n","-----------------------------------------\n","BipedalWalker - Step: 80001 | Mean Reward: -73.71 ± 1.45\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 751      |\n","|    ep_rew_mean     | -106     |\n","| time/              |          |\n","|    fps             | 965      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 94208    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 751          |\n","|    ep_rew_mean          | -106         |\n","| time/                   |              |\n","|    fps                  | 863          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 4            |\n","|    total_timesteps      | 96256        |\n","| train/                  |              |\n","|    approx_kl            | 0.0032052754 |\n","|    clip_fraction        | 0.00547      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.36        |\n","|    explained_variance   | 0.526        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 11.7         |\n","|    n_updates            | 460          |\n","|    policy_gradient_loss | -0.00407     |\n","|    std                  | 0.923        |\n","|    value_loss           | 24.9         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 766          |\n","|    ep_rew_mean          | -105         |\n","| time/                   |              |\n","|    fps                  | 728          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 98304        |\n","| train/                  |              |\n","|    approx_kl            | 0.0046109874 |\n","|    clip_fraction        | 0.0385       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.35        |\n","|    explained_variance   | 0.328        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0406       |\n","|    n_updates            | 470          |\n","|    policy_gradient_loss | -0.00501     |\n","|    std                  | 0.918        |\n","|    value_loss           | 0.254        |\n","------------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 766        |\n","|    ep_rew_mean          | -105       |\n","| time/                   |            |\n","|    fps                  | 735        |\n","|    iterations           | 4          |\n","|    time_elapsed         | 11         |\n","|    total_timesteps      | 100352     |\n","| train/                  |            |\n","|    approx_kl            | 0.00391908 |\n","|    clip_fraction        | 0.0453     |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -5.32      |\n","|    explained_variance   | 0.44       |\n","|    learning_rate        | 0.0001     |\n","|    loss                 | 0.0721     |\n","|    n_updates            | 480        |\n","|    policy_gradient_loss | -0.00596   |\n","|    std                  | 0.91       |\n","|    value_loss           | 0.185      |\n","----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 766          |\n","|    ep_rew_mean          | -105         |\n","| time/                   |              |\n","|    fps                  | 741          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 102400       |\n","| train/                  |              |\n","|    approx_kl            | 0.0005373083 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.3         |\n","|    explained_variance   | 0.483        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 21.8         |\n","|    n_updates            | 490          |\n","|    policy_gradient_loss | -0.00084     |\n","|    std                  | 0.91         |\n","|    value_loss           | 24.3         |\n","------------------------------------------\n","BipedalWalker - Step: 90001 | Mean Reward: -75.38 ± 1.04\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 766      |\n","|    ep_rew_mean     | -105     |\n","| time/              |          |\n","|    fps             | 997      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 104448   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 782         |\n","|    ep_rew_mean          | -104        |\n","| time/                   |             |\n","|    fps                  | 803         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 106496      |\n","| train/                  |             |\n","|    approx_kl            | 0.004226247 |\n","|    clip_fraction        | 0.0474      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.27       |\n","|    explained_variance   | 0.413       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0143      |\n","|    n_updates            | 510         |\n","|    policy_gradient_loss | -0.00589    |\n","|    std                  | 0.9         |\n","|    value_loss           | 0.14        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 797          |\n","|    ep_rew_mean          | -104         |\n","| time/                   |              |\n","|    fps                  | 723          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 108544       |\n","| train/                  |              |\n","|    approx_kl            | 0.0056687756 |\n","|    clip_fraction        | 0.0707       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.24        |\n","|    explained_variance   | 0.301        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0335       |\n","|    n_updates            | 520          |\n","|    policy_gradient_loss | -0.00771     |\n","|    std                  | 0.895        |\n","|    value_loss           | 0.164        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 797         |\n","|    ep_rew_mean          | -104        |\n","| time/                   |             |\n","|    fps                  | 734         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 110592      |\n","| train/                  |             |\n","|    approx_kl            | 0.005571508 |\n","|    clip_fraction        | 0.0554      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.22       |\n","|    explained_variance   | 0.584       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.014       |\n","|    n_updates            | 530         |\n","|    policy_gradient_loss | -0.00775    |\n","|    std                  | 0.89        |\n","|    value_loss           | 0.127       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 812         |\n","|    ep_rew_mean          | -104        |\n","| time/                   |             |\n","|    fps                  | 740         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 112640      |\n","| train/                  |             |\n","|    approx_kl            | 0.005205446 |\n","|    clip_fraction        | 0.0505      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.2        |\n","|    explained_variance   | 0.576       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0542      |\n","|    n_updates            | 540         |\n","|    policy_gradient_loss | -0.00758    |\n","|    std                  | 0.887       |\n","|    value_loss           | 0.165       |\n","-----------------------------------------\n","BipedalWalker - Step: 100001 | Mean Reward: -49.52 ± 3.25\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 827      |\n","|    ep_rew_mean     | -103     |\n","| time/              |          |\n","|    fps             | 1009     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 114688   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 827          |\n","|    ep_rew_mean          | -103         |\n","| time/                   |              |\n","|    fps                  | 770          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 116736       |\n","| train/                  |              |\n","|    approx_kl            | 0.0030198984 |\n","|    clip_fraction        | 0.00596      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.18        |\n","|    explained_variance   | 0.422        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0693       |\n","|    n_updates            | 560          |\n","|    policy_gradient_loss | -0.00403     |\n","|    std                  | 0.884        |\n","|    value_loss           | 0.211        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 827          |\n","|    ep_rew_mean          | -103         |\n","| time/                   |              |\n","|    fps                  | 737          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 118784       |\n","| train/                  |              |\n","|    approx_kl            | 0.0054282495 |\n","|    clip_fraction        | 0.0599       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.17        |\n","|    explained_variance   | 0.247        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0656       |\n","|    n_updates            | 570          |\n","|    policy_gradient_loss | -0.0077      |\n","|    std                  | 0.88         |\n","|    value_loss           | 0.161        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 843          |\n","|    ep_rew_mean          | -102         |\n","| time/                   |              |\n","|    fps                  | 749          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 10           |\n","|    total_timesteps      | 120832       |\n","| train/                  |              |\n","|    approx_kl            | 0.0035823872 |\n","|    clip_fraction        | 0.0233       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.15        |\n","|    explained_variance   | 0.418        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0328       |\n","|    n_updates            | 580          |\n","|    policy_gradient_loss | -0.00463     |\n","|    std                  | 0.875        |\n","|    value_loss           | 0.0793       |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 858          |\n","|    ep_rew_mean          | -102         |\n","| time/                   |              |\n","|    fps                  | 743          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 122880       |\n","| train/                  |              |\n","|    approx_kl            | 0.0022698329 |\n","|    clip_fraction        | 0.00161      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.14        |\n","|    explained_variance   | 0.404        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 3.98         |\n","|    n_updates            | 590          |\n","|    policy_gradient_loss | -0.00124     |\n","|    std                  | 0.874        |\n","|    value_loss           | 21.8         |\n","------------------------------------------\n","BipedalWalker - Step: 110001 | Mean Reward: -45.05 ± 6.32\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 858      |\n","|    ep_rew_mean     | -102     |\n","| time/              |          |\n","|    fps             | 956      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 124928   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 858          |\n","|    ep_rew_mean          | -101         |\n","| time/                   |              |\n","|    fps                  | 711          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 126976       |\n","| train/                  |              |\n","|    approx_kl            | 0.0052072485 |\n","|    clip_fraction        | 0.0459       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.12        |\n","|    explained_variance   | 0.524        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.094        |\n","|    n_updates            | 610          |\n","|    policy_gradient_loss | -0.00681     |\n","|    std                  | 0.87         |\n","|    value_loss           | 0.18         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 874         |\n","|    ep_rew_mean          | -101        |\n","| time/                   |             |\n","|    fps                  | 728         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 129024      |\n","| train/                  |             |\n","|    approx_kl            | 0.005824194 |\n","|    clip_fraction        | 0.0678      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.11       |\n","|    explained_variance   | 0.583       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0256      |\n","|    n_updates            | 620         |\n","|    policy_gradient_loss | -0.00734    |\n","|    std                  | 0.866       |\n","|    value_loss           | 0.0881      |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 846         |\n","|    ep_rew_mean          | -101        |\n","| time/                   |             |\n","|    fps                  | 740         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 131072      |\n","| train/                  |             |\n","|    approx_kl            | 0.003866037 |\n","|    clip_fraction        | 0.048       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.09       |\n","|    explained_variance   | 0.813       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0336      |\n","|    n_updates            | 630         |\n","|    policy_gradient_loss | -0.00831    |\n","|    std                  | 0.862       |\n","|    value_loss           | 0.104       |\n","-----------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 877           |\n","|    ep_rew_mean          | -100          |\n","| time/                   |               |\n","|    fps                  | 747           |\n","|    iterations           | 5             |\n","|    time_elapsed         | 13            |\n","|    total_timesteps      | 133120        |\n","| train/                  |               |\n","|    approx_kl            | 0.00011245246 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.18          |\n","|    entropy_loss         | -5.08         |\n","|    explained_variance   | 0.424         |\n","|    learning_rate        | 0.0001        |\n","|    loss                 | 63.1          |\n","|    n_updates            | 640           |\n","|    policy_gradient_loss | -0.000457     |\n","|    std                  | 0.862         |\n","|    value_loss           | 47.7          |\n","-------------------------------------------\n","BipedalWalker - Step: 120001 | Mean Reward: -49.20 ± 3.86\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 892      |\n","|    ep_rew_mean     | -99.7    |\n","| time/              |          |\n","|    fps             | 819      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 135168   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 908         |\n","|    ep_rew_mean          | -99.2       |\n","| time/                   |             |\n","|    fps                  | 713         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 137216      |\n","| train/                  |             |\n","|    approx_kl            | 0.005913876 |\n","|    clip_fraction        | 0.0607      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.05       |\n","|    explained_variance   | 0.541       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0204      |\n","|    n_updates            | 660         |\n","|    policy_gradient_loss | -0.00771    |\n","|    std                  | 0.854       |\n","|    value_loss           | 0.072       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 923         |\n","|    ep_rew_mean          | -98.8       |\n","| time/                   |             |\n","|    fps                  | 734         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 139264      |\n","| train/                  |             |\n","|    approx_kl            | 0.003962813 |\n","|    clip_fraction        | 0.0357      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.03       |\n","|    explained_variance   | 0.546       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0545      |\n","|    n_updates            | 670         |\n","|    policy_gradient_loss | -0.0059     |\n","|    std                  | 0.85        |\n","|    value_loss           | 0.109       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 938          |\n","|    ep_rew_mean          | -97.8        |\n","| time/                   |              |\n","|    fps                  | 745          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 10           |\n","|    total_timesteps      | 141312       |\n","| train/                  |              |\n","|    approx_kl            | 0.0056544123 |\n","|    clip_fraction        | 0.0518       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.02        |\n","|    explained_variance   | 0.604        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0439       |\n","|    n_updates            | 680          |\n","|    policy_gradient_loss | -0.00772     |\n","|    std                  | 0.848        |\n","|    value_loss           | 0.129        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 952          |\n","|    ep_rew_mean          | -98.1        |\n","| time/                   |              |\n","|    fps                  | 752          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 143360       |\n","| train/                  |              |\n","|    approx_kl            | 0.0055174306 |\n","|    clip_fraction        | 0.0732       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5           |\n","|    explained_variance   | 0.707        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0557       |\n","|    n_updates            | 690          |\n","|    policy_gradient_loss | -0.00929     |\n","|    std                  | 0.842        |\n","|    value_loss           | 0.129        |\n","------------------------------------------\n","BipedalWalker - Step: 130001 | Mean Reward: -59.21 ± 1.26\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 971      |\n","|    ep_rew_mean     | -97.8    |\n","| time/              |          |\n","|    fps             | 759      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 145408   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 987          |\n","|    ep_rew_mean          | -97.2        |\n","| time/                   |              |\n","|    fps                  | 705          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 147456       |\n","| train/                  |              |\n","|    approx_kl            | 0.0015540184 |\n","|    clip_fraction        | 0.000879     |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.98        |\n","|    explained_variance   | 0.606        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 7.11         |\n","|    n_updates            | 710          |\n","|    policy_gradient_loss | -0.00187     |\n","|    std                  | 0.842        |\n","|    value_loss           | 23.6         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1e+03       |\n","|    ep_rew_mean          | -96.5       |\n","| time/                   |             |\n","|    fps                  | 725         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 149504      |\n","| train/                  |             |\n","|    approx_kl            | 0.002401785 |\n","|    clip_fraction        | 0.004       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.98       |\n","|    explained_variance   | 0.607       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 12.4        |\n","|    n_updates            | 720         |\n","|    policy_gradient_loss | -0.0017     |\n","|    std                  | 0.842       |\n","|    value_loss           | 20.5        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.02e+03    |\n","|    ep_rew_mean          | -95.7       |\n","| time/                   |             |\n","|    fps                  | 737         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 151552      |\n","| train/                  |             |\n","|    approx_kl            | 0.003788304 |\n","|    clip_fraction        | 0.0354      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.98       |\n","|    explained_variance   | 0.242       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0258      |\n","|    n_updates            | 730         |\n","|    policy_gradient_loss | -0.00468    |\n","|    std                  | 0.839       |\n","|    value_loss           | 0.154       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.03e+03    |\n","|    ep_rew_mean          | -94.5       |\n","| time/                   |             |\n","|    fps                  | 746         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 153600      |\n","| train/                  |             |\n","|    approx_kl            | 0.003482772 |\n","|    clip_fraction        | 0.00815     |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.97       |\n","|    explained_variance   | 0.678       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 2.3         |\n","|    n_updates            | 740         |\n","|    policy_gradient_loss | -0.0025     |\n","|    std                  | 0.838       |\n","|    value_loss           | 17.6        |\n","-----------------------------------------\n","BipedalWalker - Step: 140001 | Mean Reward: -52.54 ± 7.90\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.05e+03 |\n","|    ep_rew_mean     | -93.8    |\n","| time/              |          |\n","|    fps             | 715      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 155648   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.05e+03     |\n","|    ep_rew_mean          | -93.3        |\n","| time/                   |              |\n","|    fps                  | 743          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 157696       |\n","| train/                  |              |\n","|    approx_kl            | 0.0014642008 |\n","|    clip_fraction        | 0.00122      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.96        |\n","|    explained_variance   | 0.693        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 12.6         |\n","|    n_updates            | 760          |\n","|    policy_gradient_loss | -0.00288     |\n","|    std                  | 0.836        |\n","|    value_loss           | 19.8         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.06e+03     |\n","|    ep_rew_mean          | -92.8        |\n","| time/                   |              |\n","|    fps                  | 750          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 159744       |\n","| train/                  |              |\n","|    approx_kl            | 0.0052522765 |\n","|    clip_fraction        | 0.046        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.96        |\n","|    explained_variance   | 0.258        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.141        |\n","|    n_updates            | 770          |\n","|    policy_gradient_loss | -0.00739     |\n","|    std                  | 0.835        |\n","|    value_loss           | 0.273        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.08e+03    |\n","|    ep_rew_mean          | -91.7       |\n","| time/                   |             |\n","|    fps                  | 754         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 161792      |\n","| train/                  |             |\n","|    approx_kl            | 0.006309334 |\n","|    clip_fraction        | 0.0556      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.94       |\n","|    explained_variance   | 0.805       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0318      |\n","|    n_updates            | 780         |\n","|    policy_gradient_loss | -0.00959    |\n","|    std                  | 0.832       |\n","|    value_loss           | 0.0861      |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.09e+03    |\n","|    ep_rew_mean          | -91         |\n","| time/                   |             |\n","|    fps                  | 745         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 163840      |\n","| train/                  |             |\n","|    approx_kl            | 0.004916654 |\n","|    clip_fraction        | 0.0441      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.93       |\n","|    explained_variance   | 0.508       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.061       |\n","|    n_updates            | 790         |\n","|    policy_gradient_loss | -0.0087     |\n","|    std                  | 0.831       |\n","|    value_loss           | 0.189       |\n","-----------------------------------------\n","BipedalWalker - Step: 150001 | Mean Reward: -14.01 ± 17.17\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.11e+03 |\n","|    ep_rew_mean     | -90.1    |\n","| time/              |          |\n","|    fps             | 826      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 165888   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.11e+03    |\n","|    ep_rew_mean          | -89.5       |\n","| time/                   |             |\n","|    fps                  | 810         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 167936      |\n","| train/                  |             |\n","|    approx_kl            | 0.005136623 |\n","|    clip_fraction        | 0.0413      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.89       |\n","|    explained_variance   | 0.731       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0692      |\n","|    n_updates            | 810         |\n","|    policy_gradient_loss | -0.00732    |\n","|    std                  | 0.822       |\n","|    value_loss           | 0.177       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.12e+03     |\n","|    ep_rew_mean          | -88.9        |\n","| time/                   |              |\n","|    fps                  | 799          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 7            |\n","|    total_timesteps      | 169984       |\n","| train/                  |              |\n","|    approx_kl            | 0.0051047322 |\n","|    clip_fraction        | 0.0395       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.88        |\n","|    explained_variance   | 0.737        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0465       |\n","|    n_updates            | 820          |\n","|    policy_gradient_loss | -0.00703     |\n","|    std                  | 0.82         |\n","|    value_loss           | 0.145        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.15e+03     |\n","|    ep_rew_mean          | -87.3        |\n","| time/                   |              |\n","|    fps                  | 795          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 10           |\n","|    total_timesteps      | 172032       |\n","| train/                  |              |\n","|    approx_kl            | 0.0058577415 |\n","|    clip_fraction        | 0.0686       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.87        |\n","|    explained_variance   | 0.858        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0551       |\n","|    n_updates            | 830          |\n","|    policy_gradient_loss | -0.008       |\n","|    std                  | 0.816        |\n","|    value_loss           | 0.166        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.17e+03     |\n","|    ep_rew_mean          | -86.3        |\n","| time/                   |              |\n","|    fps                  | 757          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 174080       |\n","| train/                  |              |\n","|    approx_kl            | 0.0046046306 |\n","|    clip_fraction        | 0.0468       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.85        |\n","|    explained_variance   | 0.773        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0732       |\n","|    n_updates            | 840          |\n","|    policy_gradient_loss | -0.00739     |\n","|    std                  | 0.814        |\n","|    value_loss           | 0.149        |\n","------------------------------------------\n","BipedalWalker - Step: 160001 | Mean Reward: 102.08 ± 4.14\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.15e+03 |\n","|    ep_rew_mean     | -85.4    |\n","| time/              |          |\n","|    fps             | 944      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 176128   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.15e+03     |\n","|    ep_rew_mean          | -84.5        |\n","| time/                   |              |\n","|    fps                  | 859          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 4            |\n","|    total_timesteps      | 178176       |\n","| train/                  |              |\n","|    approx_kl            | 0.0010249509 |\n","|    clip_fraction        | 0.000293     |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.82        |\n","|    explained_variance   | 0.542        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 4.08         |\n","|    n_updates            | 860          |\n","|    policy_gradient_loss | -0.00144     |\n","|    std                  | 0.809        |\n","|    value_loss           | 20.8         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.17e+03    |\n","|    ep_rew_mean          | -83.6       |\n","| time/                   |             |\n","|    fps                  | 836         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 180224      |\n","| train/                  |             |\n","|    approx_kl            | 0.004869857 |\n","|    clip_fraction        | 0.0352      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.82       |\n","|    explained_variance   | 0.714       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0582      |\n","|    n_updates            | 870         |\n","|    policy_gradient_loss | -0.00697    |\n","|    std                  | 0.806       |\n","|    value_loss           | 0.339       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.18e+03     |\n","|    ep_rew_mean          | -81.8        |\n","| time/                   |              |\n","|    fps                  | 825          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 9            |\n","|    total_timesteps      | 182272       |\n","| train/                  |              |\n","|    approx_kl            | 0.0060713985 |\n","|    clip_fraction        | 0.0543       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.8         |\n","|    explained_variance   | 0.585        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.198        |\n","|    n_updates            | 880          |\n","|    policy_gradient_loss | -0.00849     |\n","|    std                  | 0.803        |\n","|    value_loss           | 0.556        |\n","------------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.2e+03    |\n","|    ep_rew_mean          | -80.7      |\n","| time/                   |            |\n","|    fps                  | 769        |\n","|    iterations           | 5          |\n","|    time_elapsed         | 13         |\n","|    total_timesteps      | 184320     |\n","| train/                  |            |\n","|    approx_kl            | 0.00572865 |\n","|    clip_fraction        | 0.068      |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -4.79      |\n","|    explained_variance   | 0.643      |\n","|    learning_rate        | 0.0001     |\n","|    loss                 | 0.121      |\n","|    n_updates            | 890        |\n","|    policy_gradient_loss | -0.0102    |\n","|    std                  | 0.801      |\n","|    value_loss           | 0.348      |\n","----------------------------------------\n","BipedalWalker - Step: 170001 | Mean Reward: 103.39 ± 1.66\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.21e+03 |\n","|    ep_rew_mean     | -79.5    |\n","| time/              |          |\n","|    fps             | 995      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 186368   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.23e+03    |\n","|    ep_rew_mean          | -78.2       |\n","| time/                   |             |\n","|    fps                  | 883         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 188416      |\n","| train/                  |             |\n","|    approx_kl            | 0.006696444 |\n","|    clip_fraction        | 0.0817      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.76       |\n","|    explained_variance   | 0.862       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.077       |\n","|    n_updates            | 910         |\n","|    policy_gradient_loss | -0.00954    |\n","|    std                  | 0.795       |\n","|    value_loss           | 0.183       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.23e+03     |\n","|    ep_rew_mean          | -77.2        |\n","| time/                   |              |\n","|    fps                  | 853          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 7            |\n","|    total_timesteps      | 190464       |\n","| train/                  |              |\n","|    approx_kl            | 0.0027066162 |\n","|    clip_fraction        | 0.0062       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.75        |\n","|    explained_variance   | 0.672        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 8.71         |\n","|    n_updates            | 920          |\n","|    policy_gradient_loss | -0.00339     |\n","|    std                  | 0.795        |\n","|    value_loss           | 17.5         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.26e+03    |\n","|    ep_rew_mean          | -74.9       |\n","| time/                   |             |\n","|    fps                  | 839         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 192512      |\n","| train/                  |             |\n","|    approx_kl            | 0.005339413 |\n","|    clip_fraction        | 0.042       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.75       |\n","|    explained_variance   | 0.706       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0958      |\n","|    n_updates            | 930         |\n","|    policy_gradient_loss | -0.0078     |\n","|    std                  | 0.792       |\n","|    value_loss           | 0.242       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.27e+03    |\n","|    ep_rew_mean          | -73.7       |\n","| time/                   |             |\n","|    fps                  | 758         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 194560      |\n","| train/                  |             |\n","|    approx_kl            | 0.005497614 |\n","|    clip_fraction        | 0.044       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.73       |\n","|    explained_variance   | 0.839       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.116       |\n","|    n_updates            | 940         |\n","|    policy_gradient_loss | -0.00858    |\n","|    std                  | 0.789       |\n","|    value_loss           | 0.255       |\n","-----------------------------------------\n","BipedalWalker - Step: 180001 | Mean Reward: 94.80 ± 4.98\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.29e+03 |\n","|    ep_rew_mean     | -72.4    |\n","| time/              |          |\n","|    fps             | 1018     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 196608   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.29e+03    |\n","|    ep_rew_mean          | -71.1       |\n","| time/                   |             |\n","|    fps                  | 879         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 198656      |\n","| train/                  |             |\n","|    approx_kl            | 0.004614112 |\n","|    clip_fraction        | 0.0312      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.69       |\n","|    explained_variance   | 0.804       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.117       |\n","|    n_updates            | 960         |\n","|    policy_gradient_loss | -0.00616    |\n","|    std                  | 0.783       |\n","|    value_loss           | 0.278       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.3e+03      |\n","|    ep_rew_mean          | -69.8        |\n","| time/                   |              |\n","|    fps                  | 846          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 7            |\n","|    total_timesteps      | 200704       |\n","| train/                  |              |\n","|    approx_kl            | 0.0058879517 |\n","|    clip_fraction        | 0.0587       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.69        |\n","|    explained_variance   | 0.838        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.167        |\n","|    n_updates            | 970          |\n","|    policy_gradient_loss | -0.00833     |\n","|    std                  | 0.78         |\n","|    value_loss           | 0.336        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.29e+03    |\n","|    ep_rew_mean          | -67.2       |\n","| time/                   |             |\n","|    fps                  | 804         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 202752      |\n","| train/                  |             |\n","|    approx_kl            | 0.006137927 |\n","|    clip_fraction        | 0.0738      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.67       |\n","|    explained_variance   | 0.657       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.13        |\n","|    n_updates            | 980         |\n","|    policy_gradient_loss | -0.00845    |\n","|    std                  | 0.779       |\n","|    value_loss           | 0.343       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.29e+03     |\n","|    ep_rew_mean          | -65.8        |\n","| time/                   |              |\n","|    fps                  | 754          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 204800       |\n","| train/                  |              |\n","|    approx_kl            | 0.0041158497 |\n","|    clip_fraction        | 0.0144       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.67        |\n","|    explained_variance   | 0.692        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 16           |\n","|    n_updates            | 990          |\n","|    policy_gradient_loss | -0.00431     |\n","|    std                  | 0.779        |\n","|    value_loss           | 17.7         |\n","------------------------------------------\n","BipedalWalker - Step: 190001 | Mean Reward: 93.38 ± 1.88\n"]}]},{"cell_type":"markdown","source":["#4. PPO Bipedal\n","env = 1, batch size= 128, lr = 3e-4"],"metadata":{"id":"KkXJ7MCZCJ30"}},{"cell_type":"code","source":["def train_bipedalwalker4():\n","    # Create environment and model for BipedalWalker\n","    env_bipedal = make_vec_env(\"BipedalWalker-v3\", n_envs=1)\n","    model_b = PPO(\"MlpPolicy\", env_bipedal, verbose=1, tensorboard_log=\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/\",\n","                n_steps=2048, batch_size=128, gae_lambda=0.95, gamma=0.99,\n","                n_epochs=10, ent_coef=0.0, learning_rate=3e-4, clip_range=0.18)\n","\n","    eval_freq = 10000  # Evaluate every 10,000 steps\n","    n_eval_episodes = 10  # Number of episodes per evaluation\n","\n","    for step in range(1, 200001, eval_freq):\n","        # Train the model\n","        model_b.learn(total_timesteps=eval_freq, reset_num_timesteps=False)\n","\n","        # Evaluate the model on BipedalWalker\n","        mean_reward, std_reward = evaluate_model(model_b, env_bipedal, n_eval_episodes)\n","\n","        # Print evaluation results for BipedalWalker\n","        print(f\"BipedalWalker - Step: {step} | Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n","\n","    # Save the trained model for BipedalWalker\n","    model_b.save(\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4\")"],"metadata":{"id":"69B_Vap3CcP0","executionInfo":{"status":"ok","timestamp":1740512678631,"user_tz":-120,"elapsed":32,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    train_bipedalwalker4()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_jxGcFwCy4T","executionInfo":{"status":"ok","timestamp":1740513118414,"user_tz":-120,"elapsed":439772,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}},"outputId":"74094747-b65c-4046-ca3f-ac25d6f45fc2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 842      |\n","|    ep_rew_mean     | -103     |\n","| time/              |          |\n","|    fps             | 988      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 2048     |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.09e+03    |\n","|    ep_rew_mean          | -103        |\n","| time/                   |             |\n","|    fps                  | 837         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.008295925 |\n","|    clip_fraction        | 0.0856      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.68       |\n","|    explained_variance   | -0.000557   |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.98        |\n","|    n_updates            | 10          |\n","|    policy_gradient_loss | -0.00715    |\n","|    std                  | 1           |\n","|    value_loss           | 39.5        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.22e+03    |\n","|    ep_rew_mean          | -108        |\n","| time/                   |             |\n","|    fps                  | 813         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.010401462 |\n","|    clip_fraction        | 0.14        |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.65       |\n","|    explained_variance   | 0.627       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.209       |\n","|    n_updates            | 20          |\n","|    policy_gradient_loss | -0.013      |\n","|    std                  | 0.987       |\n","|    value_loss           | 0.515       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.17e+03     |\n","|    ep_rew_mean          | -105         |\n","| time/                   |              |\n","|    fps                  | 743          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 8192         |\n","| train/                  |              |\n","|    approx_kl            | 0.0075928858 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.59        |\n","|    explained_variance   | 0.709        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.172        |\n","|    n_updates            | 30           |\n","|    policy_gradient_loss | -0.0125      |\n","|    std                  | 0.974        |\n","|    value_loss           | 0.363        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 681         |\n","|    ep_rew_mean          | -106        |\n","| time/                   |             |\n","|    fps                  | 727         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.004589694 |\n","|    clip_fraction        | 0.0534      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.56       |\n","|    explained_variance   | 0.0995      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.85        |\n","|    n_updates            | 40          |\n","|    policy_gradient_loss | -0.00364    |\n","|    std                  | 0.972       |\n","|    value_loss           | 34.6        |\n","-----------------------------------------\n","BipedalWalker - Step: 1 | Mean Reward: -35.06 ± 0.30\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 582      |\n","|    ep_rew_mean     | -107     |\n","| time/              |          |\n","|    fps             | 987      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 12288    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 526          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 869          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 4            |\n","|    total_timesteps      | 14336        |\n","| train/                  |              |\n","|    approx_kl            | 0.0013103161 |\n","|    clip_fraction        | 0.000244     |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.56        |\n","|    explained_variance   | 0.351        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 42.9         |\n","|    n_updates            | 60           |\n","|    policy_gradient_loss | -0.00183     |\n","|    std                  | 0.972        |\n","|    value_loss           | 142          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 550         |\n","|    ep_rew_mean          | -106        |\n","| time/                   |             |\n","|    fps                  | 809         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 16384       |\n","| train/                  |             |\n","|    approx_kl            | 0.002933619 |\n","|    clip_fraction        | 0.00449     |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.56       |\n","|    explained_variance   | 0.455       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 66.3        |\n","|    n_updates            | 70          |\n","|    policy_gradient_loss | -0.00413    |\n","|    std                  | 0.971       |\n","|    value_loss           | 118         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 585         |\n","|    ep_rew_mean          | -105        |\n","| time/                   |             |\n","|    fps                  | 729         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 18432       |\n","| train/                  |             |\n","|    approx_kl            | 0.006639866 |\n","|    clip_fraction        | 0.0832      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.56       |\n","|    explained_variance   | 0.53        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 17.6        |\n","|    n_updates            | 80          |\n","|    policy_gradient_loss | -0.0077     |\n","|    std                  | 0.969       |\n","|    value_loss           | 23          |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 542         |\n","|    ep_rew_mean          | -106        |\n","| time/                   |             |\n","|    fps                  | 738         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 20480       |\n","| train/                  |             |\n","|    approx_kl            | 0.007009371 |\n","|    clip_fraction        | 0.0586      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.55       |\n","|    explained_variance   | 0.964       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.218       |\n","|    n_updates            | 90          |\n","|    policy_gradient_loss | -0.00629    |\n","|    std                  | 0.966       |\n","|    value_loss           | 1.5         |\n","-----------------------------------------\n","BipedalWalker - Step: 10001 | Mean Reward: -41.42 ± 0.07\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 570      |\n","|    ep_rew_mean     | -106     |\n","| time/              |          |\n","|    fps             | 991      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 22528    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 598          |\n","|    ep_rew_mean          | -106         |\n","| time/                   |              |\n","|    fps                  | 868          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 4            |\n","|    total_timesteps      | 24576        |\n","| train/                  |              |\n","|    approx_kl            | 0.0064966516 |\n","|    clip_fraction        | 0.0827       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.52        |\n","|    explained_variance   | 0.611        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.337        |\n","|    n_updates            | 110          |\n","|    policy_gradient_loss | -0.00749     |\n","|    std                  | 0.959        |\n","|    value_loss           | 1.53         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 610         |\n","|    ep_rew_mean          | -105        |\n","| time/                   |             |\n","|    fps                  | 742         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 26624       |\n","| train/                  |             |\n","|    approx_kl            | 0.006555662 |\n","|    clip_fraction        | 0.0823      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.47       |\n","|    explained_variance   | -0.617      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0235      |\n","|    n_updates            | 120         |\n","|    policy_gradient_loss | -0.00997    |\n","|    std                  | 0.94        |\n","|    value_loss           | 0.176       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 643          |\n","|    ep_rew_mean          | -105         |\n","| time/                   |              |\n","|    fps                  | 732          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 28672        |\n","| train/                  |              |\n","|    approx_kl            | 0.0051002596 |\n","|    clip_fraction        | 0.0482       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.43        |\n","|    explained_variance   | 0.333        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 18.2         |\n","|    n_updates            | 130          |\n","|    policy_gradient_loss | -0.00613     |\n","|    std                  | 0.941        |\n","|    value_loss           | 24.1         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 640         |\n","|    ep_rew_mean          | -105        |\n","| time/                   |             |\n","|    fps                  | 737         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 30720       |\n","| train/                  |             |\n","|    approx_kl            | 0.007288307 |\n","|    clip_fraction        | 0.0605      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.43       |\n","|    explained_variance   | 0.182       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 15.3        |\n","|    n_updates            | 140         |\n","|    policy_gradient_loss | -0.00664    |\n","|    std                  | 0.941       |\n","|    value_loss           | 28.6        |\n","-----------------------------------------\n","BipedalWalker - Step: 20001 | Mean Reward: -56.22 ± 0.41\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 660      |\n","|    ep_rew_mean     | -105     |\n","| time/              |          |\n","|    fps             | 987      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 32768    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 680          |\n","|    ep_rew_mean          | -105         |\n","| time/                   |              |\n","|    fps                  | 774          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 34816        |\n","| train/                  |              |\n","|    approx_kl            | 0.0067222565 |\n","|    clip_fraction        | 0.0783       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.41        |\n","|    explained_variance   | 0.252        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0333       |\n","|    n_updates            | 160          |\n","|    policy_gradient_loss | -0.00814     |\n","|    std                  | 0.929        |\n","|    value_loss           | 0.252        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 698          |\n","|    ep_rew_mean          | -105         |\n","| time/                   |              |\n","|    fps                  | 721          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 36864        |\n","| train/                  |              |\n","|    approx_kl            | 0.0068509984 |\n","|    clip_fraction        | 0.0833       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.34        |\n","|    explained_variance   | -0.89        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0289       |\n","|    n_updates            | 170          |\n","|    policy_gradient_loss | -0.00744     |\n","|    std                  | 0.911        |\n","|    value_loss           | 0.0918       |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 734         |\n","|    ep_rew_mean          | -105        |\n","| time/                   |             |\n","|    fps                  | 737         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 38912       |\n","| train/                  |             |\n","|    approx_kl            | 0.008536207 |\n","|    clip_fraction        | 0.105       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.27       |\n","|    explained_variance   | 0.344       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0242      |\n","|    n_updates            | 180         |\n","|    policy_gradient_loss | -0.00956    |\n","|    std                  | 0.899       |\n","|    value_loss           | 0.0624      |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 715         |\n","|    ep_rew_mean          | -105        |\n","| time/                   |             |\n","|    fps                  | 742         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 40960       |\n","| train/                  |             |\n","|    approx_kl            | 0.010527131 |\n","|    clip_fraction        | 0.124       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.22       |\n","|    explained_variance   | 0.203       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.102       |\n","|    n_updates            | 190         |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.885       |\n","|    value_loss           | 0.255       |\n","-----------------------------------------\n","BipedalWalker - Step: 30001 | Mean Reward: -55.76 ± 0.41\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 730      |\n","|    ep_rew_mean     | -105     |\n","| time/              |          |\n","|    fps             | 833      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 43008    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 746          |\n","|    ep_rew_mean          | -104         |\n","| time/                   |              |\n","|    fps                  | 697          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 45056        |\n","| train/                  |              |\n","|    approx_kl            | 0.0076031056 |\n","|    clip_fraction        | 0.08         |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.17        |\n","|    explained_variance   | 0.142        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0433       |\n","|    n_updates            | 210          |\n","|    policy_gradient_loss | -0.00763     |\n","|    std                  | 0.88         |\n","|    value_loss           | 0.15         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 760         |\n","|    ep_rew_mean          | -104        |\n","| time/                   |             |\n","|    fps                  | 716         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 47104       |\n","| train/                  |             |\n","|    approx_kl            | 0.007868832 |\n","|    clip_fraction        | 0.11        |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.14       |\n","|    explained_variance   | -0.37       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0149      |\n","|    n_updates            | 220         |\n","|    policy_gradient_loss | -0.00914    |\n","|    std                  | 0.87        |\n","|    value_loss           | 0.0716      |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 788         |\n","|    ep_rew_mean          | -104        |\n","| time/                   |             |\n","|    fps                  | 718         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 49152       |\n","| train/                  |             |\n","|    approx_kl            | 0.009150775 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.1        |\n","|    explained_variance   | 0.536       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.00694     |\n","|    n_updates            | 230         |\n","|    policy_gradient_loss | -0.0125     |\n","|    std                  | 0.863       |\n","|    value_loss           | 0.0576      |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 802          |\n","|    ep_rew_mean          | -103         |\n","| time/                   |              |\n","|    fps                  | 731          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 14           |\n","|    total_timesteps      | 51200        |\n","| train/                  |              |\n","|    approx_kl            | 0.0062328307 |\n","|    clip_fraction        | 0.0819       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.06        |\n","|    explained_variance   | 0.426        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0118       |\n","|    n_updates            | 240          |\n","|    policy_gradient_loss | -0.00958     |\n","|    std                  | 0.853        |\n","|    value_loss           | 0.0637       |\n","------------------------------------------\n","BipedalWalker - Step: 40001 | Mean Reward: -53.48 ± 2.13\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 815      |\n","|    ep_rew_mean     | -103     |\n","| time/              |          |\n","|    fps             | 726      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 53248    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 827         |\n","|    ep_rew_mean          | -103        |\n","| time/                   |             |\n","|    fps                  | 703         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 55296       |\n","| train/                  |             |\n","|    approx_kl            | 0.008689065 |\n","|    clip_fraction        | 0.0987      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.96       |\n","|    explained_variance   | 0.441       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0319      |\n","|    n_updates            | 260         |\n","|    policy_gradient_loss | -0.00976    |\n","|    std                  | 0.836       |\n","|    value_loss           | 0.135       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 839          |\n","|    ep_rew_mean          | -102         |\n","| time/                   |              |\n","|    fps                  | 718          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 57344        |\n","| train/                  |              |\n","|    approx_kl            | 0.0074474574 |\n","|    clip_fraction        | 0.0781       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.94        |\n","|    explained_variance   | 0.605        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0179       |\n","|    n_updates            | 270          |\n","|    policy_gradient_loss | -0.0094      |\n","|    std                  | 0.828        |\n","|    value_loss           | 0.0622       |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 862          |\n","|    ep_rew_mean          | -101         |\n","| time/                   |              |\n","|    fps                  | 729          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 59392        |\n","| train/                  |              |\n","|    approx_kl            | 0.0048492374 |\n","|    clip_fraction        | 0.051        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.9         |\n","|    explained_variance   | 0.678        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0426       |\n","|    n_updates            | 280          |\n","|    policy_gradient_loss | -0.00795     |\n","|    std                  | 0.819        |\n","|    value_loss           | 0.0721       |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 873         |\n","|    ep_rew_mean          | -101        |\n","| time/                   |             |\n","|    fps                  | 735         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 61440       |\n","| train/                  |             |\n","|    approx_kl            | 0.007611239 |\n","|    clip_fraction        | 0.0911      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.86       |\n","|    explained_variance   | 0.771       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0502      |\n","|    n_updates            | 290         |\n","|    policy_gradient_loss | -0.0106     |\n","|    std                  | 0.814       |\n","|    value_loss           | 0.121       |\n","-----------------------------------------\n","BipedalWalker - Step: 50001 | Mean Reward: -41.04 ± 3.47\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 884      |\n","|    ep_rew_mean     | -100     |\n","| time/              |          |\n","|    fps             | 738      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 63488    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 894         |\n","|    ep_rew_mean          | -99.7       |\n","| time/                   |             |\n","|    fps                  | 760         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 65536       |\n","| train/                  |             |\n","|    approx_kl            | 0.009014206 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.79       |\n","|    explained_variance   | 0.0759      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0349      |\n","|    n_updates            | 310         |\n","|    policy_gradient_loss | -0.0123     |\n","|    std                  | 0.798       |\n","|    value_loss           | 0.113       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 904         |\n","|    ep_rew_mean          | -99.5       |\n","| time/                   |             |\n","|    fps                  | 767         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 67584       |\n","| train/                  |             |\n","|    approx_kl            | 0.007377357 |\n","|    clip_fraction        | 0.105       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.76       |\n","|    explained_variance   | 0.791       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.078       |\n","|    n_updates            | 320         |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.793       |\n","|    value_loss           | 0.178       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 912         |\n","|    ep_rew_mean          | -98.2       |\n","| time/                   |             |\n","|    fps                  | 764         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 69632       |\n","| train/                  |             |\n","|    approx_kl            | 0.005281152 |\n","|    clip_fraction        | 0.0617      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.73       |\n","|    explained_variance   | 0.663       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0323      |\n","|    n_updates            | 330         |\n","|    policy_gradient_loss | -0.00696    |\n","|    std                  | 0.786       |\n","|    value_loss           | 0.113       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 907          |\n","|    ep_rew_mean          | -98.8        |\n","| time/                   |              |\n","|    fps                  | 744          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 71680        |\n","| train/                  |              |\n","|    approx_kl            | 0.0020960886 |\n","|    clip_fraction        | 0.0235       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.71        |\n","|    explained_variance   | 0.238        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.147        |\n","|    n_updates            | 340          |\n","|    policy_gradient_loss | -0.00378     |\n","|    std                  | 0.787        |\n","|    value_loss           | 16           |\n","------------------------------------------\n","BipedalWalker - Step: 60001 | Mean Reward: -42.68 ± 2.43\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 906      |\n","|    ep_rew_mean     | -98.3    |\n","| time/              |          |\n","|    fps             | 870      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 73728    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 915          |\n","|    ep_rew_mean          | -97.8        |\n","| time/                   |              |\n","|    fps                  | 809          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 75776        |\n","| train/                  |              |\n","|    approx_kl            | 0.0015998617 |\n","|    clip_fraction        | 0.00127      |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.72        |\n","|    explained_variance   | 0.776        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.851        |\n","|    n_updates            | 360          |\n","|    policy_gradient_loss | -0.00265     |\n","|    std                  | 0.789        |\n","|    value_loss           | 17.4         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 924         |\n","|    ep_rew_mean          | -97.2       |\n","| time/                   |             |\n","|    fps                  | 799         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 77824       |\n","| train/                  |             |\n","|    approx_kl            | 0.004458145 |\n","|    clip_fraction        | 0.0427      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.72       |\n","|    explained_variance   | 0.866       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.234       |\n","|    n_updates            | 370         |\n","|    policy_gradient_loss | -0.00544    |\n","|    std                  | 0.787       |\n","|    value_loss           | 0.77        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 941         |\n","|    ep_rew_mean          | -95.9       |\n","| time/                   |             |\n","|    fps                  | 788         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 79872       |\n","| train/                  |             |\n","|    approx_kl            | 0.008675935 |\n","|    clip_fraction        | 0.106       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.7        |\n","|    explained_variance   | 0.451       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0564      |\n","|    n_updates            | 380         |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.781       |\n","|    value_loss           | 0.132       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 949         |\n","|    ep_rew_mean          | -95.1       |\n","| time/                   |             |\n","|    fps                  | 737         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 81920       |\n","| train/                  |             |\n","|    approx_kl            | 0.008267079 |\n","|    clip_fraction        | 0.117       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.68       |\n","|    explained_variance   | 0.571       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0366      |\n","|    n_updates            | 390         |\n","|    policy_gradient_loss | -0.0107     |\n","|    std                  | 0.777       |\n","|    value_loss           | 0.146       |\n","-----------------------------------------\n","BipedalWalker - Step: 70001 | Mean Reward: -47.01 ± 41.84\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 946      |\n","|    ep_rew_mean     | -94.7    |\n","| time/              |          |\n","|    fps             | 763      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 83968    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 953          |\n","|    ep_rew_mean          | -93.9        |\n","| time/                   |              |\n","|    fps                  | 722          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 5            |\n","|    total_timesteps      | 86016        |\n","| train/                  |              |\n","|    approx_kl            | 0.0034670434 |\n","|    clip_fraction        | 0.0516       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.62        |\n","|    explained_variance   | 0.747        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 3.05         |\n","|    n_updates            | 410          |\n","|    policy_gradient_loss | -0.00557     |\n","|    std                  | 0.768        |\n","|    value_loss           | 13.5         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 961         |\n","|    ep_rew_mean          | -93         |\n","| time/                   |             |\n","|    fps                  | 739         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 88064       |\n","| train/                  |             |\n","|    approx_kl            | 0.009259375 |\n","|    clip_fraction        | 0.136       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.6        |\n","|    explained_variance   | 0.761       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.16        |\n","|    n_updates            | 420         |\n","|    policy_gradient_loss | -0.015      |\n","|    std                  | 0.761       |\n","|    value_loss           | 0.28        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 976          |\n","|    ep_rew_mean          | -91.7        |\n","| time/                   |              |\n","|    fps                  | 752          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 10           |\n","|    total_timesteps      | 90112        |\n","| train/                  |              |\n","|    approx_kl            | 0.0064595314 |\n","|    clip_fraction        | 0.0713       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.58        |\n","|    explained_variance   | 0.783        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.213        |\n","|    n_updates            | 430          |\n","|    policy_gradient_loss | -0.00668     |\n","|    std                  | 0.758        |\n","|    value_loss           | 0.661        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 978         |\n","|    ep_rew_mean          | -91.9       |\n","| time/                   |             |\n","|    fps                  | 753         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 92160       |\n","| train/                  |             |\n","|    approx_kl            | 0.008359031 |\n","|    clip_fraction        | 0.12        |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.56       |\n","|    explained_variance   | 0.896       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.118       |\n","|    n_updates            | 440         |\n","|    policy_gradient_loss | -0.0131     |\n","|    std                  | 0.754       |\n","|    value_loss           | 0.307       |\n","-----------------------------------------\n","BipedalWalker - Step: 80001 | Mean Reward: -93.59 ± 0.58\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 984      |\n","|    ep_rew_mean     | -91.4    |\n","| time/              |          |\n","|    fps             | 822      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 94208    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 991         |\n","|    ep_rew_mean          | -90.6       |\n","| time/                   |             |\n","|    fps                  | 808         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 96256       |\n","| train/                  |             |\n","|    approx_kl            | 0.009431769 |\n","|    clip_fraction        | 0.119       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.55       |\n","|    explained_variance   | 0.895       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.217       |\n","|    n_updates            | 460         |\n","|    policy_gradient_loss | -0.00996    |\n","|    std                  | 0.756       |\n","|    value_loss           | 0.47        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 998         |\n","|    ep_rew_mean          | -89.7       |\n","| time/                   |             |\n","|    fps                  | 795         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 98304       |\n","| train/                  |             |\n","|    approx_kl            | 0.009013521 |\n","|    clip_fraction        | 0.125       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.53       |\n","|    explained_variance   | 0.672       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.104       |\n","|    n_updates            | 470         |\n","|    policy_gradient_loss | -0.013      |\n","|    std                  | 0.749       |\n","|    value_loss           | 0.36        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.01e+03    |\n","|    ep_rew_mean          | -88         |\n","| time/                   |             |\n","|    fps                  | 786         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 100352      |\n","| train/                  |             |\n","|    approx_kl            | 0.007352375 |\n","|    clip_fraction        | 0.1         |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.5        |\n","|    explained_variance   | 0.792       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.116       |\n","|    n_updates            | 480         |\n","|    policy_gradient_loss | -0.00851    |\n","|    std                  | 0.744       |\n","|    value_loss           | 0.294       |\n","-----------------------------------------\n","---------------------------------------\n","| rollout/                |           |\n","|    ep_len_mean          | 1.02e+03  |\n","|    ep_rew_mean          | -87.1     |\n","| time/                   |           |\n","|    fps                  | 742       |\n","|    iterations           | 5         |\n","|    time_elapsed         | 13        |\n","|    total_timesteps      | 102400    |\n","| train/                  |           |\n","|    approx_kl            | 0.0083644 |\n","|    clip_fraction        | 0.126     |\n","|    clip_range           | 0.18      |\n","|    entropy_loss         | -4.49     |\n","|    explained_variance   | 0.823     |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 0.125     |\n","|    n_updates            | 490       |\n","|    policy_gradient_loss | -0.013    |\n","|    std                  | 0.743     |\n","|    value_loss           | 0.296     |\n","---------------------------------------\n","BipedalWalker - Step: 90001 | Mean Reward: 110.16 ± 8.19\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.02e+03 |\n","|    ep_rew_mean     | -86.2    |\n","| time/              |          |\n","|    fps             | 973      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 104448   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.03e+03    |\n","|    ep_rew_mean          | -85.2       |\n","| time/                   |             |\n","|    fps                  | 865         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 106496      |\n","| train/                  |             |\n","|    approx_kl            | 0.008544436 |\n","|    clip_fraction        | 0.131       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.44       |\n","|    explained_variance   | 0.776       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0554      |\n","|    n_updates            | 510         |\n","|    policy_gradient_loss | -0.0143     |\n","|    std                  | 0.729       |\n","|    value_loss           | 0.207       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.03e+03    |\n","|    ep_rew_mean          | -84.1       |\n","| time/                   |             |\n","|    fps                  | 839         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 108544      |\n","| train/                  |             |\n","|    approx_kl            | 0.008277118 |\n","|    clip_fraction        | 0.0989      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.38       |\n","|    explained_variance   | 0.818       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0668      |\n","|    n_updates            | 520         |\n","|    policy_gradient_loss | -0.0108     |\n","|    std                  | 0.719       |\n","|    value_loss           | 0.209       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.05e+03    |\n","|    ep_rew_mean          | -81.9       |\n","| time/                   |             |\n","|    fps                  | 813         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 110592      |\n","| train/                  |             |\n","|    approx_kl            | 0.008235138 |\n","|    clip_fraction        | 0.129       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.33       |\n","|    explained_variance   | 0.773       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0908      |\n","|    n_updates            | 530         |\n","|    policy_gradient_loss | -0.0139     |\n","|    std                  | 0.709       |\n","|    value_loss           | 0.224       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.06e+03     |\n","|    ep_rew_mean          | -80.3        |\n","| time/                   |              |\n","|    fps                  | 749          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 112640       |\n","| train/                  |              |\n","|    approx_kl            | 0.0070052897 |\n","|    clip_fraction        | 0.115        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.29        |\n","|    explained_variance   | 0.807        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.124        |\n","|    n_updates            | 540          |\n","|    policy_gradient_loss | -0.0114      |\n","|    std                  | 0.707        |\n","|    value_loss           | 0.317        |\n","------------------------------------------\n","BipedalWalker - Step: 100001 | Mean Reward: 115.00 ± 121.12\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.05e+03 |\n","|    ep_rew_mean     | -79.1    |\n","| time/              |          |\n","|    fps             | 737      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 114688   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.05e+03    |\n","|    ep_rew_mean          | -77.3       |\n","| time/                   |             |\n","|    fps                  | 758         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 116736      |\n","| train/                  |             |\n","|    approx_kl            | 0.004663836 |\n","|    clip_fraction        | 0.0314      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.24       |\n","|    explained_variance   | -0.501      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 26.6        |\n","|    n_updates            | 560         |\n","|    policy_gradient_loss | -0.00628    |\n","|    std                  | 0.699       |\n","|    value_loss           | 41.9        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.05e+03     |\n","|    ep_rew_mean          | -75.4        |\n","| time/                   |              |\n","|    fps                  | 767          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 118784       |\n","| train/                  |              |\n","|    approx_kl            | 0.0061623165 |\n","|    clip_fraction        | 0.077        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.21        |\n","|    explained_variance   | 0.563        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.214        |\n","|    n_updates            | 570          |\n","|    policy_gradient_loss | -0.00721     |\n","|    std                  | 0.688        |\n","|    value_loss           | 0.457        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.05e+03    |\n","|    ep_rew_mean          | -74         |\n","| time/                   |             |\n","|    fps                  | 769         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 120832      |\n","| train/                  |             |\n","|    approx_kl            | 0.010825502 |\n","|    clip_fraction        | 0.129       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.17       |\n","|    explained_variance   | 0.765       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.133       |\n","|    n_updates            | 580         |\n","|    policy_gradient_loss | -0.0113     |\n","|    std                  | 0.686       |\n","|    value_loss           | 0.413       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.06e+03    |\n","|    ep_rew_mean          | -70.3       |\n","| time/                   |             |\n","|    fps                  | 748         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 122880      |\n","| train/                  |             |\n","|    approx_kl            | 0.006515529 |\n","|    clip_fraction        | 0.0666      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.16       |\n","|    explained_variance   | 0.483       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 16.3        |\n","|    n_updates            | 590         |\n","|    policy_gradient_loss | -0.00545    |\n","|    std                  | 0.686       |\n","|    value_loss           | 53.9        |\n","-----------------------------------------\n","BipedalWalker - Step: 110001 | Mean Reward: -33.10 ± 37.64\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.08e+03 |\n","|    ep_rew_mean     | -68.4    |\n","| time/              |          |\n","|    fps             | 974      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 124928   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.09e+03    |\n","|    ep_rew_mean          | -66.6       |\n","| time/                   |             |\n","|    fps                  | 848         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 126976      |\n","| train/                  |             |\n","|    approx_kl            | 0.007877627 |\n","|    clip_fraction        | 0.0739      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.17       |\n","|    explained_variance   | 0.868       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.37        |\n","|    n_updates            | 610         |\n","|    policy_gradient_loss | -0.00701    |\n","|    std                  | 0.685       |\n","|    value_loss           | 10.7        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.11e+03    |\n","|    ep_rew_mean          | -64.8       |\n","| time/                   |             |\n","|    fps                  | 763         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 129024      |\n","| train/                  |             |\n","|    approx_kl            | 0.007481156 |\n","|    clip_fraction        | 0.0934      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.15       |\n","|    explained_variance   | 0.676       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.198       |\n","|    n_updates            | 620         |\n","|    policy_gradient_loss | -0.00968    |\n","|    std                  | 0.681       |\n","|    value_loss           | 0.448       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.14e+03     |\n","|    ep_rew_mean          | -61          |\n","| time/                   |              |\n","|    fps                  | 732          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 131072       |\n","| train/                  |              |\n","|    approx_kl            | 0.0065673054 |\n","|    clip_fraction        | 0.0891       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.11        |\n","|    explained_variance   | 0.822        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.204        |\n","|    n_updates            | 630          |\n","|    policy_gradient_loss | -0.00729     |\n","|    std                  | 0.674        |\n","|    value_loss           | 0.41         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.15e+03    |\n","|    ep_rew_mean          | -59         |\n","| time/                   |             |\n","|    fps                  | 740         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 133120      |\n","| train/                  |             |\n","|    approx_kl            | 0.008345779 |\n","|    clip_fraction        | 0.125       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.08       |\n","|    explained_variance   | 0.812       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.203       |\n","|    n_updates            | 640         |\n","|    policy_gradient_loss | -0.0133     |\n","|    std                  | 0.669       |\n","|    value_loss           | 0.437       |\n","-----------------------------------------\n","BipedalWalker - Step: 120001 | Mean Reward: -86.44 ± 28.89\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.17e+03 |\n","|    ep_rew_mean     | -56.9    |\n","| time/              |          |\n","|    fps             | 1002     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 135168   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.17e+03    |\n","|    ep_rew_mean          | -54.8       |\n","| time/                   |             |\n","|    fps                  | 726         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 137216      |\n","| train/                  |             |\n","|    approx_kl            | 0.008337402 |\n","|    clip_fraction        | 0.116       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.01       |\n","|    explained_variance   | 0.805       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.198       |\n","|    n_updates            | 660         |\n","|    policy_gradient_loss | -0.0143     |\n","|    std                  | 0.656       |\n","|    value_loss           | 0.494       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.19e+03     |\n","|    ep_rew_mean          | -52.6        |\n","| time/                   |              |\n","|    fps                  | 739          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 139264       |\n","| train/                  |              |\n","|    approx_kl            | 0.0076512042 |\n","|    clip_fraction        | 0.0999       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -3.97        |\n","|    explained_variance   | 0.865        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.139        |\n","|    n_updates            | 670          |\n","|    policy_gradient_loss | -0.00889     |\n","|    std                  | 0.65         |\n","|    value_loss           | 0.353        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.2e+03      |\n","|    ep_rew_mean          | -48.2        |\n","| time/                   |              |\n","|    fps                  | 748          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 10           |\n","|    total_timesteps      | 141312       |\n","| train/                  |              |\n","|    approx_kl            | 0.0075554033 |\n","|    clip_fraction        | 0.0771       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -3.95        |\n","|    explained_variance   | 0.783        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.281        |\n","|    n_updates            | 680          |\n","|    policy_gradient_loss | -0.00746     |\n","|    std                  | 0.649        |\n","|    value_loss           | 0.628        |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.22e+03     |\n","|    ep_rew_mean          | -46          |\n","| time/                   |              |\n","|    fps                  | 753          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 143360       |\n","| train/                  |              |\n","|    approx_kl            | 0.0048136786 |\n","|    clip_fraction        | 0.0462       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -3.95        |\n","|    explained_variance   | 0.797        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 10.4         |\n","|    n_updates            | 690          |\n","|    policy_gradient_loss | -0.00517     |\n","|    std                  | 0.65         |\n","|    value_loss           | 18.4         |\n","------------------------------------------\n","BipedalWalker - Step: 130001 | Mean Reward: -100.31 ± 1.22\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.2e+03  |\n","|    ep_rew_mean     | -44.6    |\n","| time/              |          |\n","|    fps             | 780      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 145408   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.2e+03     |\n","|    ep_rew_mean          | -42.4       |\n","| time/                   |             |\n","|    fps                  | 715         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 147456      |\n","| train/                  |             |\n","|    approx_kl            | 0.004853746 |\n","|    clip_fraction        | 0.0433      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.93       |\n","|    explained_variance   | 0.0969      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 48.2        |\n","|    n_updates            | 710         |\n","|    policy_gradient_loss | -0.00421    |\n","|    std                  | 0.647       |\n","|    value_loss           | 74.2        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.22e+03    |\n","|    ep_rew_mean          | -40.1       |\n","| time/                   |             |\n","|    fps                  | 741         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 149504      |\n","| train/                  |             |\n","|    approx_kl            | 0.009230667 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.92       |\n","|    explained_variance   | 0.699       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.253       |\n","|    n_updates            | 720         |\n","|    policy_gradient_loss | -0.0123     |\n","|    std                  | 0.644       |\n","|    value_loss           | 0.86        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.23e+03     |\n","|    ep_rew_mean          | -37.7        |\n","| time/                   |              |\n","|    fps                  | 753          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 10           |\n","|    total_timesteps      | 151552       |\n","| train/                  |              |\n","|    approx_kl            | 0.0047950316 |\n","|    clip_fraction        | 0.0658       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -3.92        |\n","|    explained_variance   | 0.862        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.831        |\n","|    n_updates            | 730          |\n","|    policy_gradient_loss | -0.0058      |\n","|    std                  | 0.646        |\n","|    value_loss           | 6.73         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.26e+03     |\n","|    ep_rew_mean          | -33          |\n","| time/                   |              |\n","|    fps                  | 756          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 153600       |\n","| train/                  |              |\n","|    approx_kl            | 0.0076635955 |\n","|    clip_fraction        | 0.0991       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -3.9         |\n","|    explained_variance   | 0.777        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.187        |\n","|    n_updates            | 740          |\n","|    policy_gradient_loss | -0.0108      |\n","|    std                  | 0.639        |\n","|    value_loss           | 0.459        |\n","------------------------------------------\n","BipedalWalker - Step: 140001 | Mean Reward: -101.55 ± 0.46\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.25e+03 |\n","|    ep_rew_mean     | -30.4    |\n","| time/              |          |\n","|    fps             | 737      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 155648   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.25e+03    |\n","|    ep_rew_mean          | -28         |\n","| time/                   |             |\n","|    fps                  | 753         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 157696      |\n","| train/                  |             |\n","|    approx_kl            | 0.009774676 |\n","|    clip_fraction        | 0.106       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.85       |\n","|    explained_variance   | 0.887       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.43        |\n","|    n_updates            | 760         |\n","|    policy_gradient_loss | -0.0108     |\n","|    std                  | 0.635       |\n","|    value_loss           | 8.29        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.26e+03     |\n","|    ep_rew_mean          | -25.5        |\n","| time/                   |              |\n","|    fps                  | 760          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 8            |\n","|    total_timesteps      | 159744       |\n","| train/                  |              |\n","|    approx_kl            | 0.0081463745 |\n","|    clip_fraction        | 0.129        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -3.86        |\n","|    explained_variance   | 0.646        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.242        |\n","|    n_updates            | 770          |\n","|    policy_gradient_loss | -0.0099      |\n","|    std                  | 0.635        |\n","|    value_loss           | 0.668        |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.26e+03    |\n","|    ep_rew_mean          | -20.7       |\n","| time/                   |             |\n","|    fps                  | 767         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 161792      |\n","| train/                  |             |\n","|    approx_kl            | 0.010284533 |\n","|    clip_fraction        | 0.117       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.82       |\n","|    explained_variance   | 0.841       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.186       |\n","|    n_updates            | 780         |\n","|    policy_gradient_loss | -0.01       |\n","|    std                  | 0.625       |\n","|    value_loss           | 0.391       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.28e+03    |\n","|    ep_rew_mean          | -17.8       |\n","| time/                   |             |\n","|    fps                  | 756         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 163840      |\n","| train/                  |             |\n","|    approx_kl            | 0.004506965 |\n","|    clip_fraction        | 0.0832      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.79       |\n","|    explained_variance   | 0.181       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.34        |\n","|    n_updates            | 790         |\n","|    policy_gradient_loss | -0.00483    |\n","|    std                  | 0.625       |\n","|    value_loss           | 34.7        |\n","-----------------------------------------\n","BipedalWalker - Step: 150001 | Mean Reward: -116.62 ± 2.45\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.25e+03 |\n","|    ep_rew_mean     | -15.6    |\n","| time/              |          |\n","|    fps             | 881      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 165888   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.25e+03    |\n","|    ep_rew_mean          | -13.1       |\n","| time/                   |             |\n","|    fps                  | 834         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 167936      |\n","| train/                  |             |\n","|    approx_kl            | 0.004829753 |\n","|    clip_fraction        | 0.0504      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.76       |\n","|    explained_variance   | 0.458       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 34.5        |\n","|    n_updates            | 810         |\n","|    policy_gradient_loss | -0.00703    |\n","|    std                  | 0.621       |\n","|    value_loss           | 96.7        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.22e+03    |\n","|    ep_rew_mean          | -9.51       |\n","| time/                   |             |\n","|    fps                  | 818         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 169984      |\n","| train/                  |             |\n","|    approx_kl            | 0.008749002 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.76       |\n","|    explained_variance   | 0.664       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.04        |\n","|    n_updates            | 820         |\n","|    policy_gradient_loss | -0.0111     |\n","|    std                  | 0.62        |\n","|    value_loss           | 2.34        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.15e+03   |\n","|    ep_rew_mean          | -8.99      |\n","| time/                   |            |\n","|    fps                  | 810        |\n","|    iterations           | 4          |\n","|    time_elapsed         | 10         |\n","|    total_timesteps      | 172032     |\n","| train/                  |            |\n","|    approx_kl            | 0.00536548 |\n","|    clip_fraction        | 0.0647     |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -3.76      |\n","|    explained_variance   | 0.721      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 42         |\n","|    n_updates            | 830        |\n","|    policy_gradient_loss | -0.00908   |\n","|    std                  | 0.621      |\n","|    value_loss           | 79.5       |\n","----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.14e+03     |\n","|    ep_rew_mean          | -7.86        |\n","| time/                   |              |\n","|    fps                  | 762          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 174080       |\n","| train/                  |              |\n","|    approx_kl            | 0.0038253735 |\n","|    clip_fraction        | 0.0314       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -3.77        |\n","|    explained_variance   | 0.869        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 30.8         |\n","|    n_updates            | 840          |\n","|    policy_gradient_loss | -0.00603     |\n","|    std                  | 0.621        |\n","|    value_loss           | 80.9         |\n","------------------------------------------\n","BipedalWalker - Step: 160001 | Mean Reward: -112.17 ± 0.55\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.12e+03 |\n","|    ep_rew_mean     | -6.22    |\n","| time/              |          |\n","|    fps             | 1018     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 176128   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.05e+03     |\n","|    ep_rew_mean          | -7.1         |\n","| time/                   |              |\n","|    fps                  | 895          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 4            |\n","|    total_timesteps      | 178176       |\n","| train/                  |              |\n","|    approx_kl            | 0.0054304795 |\n","|    clip_fraction        | 0.0708       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -3.77        |\n","|    explained_variance   | 0.896        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 17.6         |\n","|    n_updates            | 860          |\n","|    policy_gradient_loss | -0.00635     |\n","|    std                  | 0.622        |\n","|    value_loss           | 30.9         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.05e+03    |\n","|    ep_rew_mean          | -6.44       |\n","| time/                   |             |\n","|    fps                  | 853         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 180224      |\n","| train/                  |             |\n","|    approx_kl            | 0.004377803 |\n","|    clip_fraction        | 0.025       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.78       |\n","|    explained_variance   | 0.805       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 57.5        |\n","|    n_updates            | 870         |\n","|    policy_gradient_loss | -0.00408    |\n","|    std                  | 0.622       |\n","|    value_loss           | 98.1        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.06e+03    |\n","|    ep_rew_mean          | -1.43       |\n","| time/                   |             |\n","|    fps                  | 810         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 182272      |\n","| train/                  |             |\n","|    approx_kl            | 0.008207945 |\n","|    clip_fraction        | 0.0791      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.77       |\n","|    explained_variance   | 0.793       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 11          |\n","|    n_updates            | 880         |\n","|    policy_gradient_loss | -0.00615    |\n","|    std                  | 0.622       |\n","|    value_loss           | 39.4        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.07e+03    |\n","|    ep_rew_mean          | 1.25        |\n","| time/                   |             |\n","|    fps                  | 758         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 184320      |\n","| train/                  |             |\n","|    approx_kl            | 0.010770737 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.76       |\n","|    explained_variance   | -0.0835     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.311       |\n","|    n_updates            | 890         |\n","|    policy_gradient_loss | -0.00958    |\n","|    std                  | 0.616       |\n","|    value_loss           | 1.58        |\n","-----------------------------------------\n","BipedalWalker - Step: 170001 | Mean Reward: -101.84 ± 1.19\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.07e+03 |\n","|    ep_rew_mean     | 2.83     |\n","| time/              |          |\n","|    fps             | 1001     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 186368   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.04e+03    |\n","|    ep_rew_mean          | 4.12        |\n","| time/                   |             |\n","|    fps                  | 881         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 188416      |\n","| train/                  |             |\n","|    approx_kl            | 0.012932127 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.73       |\n","|    explained_variance   | 0.727       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.257       |\n","|    n_updates            | 910         |\n","|    policy_gradient_loss | -0.0109     |\n","|    std                  | 0.615       |\n","|    value_loss           | 0.842       |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.02e+03     |\n","|    ep_rew_mean          | 4.49         |\n","| time/                   |              |\n","|    fps                  | 850          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 7            |\n","|    total_timesteps      | 190464       |\n","| train/                  |              |\n","|    approx_kl            | 0.0060357945 |\n","|    clip_fraction        | 0.0652       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -3.73        |\n","|    explained_variance   | 0.893        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 11.8         |\n","|    n_updates            | 920          |\n","|    policy_gradient_loss | -0.00524     |\n","|    std                  | 0.615        |\n","|    value_loss           | 30.8         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 984         |\n","|    ep_rew_mean          | 3.87        |\n","| time/                   |             |\n","|    fps                  | 780         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 192512      |\n","| train/                  |             |\n","|    approx_kl            | 0.003415252 |\n","|    clip_fraction        | 0.0253      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.73       |\n","|    explained_variance   | 0.892       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 11.4        |\n","|    n_updates            | 930         |\n","|    policy_gradient_loss | -0.00412    |\n","|    std                  | 0.615       |\n","|    value_loss           | 26.8        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 983         |\n","|    ep_rew_mean          | 4.7         |\n","| time/                   |             |\n","|    fps                  | 759         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 194560      |\n","| train/                  |             |\n","|    approx_kl            | 0.004879821 |\n","|    clip_fraction        | 0.045       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.73       |\n","|    explained_variance   | 0.89        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 20.8        |\n","|    n_updates            | 940         |\n","|    policy_gradient_loss | -0.00707    |\n","|    std                  | 0.615       |\n","|    value_loss           | 64          |\n","-----------------------------------------\n","BipedalWalker - Step: 180001 | Mean Reward: -94.44 ± 0.32\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv1B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 987      |\n","|    ep_rew_mean     | 7.08     |\n","| time/              |          |\n","|    fps             | 980      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 2        |\n","|    total_timesteps | 196608   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 987         |\n","|    ep_rew_mean          | 9.03        |\n","| time/                   |             |\n","|    fps                  | 859         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 198656      |\n","| train/                  |             |\n","|    approx_kl            | 0.007856631 |\n","|    clip_fraction        | 0.0974      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.73       |\n","|    explained_variance   | 0.569       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.827       |\n","|    n_updates            | 960         |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.614       |\n","|    value_loss           | 2.69        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 987         |\n","|    ep_rew_mean          | 10.6        |\n","| time/                   |             |\n","|    fps                  | 802         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 200704      |\n","| train/                  |             |\n","|    approx_kl            | 0.008023263 |\n","|    clip_fraction        | 0.119       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.71       |\n","|    explained_variance   | 0.727       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.306       |\n","|    n_updates            | 970         |\n","|    policy_gradient_loss | -0.00984    |\n","|    std                  | 0.61        |\n","|    value_loss           | 0.822       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 972         |\n","|    ep_rew_mean          | 13          |\n","| time/                   |             |\n","|    fps                  | 743         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 202752      |\n","| train/                  |             |\n","|    approx_kl            | 0.009343011 |\n","|    clip_fraction        | 0.136       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.67       |\n","|    explained_variance   | 0.844       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.202       |\n","|    n_updates            | 980         |\n","|    policy_gradient_loss | -0.0106     |\n","|    std                  | 0.602       |\n","|    value_loss           | 0.551       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 908         |\n","|    ep_rew_mean          | 8.67        |\n","| time/                   |             |\n","|    fps                  | 752         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 204800      |\n","| train/                  |             |\n","|    approx_kl            | 0.004703961 |\n","|    clip_fraction        | 0.0781      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.63       |\n","|    explained_variance   | 0.887       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.06        |\n","|    n_updates            | 990         |\n","|    policy_gradient_loss | -0.00549    |\n","|    std                  | 0.6         |\n","|    value_loss           | 11.5        |\n","-----------------------------------------\n","BipedalWalker - Step: 190001 | Mean Reward: -101.18 ± 1.43\n"]}]},{"cell_type":"markdown","source":["#5. PPO Bipedal\n","env = 4, batch size= 64, lr = 1e-4"],"metadata":{"id":"ldc0MMH_DBve"}},{"cell_type":"code","source":["def train_bipedalwalker5():\n","    # Create environment and model for BipedalWalker\n","    env_bipedal = make_vec_env(\"BipedalWalker-v3\", n_envs=4)\n","    model_b = PPO(\"MlpPolicy\", env_bipedal, verbose=1, tensorboard_log=\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/\",\n","                n_steps=2048, batch_size=64, gae_lambda=0.95, gamma=0.99,\n","                n_epochs=10, ent_coef=0.0, learning_rate=1e-4, clip_range=0.18)\n","\n","    eval_freq = 10000  # Evaluate every 10,000 steps\n","    n_eval_episodes = 10  # Number of episodes per evaluation\n","\n","    for step in range(1, 200001, eval_freq):\n","        # Train the model\n","        model_b.learn(total_timesteps=eval_freq, reset_num_timesteps=False)\n","\n","        # Evaluate the model on BipedalWalker\n","        mean_reward, std_reward = evaluate_model(model_b, env_bipedal, n_eval_episodes)\n","\n","        # Print evaluation results for BipedalWalker\n","        print(f\"BipedalWalker - Step: {step} | Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n","\n","    # Save the trained model for BipedalWalker\n","    model_b.save(\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4\")"],"metadata":{"id":"zxhe-xgcDHIr","executionInfo":{"status":"ok","timestamp":1740513118424,"user_tz":-120,"elapsed":31,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    train_bipedalwalker5()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v4oBM31zDc-h","executionInfo":{"status":"ok","timestamp":1740513639648,"user_tz":-120,"elapsed":521226,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}},"outputId":"1ba3eec5-1d05-439c-b8d4-aa07197ccd56"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 363      |\n","|    ep_rew_mean     | -118     |\n","| time/              |          |\n","|    fps             | 1992     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 8192     |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 504          |\n","|    ep_rew_mean          | -115         |\n","| time/                   |              |\n","|    fps                  | 1189         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 16384        |\n","| train/                  |              |\n","|    approx_kl            | 0.0035891098 |\n","|    clip_fraction        | 0.0283       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.67        |\n","|    explained_variance   | -0.00123     |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 8.5          |\n","|    n_updates            | 10           |\n","|    policy_gradient_loss | -0.00286     |\n","|    std                  | 0.997        |\n","|    value_loss           | 129          |\n","------------------------------------------\n","BipedalWalker - Step: 1 | Mean Reward: -92.91 ± 0.82\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 411      |\n","|    ep_rew_mean     | -112     |\n","| time/              |          |\n","|    fps             | 1576     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 24576    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 408          |\n","|    ep_rew_mean          | -112         |\n","| time/                   |              |\n","|    fps                  | 1167         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 14           |\n","|    total_timesteps      | 32768        |\n","| train/                  |              |\n","|    approx_kl            | 0.0034824111 |\n","|    clip_fraction        | 0.0222       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.64        |\n","|    explained_variance   | 0.269        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 20.1         |\n","|    n_updates            | 30           |\n","|    policy_gradient_loss | -0.00275     |\n","|    std                  | 0.99         |\n","|    value_loss           | 127          |\n","------------------------------------------\n","BipedalWalker - Step: 10001 | Mean Reward: -103.41 ± 0.30\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 358      |\n","|    ep_rew_mean     | -111     |\n","| time/              |          |\n","|    fps             | 1989     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 40960    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 410         |\n","|    ep_rew_mean          | -111        |\n","| time/                   |             |\n","|    fps                  | 1172        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 49152       |\n","| train/                  |             |\n","|    approx_kl            | 0.004239742 |\n","|    clip_fraction        | 0.0223      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.63       |\n","|    explained_variance   | 0.573       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 54          |\n","|    n_updates            | 50          |\n","|    policy_gradient_loss | -0.00266    |\n","|    std                  | 0.989       |\n","|    value_loss           | 129         |\n","-----------------------------------------\n","BipedalWalker - Step: 20001 | Mean Reward: -113.19 ± 0.61\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 384      |\n","|    ep_rew_mean     | -110     |\n","| time/              |          |\n","|    fps             | 1536     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 57344    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 412         |\n","|    ep_rew_mean          | -109        |\n","| time/                   |             |\n","|    fps                  | 1169        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 65536       |\n","| train/                  |             |\n","|    approx_kl            | 0.004864579 |\n","|    clip_fraction        | 0.0373      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.62       |\n","|    explained_variance   | 0.729       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 22.6        |\n","|    n_updates            | 70          |\n","|    policy_gradient_loss | -0.0035     |\n","|    std                  | 0.986       |\n","|    value_loss           | 50.7        |\n","-----------------------------------------\n","BipedalWalker - Step: 30001 | Mean Reward: -95.99 ± 24.03\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 460      |\n","|    ep_rew_mean     | -109     |\n","| time/              |          |\n","|    fps             | 1983     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 73728    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 505          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 1187         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 81920        |\n","| train/                  |              |\n","|    approx_kl            | 0.0026238512 |\n","|    clip_fraction        | 0.0104       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.61        |\n","|    explained_variance   | 0.699        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 9.57         |\n","|    n_updates            | 90           |\n","|    policy_gradient_loss | -0.00209     |\n","|    std                  | 0.981        |\n","|    value_loss           | 48.3         |\n","------------------------------------------\n","BipedalWalker - Step: 40001 | Mean Reward: -40.24 ± 6.38\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 552      |\n","|    ep_rew_mean     | -108     |\n","| time/              |          |\n","|    fps             | 1891     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 90112    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 582         |\n","|    ep_rew_mean          | -108        |\n","| time/                   |             |\n","|    fps                  | 1169        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 98304       |\n","| train/                  |             |\n","|    approx_kl            | 0.004196893 |\n","|    clip_fraction        | 0.0462      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.54       |\n","|    explained_variance   | 0.571       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 29.8        |\n","|    n_updates            | 110         |\n","|    policy_gradient_loss | -0.00335    |\n","|    std                  | 0.965       |\n","|    value_loss           | 28.3        |\n","-----------------------------------------\n","BipedalWalker - Step: 50001 | Mean Reward: -64.63 ± 6.02\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 628      |\n","|    ep_rew_mean     | -107     |\n","| time/              |          |\n","|    fps             | 1614     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 106496   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 690          |\n","|    ep_rew_mean          | -106         |\n","| time/                   |              |\n","|    fps                  | 1175         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 114688       |\n","| train/                  |              |\n","|    approx_kl            | 0.0051359795 |\n","|    clip_fraction        | 0.0372       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.47        |\n","|    explained_variance   | 0.309        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 2.21         |\n","|    n_updates            | 130          |\n","|    policy_gradient_loss | -0.0017      |\n","|    std                  | 0.949        |\n","|    value_loss           | 29.9         |\n","------------------------------------------\n","BipedalWalker - Step: 60001 | Mean Reward: -54.82 ± 10.11\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 751      |\n","|    ep_rew_mean     | -105     |\n","| time/              |          |\n","|    fps             | 1546     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 122880   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 766         |\n","|    ep_rew_mean          | -105        |\n","| time/                   |             |\n","|    fps                  | 1181        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 131072      |\n","| train/                  |             |\n","|    approx_kl            | 0.005470494 |\n","|    clip_fraction        | 0.0621      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.43       |\n","|    explained_variance   | 0.669       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 10.6        |\n","|    n_updates            | 150         |\n","|    policy_gradient_loss | -0.00383    |\n","|    std                  | 0.939       |\n","|    value_loss           | 13.6        |\n","-----------------------------------------\n","BipedalWalker - Step: 70001 | Mean Reward: -76.63 ± 0.65\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 797      |\n","|    ep_rew_mean     | -104     |\n","| time/              |          |\n","|    fps             | 1535     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 139264   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 827         |\n","|    ep_rew_mean          | -103        |\n","| time/                   |             |\n","|    fps                  | 1174        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 147456      |\n","| train/                  |             |\n","|    approx_kl            | 0.006096462 |\n","|    clip_fraction        | 0.0615      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.38       |\n","|    explained_variance   | 0.791       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 10.2        |\n","|    n_updates            | 170         |\n","|    policy_gradient_loss | -0.00292    |\n","|    std                  | 0.927       |\n","|    value_loss           | 7.01        |\n","-----------------------------------------\n","BipedalWalker - Step: 80001 | Mean Reward: -82.79 ± 4.44\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 887      |\n","|    ep_rew_mean     | -102     |\n","| time/              |          |\n","|    fps             | 1507     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 155648   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 902         |\n","|    ep_rew_mean          | -101        |\n","| time/                   |             |\n","|    fps                  | 1168        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 163840      |\n","| train/                  |             |\n","|    approx_kl            | 0.004357354 |\n","|    clip_fraction        | 0.0358      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.3        |\n","|    explained_variance   | 0.862       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0512      |\n","|    n_updates            | 190         |\n","|    policy_gradient_loss | -0.00194    |\n","|    std                  | 0.909       |\n","|    value_loss           | 1.94        |\n","-----------------------------------------\n","BipedalWalker - Step: 90001 | Mean Reward: -86.43 ± 14.04\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 932      |\n","|    ep_rew_mean     | -101     |\n","| time/              |          |\n","|    fps             | 1554     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 172032   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 976          |\n","|    ep_rew_mean          | -100         |\n","| time/                   |              |\n","|    fps                  | 1188         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 180224       |\n","| train/                  |              |\n","|    approx_kl            | 0.0026654769 |\n","|    clip_fraction        | 0.0082       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.27        |\n","|    explained_variance   | 0.917        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 4.1          |\n","|    n_updates            | 210          |\n","|    policy_gradient_loss | -0.00259     |\n","|    std                  | 0.905        |\n","|    value_loss           | 8.85         |\n","------------------------------------------\n","BipedalWalker - Step: 100001 | Mean Reward: -99.12 ± 14.34\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.05e+03 |\n","|    ep_rew_mean     | -100     |\n","| time/              |          |\n","|    fps             | 1676     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 188416   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.06e+03    |\n","|    ep_rew_mean          | -99.5       |\n","| time/                   |             |\n","|    fps                  | 1157        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 196608      |\n","| train/                  |             |\n","|    approx_kl            | 0.003901671 |\n","|    clip_fraction        | 0.0424      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.22       |\n","|    explained_variance   | 0.904       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.488       |\n","|    n_updates            | 230         |\n","|    policy_gradient_loss | -0.00189    |\n","|    std                  | 0.891       |\n","|    value_loss           | 2.89        |\n","-----------------------------------------\n","BipedalWalker - Step: 110001 | Mean Reward: -87.68 ± 21.99\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.11e+03 |\n","|    ep_rew_mean     | -98      |\n","| time/              |          |\n","|    fps             | 2008     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 204800   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.11e+03     |\n","|    ep_rew_mean          | -97          |\n","| time/                   |              |\n","|    fps                  | 1173         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 212992       |\n","| train/                  |              |\n","|    approx_kl            | 0.0055734646 |\n","|    clip_fraction        | 0.066        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.16        |\n","|    explained_variance   | 0.0259       |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0572       |\n","|    n_updates            | 250          |\n","|    policy_gradient_loss | -0.0062      |\n","|    std                  | 0.875        |\n","|    value_loss           | 0.163        |\n","------------------------------------------\n","BipedalWalker - Step: 120001 | Mean Reward: -56.34 ± 1.96\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.12e+03 |\n","|    ep_rew_mean     | -95.4    |\n","| time/              |          |\n","|    fps             | 2074     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 3        |\n","|    total_timesteps | 221184   |\n","---------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.17e+03   |\n","|    ep_rew_mean          | -93.3      |\n","| time/                   |            |\n","|    fps                  | 1211       |\n","|    iterations           | 2          |\n","|    time_elapsed         | 13         |\n","|    total_timesteps      | 229376     |\n","| train/                  |            |\n","|    approx_kl            | 0.00485747 |\n","|    clip_fraction        | 0.0652     |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -5.12      |\n","|    explained_variance   | 0.403      |\n","|    learning_rate        | 0.0001     |\n","|    loss                 | 0.131      |\n","|    n_updates            | 270        |\n","|    policy_gradient_loss | -0.00474   |\n","|    std                  | 0.867      |\n","|    value_loss           | 0.207      |\n","----------------------------------------\n","BipedalWalker - Step: 130001 | Mean Reward: -47.81 ± 3.99\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.2e+03  |\n","|    ep_rew_mean     | -91.4    |\n","| time/              |          |\n","|    fps             | 1895     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 237568   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.23e+03     |\n","|    ep_rew_mean          | -89.2        |\n","| time/                   |              |\n","|    fps                  | 1208         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 245760       |\n","| train/                  |              |\n","|    approx_kl            | 0.0058985176 |\n","|    clip_fraction        | 0.0731       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.02        |\n","|    explained_variance   | 0.667        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.025        |\n","|    n_updates            | 290          |\n","|    policy_gradient_loss | -0.00656     |\n","|    std                  | 0.845        |\n","|    value_loss           | 0.171        |\n","------------------------------------------\n","BipedalWalker - Step: 140001 | Mean Reward: 36.03 ± 52.92\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.24e+03 |\n","|    ep_rew_mean     | -86.9    |\n","| time/              |          |\n","|    fps             | 1798     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 253952   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.26e+03    |\n","|    ep_rew_mean          | -84.7       |\n","| time/                   |             |\n","|    fps                  | 1206        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 262144      |\n","| train/                  |             |\n","|    approx_kl            | 0.005409382 |\n","|    clip_fraction        | 0.0536      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.94       |\n","|    explained_variance   | 0.586       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.126       |\n","|    n_updates            | 310         |\n","|    policy_gradient_loss | -0.00365    |\n","|    std                  | 0.83        |\n","|    value_loss           | 12.3        |\n","-----------------------------------------\n","BipedalWalker - Step: 150001 | Mean Reward: 113.39 ± 4.00\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.29e+03 |\n","|    ep_rew_mean     | -81.7    |\n","| time/              |          |\n","|    fps             | 1753     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 270336   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.32e+03     |\n","|    ep_rew_mean          | -78          |\n","| time/                   |              |\n","|    fps                  | 1192         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 278528       |\n","| train/                  |              |\n","|    approx_kl            | 0.0066178963 |\n","|    clip_fraction        | 0.0862       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.86        |\n","|    explained_variance   | 0.78         |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0699       |\n","|    n_updates            | 330          |\n","|    policy_gradient_loss | -0.00869     |\n","|    std                  | 0.811        |\n","|    value_loss           | 0.177        |\n","------------------------------------------\n","BipedalWalker - Step: 160001 | Mean Reward: 100.63 ± 5.64\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.35e+03 |\n","|    ep_rew_mean     | -74.1    |\n","| time/              |          |\n","|    fps             | 1712     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 286720   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.35e+03    |\n","|    ep_rew_mean          | -70.2       |\n","| time/                   |             |\n","|    fps                  | 1213        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 294912      |\n","| train/                  |             |\n","|    approx_kl            | 0.007088203 |\n","|    clip_fraction        | 0.1         |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.74       |\n","|    explained_variance   | 0.831       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0894      |\n","|    n_updates            | 350         |\n","|    policy_gradient_loss | -0.00861    |\n","|    std                  | 0.787       |\n","|    value_loss           | 0.226       |\n","-----------------------------------------\n","BipedalWalker - Step: 170001 | Mean Reward: 116.56 ± 8.21\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.4e+03  |\n","|    ep_rew_mean     | -65.5    |\n","| time/              |          |\n","|    fps             | 1714     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 303104   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.41e+03    |\n","|    ep_rew_mean          | -60.6       |\n","| time/                   |             |\n","|    fps                  | 1209        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 311296      |\n","| train/                  |             |\n","|    approx_kl            | 0.005964989 |\n","|    clip_fraction        | 0.0753      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.64       |\n","|    explained_variance   | 0.845       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.132       |\n","|    n_updates            | 370         |\n","|    policy_gradient_loss | -0.00626    |\n","|    std                  | 0.768       |\n","|    value_loss           | 0.238       |\n","-----------------------------------------\n","BipedalWalker - Step: 180001 | Mean Reward: 124.36 ± 11.94\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.41e+03 |\n","|    ep_rew_mean     | -56      |\n","| time/              |          |\n","|    fps             | 1627     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 319488   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.43e+03    |\n","|    ep_rew_mean          | -50.3       |\n","| time/                   |             |\n","|    fps                  | 1204        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 327680      |\n","| train/                  |             |\n","|    approx_kl            | 0.007120276 |\n","|    clip_fraction        | 0.0828      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.48       |\n","|    explained_variance   | 0.861       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.123       |\n","|    n_updates            | 390         |\n","|    policy_gradient_loss | -0.00768    |\n","|    std                  | 0.736       |\n","|    value_loss           | 0.274       |\n","-----------------------------------------\n","BipedalWalker - Step: 190001 | Mean Reward: 117.92 ± 9.15\n"]}]},{"cell_type":"markdown","source":["#6. PPO Bipedal\n","env = 4, batch size= 64, lr = 3e-4"],"metadata":{"id":"H_aa_ySRDfcr"}},{"cell_type":"code","source":["def train_bipedalwalker6():\n","    # Create environment and model for BipedalWalker\n","    env_bipedal = make_vec_env(\"BipedalWalker-v3\", n_envs=4)\n","    model_b = PPO(\"MlpPolicy\", env_bipedal, verbose=1, tensorboard_log=\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/\",\n","                n_steps=2048, batch_size=64, gae_lambda=0.95, gamma=0.99,\n","                n_epochs=10, ent_coef=0.0, learning_rate=3e-4, clip_range=0.18)\n","\n","    eval_freq = 10000  # Evaluate every 10,000 steps\n","    n_eval_episodes = 10  # Number of episodes per evaluation\n","\n","    for step in range(1, 200001, eval_freq):\n","        # Train the model\n","        model_b.learn(total_timesteps=eval_freq, reset_num_timesteps=False)\n","\n","        # Evaluate the model on BipedalWalker\n","        mean_reward, std_reward = evaluate_model(model_b, env_bipedal, n_eval_episodes)\n","\n","        # Print evaluation results for BipedalWalker\n","        print(f\"BipedalWalker - Step: {step} | Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n","\n","    # Save the trained model for BipedalWalker\n","    model_b.save(\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4\")"],"metadata":{"id":"qB-c4MN6Dp0x","executionInfo":{"status":"ok","timestamp":1740513639703,"user_tz":-120,"elapsed":20,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    train_bipedalwalker6()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghizIL9nDt7Y","executionInfo":{"status":"ok","timestamp":1740514185808,"user_tz":-120,"elapsed":546099,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}},"outputId":"aa819c3b-b995-4c12-84dd-f631e4d157a0"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 320      |\n","|    ep_rew_mean     | -107     |\n","| time/              |          |\n","|    fps             | 1504     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 8192     |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 485          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 1157         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 14           |\n","|    total_timesteps      | 16384        |\n","| train/                  |              |\n","|    approx_kl            | 0.0039209174 |\n","|    clip_fraction        | 0.0539       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.69        |\n","|    explained_variance   | -0.0217      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 2.41         |\n","|    n_updates            | 10           |\n","|    policy_gradient_loss | -0.00404     |\n","|    std                  | 1.01         |\n","|    value_loss           | 124          |\n","------------------------------------------\n","BipedalWalker - Step: 1 | Mean Reward: -1.31 ± 0.28\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 482      |\n","|    ep_rew_mean     | -109     |\n","| time/              |          |\n","|    fps             | 1479     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 24576    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 496         |\n","|    ep_rew_mean          | -109        |\n","| time/                   |             |\n","|    fps                  | 1161        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 32768       |\n","| train/                  |             |\n","|    approx_kl            | 0.007152944 |\n","|    clip_fraction        | 0.0881      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.69       |\n","|    explained_variance   | 0.512       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 11.4        |\n","|    n_updates            | 30          |\n","|    policy_gradient_loss | -0.00459    |\n","|    std                  | 1           |\n","|    value_loss           | 45.6        |\n","-----------------------------------------\n","BipedalWalker - Step: 10001 | Mean Reward: -7.37 ± 2.75\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 537      |\n","|    ep_rew_mean     | -109     |\n","| time/              |          |\n","|    fps             | 1575     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 40960    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 564          |\n","|    ep_rew_mean          | -109         |\n","| time/                   |              |\n","|    fps                  | 1140         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 14           |\n","|    total_timesteps      | 49152        |\n","| train/                  |              |\n","|    approx_kl            | 0.0077409334 |\n","|    clip_fraction        | 0.122        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.66        |\n","|    explained_variance   | 0.752        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 4.6          |\n","|    n_updates            | 50           |\n","|    policy_gradient_loss | -0.00223     |\n","|    std                  | 0.991        |\n","|    value_loss           | 11.6         |\n","------------------------------------------\n","BipedalWalker - Step: 20001 | Mean Reward: -19.50 ± 2.29\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 602      |\n","|    ep_rew_mean     | -109     |\n","| time/              |          |\n","|    fps             | 1836     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 57344    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 643         |\n","|    ep_rew_mean          | -109        |\n","| time/                   |             |\n","|    fps                  | 1130        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 65536       |\n","| train/                  |             |\n","|    approx_kl            | 0.009099055 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.55       |\n","|    explained_variance   | 0.611       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.63        |\n","|    n_updates            | 70          |\n","|    policy_gradient_loss | -0.00152    |\n","|    std                  | 0.962       |\n","|    value_loss           | 11.7        |\n","-----------------------------------------\n","BipedalWalker - Step: 30001 | Mean Reward: -41.99 ± 2.92\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 672      |\n","|    ep_rew_mean     | -108     |\n","| time/              |          |\n","|    fps             | 1976     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 73728    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 711         |\n","|    ep_rew_mean          | -108        |\n","| time/                   |             |\n","|    fps                  | 1161        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 81920       |\n","| train/                  |             |\n","|    approx_kl            | 0.006115543 |\n","|    clip_fraction        | 0.0735      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.42       |\n","|    explained_variance   | 0.736       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.637       |\n","|    n_updates            | 90          |\n","|    policy_gradient_loss | -0.00148    |\n","|    std                  | 0.933       |\n","|    value_loss           | 2.94        |\n","-----------------------------------------\n","BipedalWalker - Step: 40001 | Mean Reward: -42.85 ± 5.39\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 746      |\n","|    ep_rew_mean     | -107     |\n","| time/              |          |\n","|    fps             | 1980     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 90112    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 807         |\n","|    ep_rew_mean          | -106        |\n","| time/                   |             |\n","|    fps                  | 1168        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 98304       |\n","| train/                  |             |\n","|    approx_kl            | 0.008974623 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.23       |\n","|    explained_variance   | 0.661       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0117      |\n","|    n_updates            | 110         |\n","|    policy_gradient_loss | -0.0109     |\n","|    std                  | 0.89        |\n","|    value_loss           | 0.0696      |\n","-----------------------------------------\n","BipedalWalker - Step: 50001 | Mean Reward: -40.65 ± 2.02\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 868      |\n","|    ep_rew_mean     | -105     |\n","| time/              |          |\n","|    fps             | 2001     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 106496   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 883         |\n","|    ep_rew_mean          | -104        |\n","| time/                   |             |\n","|    fps                  | 1173        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 114688      |\n","| train/                  |             |\n","|    approx_kl            | 0.008119401 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.14       |\n","|    explained_variance   | 0.631       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 114         |\n","|    n_updates            | 130         |\n","|    policy_gradient_loss | -0.00394    |\n","|    std                  | 0.875       |\n","|    value_loss           | 10.9        |\n","-----------------------------------------\n","BipedalWalker - Step: 60001 | Mean Reward: -36.62 ± 7.07\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 882      |\n","|    ep_rew_mean     | -102     |\n","| time/              |          |\n","|    fps             | 2001     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 122880   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 928         |\n","|    ep_rew_mean          | -101        |\n","| time/                   |             |\n","|    fps                  | 1161        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 131072      |\n","| train/                  |             |\n","|    approx_kl            | 0.008586629 |\n","|    clip_fraction        | 0.103       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.07       |\n","|    explained_variance   | 0.72        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.54        |\n","|    n_updates            | 150         |\n","|    policy_gradient_loss | -0.00335    |\n","|    std                  | 0.855       |\n","|    value_loss           | 2.96        |\n","-----------------------------------------\n","BipedalWalker - Step: 70001 | Mean Reward: -39.65 ± 1.53\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 974      |\n","|    ep_rew_mean     | -99      |\n","| time/              |          |\n","|    fps             | 2045     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 139264   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.01e+03    |\n","|    ep_rew_mean          | -97.1       |\n","| time/                   |             |\n","|    fps                  | 1178        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 147456      |\n","| train/                  |             |\n","|    approx_kl            | 0.006713631 |\n","|    clip_fraction        | 0.0966      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.91       |\n","|    explained_variance   | 0.844       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0681      |\n","|    n_updates            | 170         |\n","|    policy_gradient_loss | -0.00428    |\n","|    std                  | 0.823       |\n","|    value_loss           | 2.2         |\n","-----------------------------------------\n","BipedalWalker - Step: 80001 | Mean Reward: -21.87 ± 1.31\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.05e+03 |\n","|    ep_rew_mean     | -94.8    |\n","| time/              |          |\n","|    fps             | 2021     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 155648   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.08e+03    |\n","|    ep_rew_mean          | -92.4       |\n","| time/                   |             |\n","|    fps                  | 1188        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 163840      |\n","| train/                  |             |\n","|    approx_kl            | 0.008874582 |\n","|    clip_fraction        | 0.122       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.79       |\n","|    explained_variance   | 0.928       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.116       |\n","|    n_updates            | 190         |\n","|    policy_gradient_loss | -0.00557    |\n","|    std                  | 0.797       |\n","|    value_loss           | 0.675       |\n","-----------------------------------------\n","BipedalWalker - Step: 90001 | Mean Reward: -48.44 ± 4.27\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.13e+03 |\n","|    ep_rew_mean     | -89.7    |\n","| time/              |          |\n","|    fps             | 2021     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 172032   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.17e+03    |\n","|    ep_rew_mean          | -86.7       |\n","| time/                   |             |\n","|    fps                  | 1192        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 180224      |\n","| train/                  |             |\n","|    approx_kl            | 0.007860841 |\n","|    clip_fraction        | 0.125       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.66       |\n","|    explained_variance   | 0.791       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0522      |\n","|    n_updates            | 210         |\n","|    policy_gradient_loss | -0.00777    |\n","|    std                  | 0.774       |\n","|    value_loss           | 0.156       |\n","-----------------------------------------\n","BipedalWalker - Step: 100001 | Mean Reward: -3.74 ± 11.73\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.23e+03 |\n","|    ep_rew_mean     | -83.3    |\n","| time/              |          |\n","|    fps             | 1945     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 188416   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.26e+03    |\n","|    ep_rew_mean          | -79.6       |\n","| time/                   |             |\n","|    fps                  | 1191        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 196608      |\n","| train/                  |             |\n","|    approx_kl            | 0.009216338 |\n","|    clip_fraction        | 0.114       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.5        |\n","|    explained_variance   | 0.832       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0615      |\n","|    n_updates            | 230         |\n","|    policy_gradient_loss | -0.00643    |\n","|    std                  | 0.742       |\n","|    value_loss           | 0.178       |\n","-----------------------------------------\n","BipedalWalker - Step: 110001 | Mean Reward: 75.35 ± 7.47\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.26e+03 |\n","|    ep_rew_mean     | -75.7    |\n","| time/              |          |\n","|    fps             | 1842     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 204800   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.3e+03     |\n","|    ep_rew_mean          | -71.3       |\n","| time/                   |             |\n","|    fps                  | 1198        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 212992      |\n","| train/                  |             |\n","|    approx_kl            | 0.009828628 |\n","|    clip_fraction        | 0.144       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.35       |\n","|    explained_variance   | 0.832       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0637      |\n","|    n_updates            | 250         |\n","|    policy_gradient_loss | -0.00921    |\n","|    std                  | 0.714       |\n","|    value_loss           | 0.173       |\n","-----------------------------------------\n","BipedalWalker - Step: 120001 | Mean Reward: 104.27 ± 2.77\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.34e+03 |\n","|    ep_rew_mean     | -66.5    |\n","| time/              |          |\n","|    fps             | 1737     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 221184   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.36e+03    |\n","|    ep_rew_mean          | -61.2       |\n","| time/                   |             |\n","|    fps                  | 1206        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 229376      |\n","| train/                  |             |\n","|    approx_kl            | 0.010558058 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.18       |\n","|    explained_variance   | 0.846       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.104       |\n","|    n_updates            | 270         |\n","|    policy_gradient_loss | -0.00785    |\n","|    std                  | 0.686       |\n","|    value_loss           | 0.196       |\n","-----------------------------------------\n","BipedalWalker - Step: 130001 | Mean Reward: 118.72 ± 7.54\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.36e+03 |\n","|    ep_rew_mean     | -55.1    |\n","| time/              |          |\n","|    fps             | 1747     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 237568   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.39e+03    |\n","|    ep_rew_mean          | -48.5       |\n","| time/                   |             |\n","|    fps                  | 1207        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 245760      |\n","| train/                  |             |\n","|    approx_kl            | 0.011491832 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.04       |\n","|    explained_variance   | 0.128       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.531       |\n","|    n_updates            | 290         |\n","|    policy_gradient_loss | -0.00834    |\n","|    std                  | 0.661       |\n","|    value_loss           | 1.66        |\n","-----------------------------------------\n","BipedalWalker - Step: 140001 | Mean Reward: 175.37 ± 3.17\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.4e+03  |\n","|    ep_rew_mean     | -41.3    |\n","| time/              |          |\n","|    fps             | 1732     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 253952   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.4e+03     |\n","|    ep_rew_mean          | -34.2       |\n","| time/                   |             |\n","|    fps                  | 1204        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 262144      |\n","| train/                  |             |\n","|    approx_kl            | 0.010749238 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.88       |\n","|    explained_variance   | 0.884       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.137       |\n","|    n_updates            | 310         |\n","|    policy_gradient_loss | -0.00682    |\n","|    std                  | 0.635       |\n","|    value_loss           | 0.325       |\n","-----------------------------------------\n","BipedalWalker - Step: 150001 | Mean Reward: 228.18 ± 3.81\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.4e+03  |\n","|    ep_rew_mean     | -26.7    |\n","| time/              |          |\n","|    fps             | 1706     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 270336   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.42e+03    |\n","|    ep_rew_mean          | -19         |\n","| time/                   |             |\n","|    fps                  | 1201        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 278528      |\n","| train/                  |             |\n","|    approx_kl            | 0.011239056 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.72       |\n","|    explained_variance   | 0.89        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.143       |\n","|    n_updates            | 330         |\n","|    policy_gradient_loss | -0.0104     |\n","|    std                  | 0.61        |\n","|    value_loss           | 0.308       |\n","-----------------------------------------\n","BipedalWalker - Step: 160001 | Mean Reward: 258.31 ± 7.42\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.45e+03 |\n","|    ep_rew_mean     | -11      |\n","| time/              |          |\n","|    fps             | 1672     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 286720   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.44e+03    |\n","|    ep_rew_mean          | -4.66       |\n","| time/                   |             |\n","|    fps                  | 1193        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 294912      |\n","| train/                  |             |\n","|    approx_kl            | 0.009767801 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.62       |\n","|    explained_variance   | 0.25        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.206       |\n","|    n_updates            | 350         |\n","|    policy_gradient_loss | -0.00392    |\n","|    std                  | 0.598       |\n","|    value_loss           | 2.28        |\n","-----------------------------------------\n","BipedalWalker - Step: 170001 | Mean Reward: -96.57 ± 56.51\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.43e+03 |\n","|    ep_rew_mean     | 2.76     |\n","| time/              |          |\n","|    fps             | 2065     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 3        |\n","|    total_timesteps | 303104   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.45e+03    |\n","|    ep_rew_mean          | 12.8        |\n","| time/                   |             |\n","|    fps                  | 1207        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 311296      |\n","| train/                  |             |\n","|    approx_kl            | 0.009183544 |\n","|    clip_fraction        | 0.14        |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.54       |\n","|    explained_variance   | 0.742       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.09        |\n","|    n_updates            | 370         |\n","|    policy_gradient_loss | -0.00195    |\n","|    std                  | 0.586       |\n","|    value_loss           | 6.96        |\n","-----------------------------------------\n","BipedalWalker - Step: 180001 | Mean Reward: -48.55 ± 102.74\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B64LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.43e+03 |\n","|    ep_rew_mean     | 21.9     |\n","| time/              |          |\n","|    fps             | 2073     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 3        |\n","|    total_timesteps | 319488   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.41e+03    |\n","|    ep_rew_mean          | 29.9        |\n","| time/                   |             |\n","|    fps                  | 1202        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 327680      |\n","| train/                  |             |\n","|    approx_kl            | 0.010794006 |\n","|    clip_fraction        | 0.151       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -3.48       |\n","|    explained_variance   | 0.804       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.06        |\n","|    n_updates            | 390         |\n","|    policy_gradient_loss | -0.00227    |\n","|    std                  | 0.577       |\n","|    value_loss           | 3.08        |\n","-----------------------------------------\n","BipedalWalker - Step: 190001 | Mean Reward: -17.46 ± 117.52\n"]}]},{"cell_type":"markdown","source":["#7. PPO Bipedal\n","env = 4, batch size= 128, lr = 1e-4"],"metadata":{"id":"iRRZ16bgD5gT"}},{"cell_type":"code","source":["def train_bipedalwalker7():\n","    # Create environment and model for BipedalWalker\n","    env_bipedal = make_vec_env(\"BipedalWalker-v3\", n_envs=4)\n","    model_b = PPO(\"MlpPolicy\", env_bipedal, verbose=1, tensorboard_log=\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/\",\n","                n_steps=2048, batch_size=128, gae_lambda=0.95, gamma=0.99,\n","                n_epochs=10, ent_coef=0.0, learning_rate=1e-4, clip_range=0.18)\n","\n","    eval_freq = 10000  # Evaluate every 10,000 steps\n","    n_eval_episodes = 10  # Number of episodes per evaluation\n","\n","    for step in range(1, 200001, eval_freq):\n","        # Train the model\n","        model_b.learn(total_timesteps=eval_freq, reset_num_timesteps=False)\n","\n","        # Evaluate the model on BipedalWalker\n","        mean_reward, std_reward = evaluate_model(model_b, env_bipedal, n_eval_episodes)\n","\n","        # Print evaluation results for BipedalWalker\n","        print(f\"BipedalWalker - Step: {step} | Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n","\n","    # Save the trained model for BipedalWalker\n","    model_b.save(\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4\")"],"metadata":{"id":"Ne40MoChD_Xq","executionInfo":{"status":"ok","timestamp":1740514185824,"user_tz":-120,"elapsed":10,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    train_bipedalwalker7()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OOXMz1OVEKs1","executionInfo":{"status":"ok","timestamp":1740514603377,"user_tz":-120,"elapsed":417529,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}},"outputId":"3785074f-744f-46c0-e090-45eec66e0b34"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 313      |\n","|    ep_rew_mean     | -114     |\n","| time/              |          |\n","|    fps             | 1512     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 8192     |\n","---------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 383        |\n","|    ep_rew_mean          | -111       |\n","| time/                   |            |\n","|    fps                  | 1363       |\n","|    iterations           | 2          |\n","|    time_elapsed         | 12         |\n","|    total_timesteps      | 16384      |\n","| train/                  |            |\n","|    approx_kl            | 0.00284266 |\n","|    clip_fraction        | 0.0154     |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -5.68      |\n","|    explained_variance   | 0.00881    |\n","|    learning_rate        | 0.0001     |\n","|    loss                 | 129        |\n","|    n_updates            | 10         |\n","|    policy_gradient_loss | -0.00281   |\n","|    std                  | 1          |\n","|    value_loss           | 150        |\n","----------------------------------------\n","BipedalWalker - Step: 1 | Mean Reward: -99.58 ± 0.08\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 377      |\n","|    ep_rew_mean     | -110     |\n","| time/              |          |\n","|    fps             | 1565     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 24576    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 409         |\n","|    ep_rew_mean          | -110        |\n","| time/                   |             |\n","|    fps                  | 1384        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 32768       |\n","| train/                  |             |\n","|    approx_kl            | 0.003225476 |\n","|    clip_fraction        | 0.011       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.69       |\n","|    explained_variance   | 0.274       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 19.5        |\n","|    n_updates            | 30          |\n","|    policy_gradient_loss | -0.00228    |\n","|    std                  | 1           |\n","|    value_loss           | 132         |\n","-----------------------------------------\n","BipedalWalker - Step: 10001 | Mean Reward: -101.29 ± 0.21\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 375      |\n","|    ep_rew_mean     | -109     |\n","| time/              |          |\n","|    fps             | 1862     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 40960    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 428         |\n","|    ep_rew_mean          | -109        |\n","| time/                   |             |\n","|    fps                  | 1461        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 49152       |\n","| train/                  |             |\n","|    approx_kl            | 0.002657644 |\n","|    clip_fraction        | 0.0128      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.68       |\n","|    explained_variance   | 0.404       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 70.3        |\n","|    n_updates            | 50          |\n","|    policy_gradient_loss | -0.00257    |\n","|    std                  | 1           |\n","|    value_loss           | 104         |\n","-----------------------------------------\n","BipedalWalker - Step: 20001 | Mean Reward: -103.33 ± 0.04\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 385      |\n","|    ep_rew_mean     | -109     |\n","| time/              |          |\n","|    fps             | 2019     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 57344    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 371          |\n","|    ep_rew_mean          | -110         |\n","| time/                   |              |\n","|    fps                  | 1461         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 65536        |\n","| train/                  |              |\n","|    approx_kl            | 0.0038570538 |\n","|    clip_fraction        | 0.0169       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.66        |\n","|    explained_variance   | 0.428        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 41.4         |\n","|    n_updates            | 70           |\n","|    policy_gradient_loss | -0.00292     |\n","|    std                  | 0.997        |\n","|    value_loss           | 125          |\n","------------------------------------------\n","BipedalWalker - Step: 30001 | Mean Reward: -104.37 ± 0.39\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 358      |\n","|    ep_rew_mean     | -110     |\n","| time/              |          |\n","|    fps             | 2018     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 73728    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 418          |\n","|    ep_rew_mean          | -111         |\n","| time/                   |              |\n","|    fps                  | 1411         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 81920        |\n","| train/                  |              |\n","|    approx_kl            | 0.0062610563 |\n","|    clip_fraction        | 0.0728       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.67        |\n","|    explained_variance   | 0.643        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 4.91         |\n","|    n_updates            | 90           |\n","|    policy_gradient_loss | -0.00386     |\n","|    std                  | 0.997        |\n","|    value_loss           | 44.2         |\n","------------------------------------------\n","BipedalWalker - Step: 40001 | Mean Reward: -105.53 ± 0.13\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 356      |\n","|    ep_rew_mean     | -110     |\n","| time/              |          |\n","|    fps             | 2005     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 90112    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 419          |\n","|    ep_rew_mean          | -110         |\n","| time/                   |              |\n","|    fps                  | 1392         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 98304        |\n","| train/                  |              |\n","|    approx_kl            | 0.0052228454 |\n","|    clip_fraction        | 0.0505       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.67        |\n","|    explained_variance   | 0.7          |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 15.7         |\n","|    n_updates            | 110          |\n","|    policy_gradient_loss | -0.00375     |\n","|    std                  | 0.996        |\n","|    value_loss           | 55.1         |\n","------------------------------------------\n","BipedalWalker - Step: 50001 | Mean Reward: -62.68 ± 2.66\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 450      |\n","|    ep_rew_mean     | -108     |\n","| time/              |          |\n","|    fps             | 1780     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 106496   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 511         |\n","|    ep_rew_mean          | -107        |\n","| time/                   |             |\n","|    fps                  | 1486        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 114688      |\n","| train/                  |             |\n","|    approx_kl            | 0.003487009 |\n","|    clip_fraction        | 0.0396      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.66       |\n","|    explained_variance   | 0.771       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 14.4        |\n","|    n_updates            | 130         |\n","|    policy_gradient_loss | -0.00361    |\n","|    std                  | 0.996       |\n","|    value_loss           | 24.3        |\n","-----------------------------------------\n","BipedalWalker - Step: 60001 | Mean Reward: -74.47 ± 0.03\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 541      |\n","|    ep_rew_mean     | -107     |\n","| time/              |          |\n","|    fps             | 1739     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 122880   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 602          |\n","|    ep_rew_mean          | -106         |\n","| time/                   |              |\n","|    fps                  | 1397         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 131072       |\n","| train/                  |              |\n","|    approx_kl            | 0.0042488286 |\n","|    clip_fraction        | 0.0201       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.6         |\n","|    explained_variance   | 0.481        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 9.68         |\n","|    n_updates            | 150          |\n","|    policy_gradient_loss | -0.00201     |\n","|    std                  | 0.981        |\n","|    value_loss           | 19           |\n","------------------------------------------\n","BipedalWalker - Step: 70001 | Mean Reward: -77.91 ± 0.79\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 663      |\n","|    ep_rew_mean     | -105     |\n","| time/              |          |\n","|    fps             | 2009     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 139264   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 724          |\n","|    ep_rew_mean          | -104         |\n","| time/                   |              |\n","|    fps                  | 1375         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 147456       |\n","| train/                  |              |\n","|    approx_kl            | 0.0028703187 |\n","|    clip_fraction        | 0.0207       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.54        |\n","|    explained_variance   | 0.318        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.214        |\n","|    n_updates            | 170          |\n","|    policy_gradient_loss | -0.00326     |\n","|    std                  | 0.961        |\n","|    value_loss           | 0.226        |\n","------------------------------------------\n","BipedalWalker - Step: 80001 | Mean Reward: -72.21 ± 0.12\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 769      |\n","|    ep_rew_mean     | -103     |\n","| time/              |          |\n","|    fps             | 1849     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 155648   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 814         |\n","|    ep_rew_mean          | -101        |\n","| time/                   |             |\n","|    fps                  | 1503        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 163840      |\n","| train/                  |             |\n","|    approx_kl            | 0.003062672 |\n","|    clip_fraction        | 0.0111      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.51       |\n","|    explained_variance   | 0.772       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.164       |\n","|    n_updates            | 190         |\n","|    policy_gradient_loss | -0.00237    |\n","|    std                  | 0.957       |\n","|    value_loss           | 3.42        |\n","-----------------------------------------\n","BipedalWalker - Step: 90001 | Mean Reward: -63.57 ± 0.08\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 874      |\n","|    ep_rew_mean     | -100     |\n","| time/              |          |\n","|    fps             | 1668     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 172032   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 889          |\n","|    ep_rew_mean          | -99.4        |\n","| time/                   |              |\n","|    fps                  | 1398         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 180224       |\n","| train/                  |              |\n","|    approx_kl            | 0.0034922808 |\n","|    clip_fraction        | 0.0282       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.45        |\n","|    explained_variance   | 0.32         |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 2.37         |\n","|    n_updates            | 210          |\n","|    policy_gradient_loss | -0.00393     |\n","|    std                  | 0.945        |\n","|    value_loss           | 11.8         |\n","------------------------------------------\n","BipedalWalker - Step: 100001 | Mean Reward: -72.67 ± 0.11\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 920      |\n","|    ep_rew_mean     | -98.1    |\n","| time/              |          |\n","|    fps             | 2013     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 188416   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 965          |\n","|    ep_rew_mean          | -97          |\n","| time/                   |              |\n","|    fps                  | 1388         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 196608       |\n","| train/                  |              |\n","|    approx_kl            | 0.0035002804 |\n","|    clip_fraction        | 0.0176       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.41        |\n","|    explained_variance   | 0.579        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 17.6         |\n","|    n_updates            | 230          |\n","|    policy_gradient_loss | -0.00171     |\n","|    std                  | 0.935        |\n","|    value_loss           | 16.3         |\n","------------------------------------------\n","BipedalWalker - Step: 110001 | Mean Reward: -57.77 ± 0.80\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 995      |\n","|    ep_rew_mean     | -96      |\n","| time/              |          |\n","|    fps             | 1933     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 204800   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.06e+03     |\n","|    ep_rew_mean          | -94.7        |\n","| time/                   |              |\n","|    fps                  | 1504         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 10           |\n","|    total_timesteps      | 212992       |\n","| train/                  |              |\n","|    approx_kl            | 0.0038663398 |\n","|    clip_fraction        | 0.0213       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.38        |\n","|    explained_variance   | 0.645        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.395        |\n","|    n_updates            | 250          |\n","|    policy_gradient_loss | -0.0026      |\n","|    std                  | 0.928        |\n","|    value_loss           | 17.1         |\n","------------------------------------------\n","BipedalWalker - Step: 120001 | Mean Reward: -60.66 ± 0.50\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.12e+03 |\n","|    ep_rew_mean     | -93.6    |\n","| time/              |          |\n","|    fps             | 1610     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 221184   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.18e+03    |\n","|    ep_rew_mean          | -92.4       |\n","| time/                   |             |\n","|    fps                  | 1379        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 229376      |\n","| train/                  |             |\n","|    approx_kl            | 0.005419238 |\n","|    clip_fraction        | 0.0553      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.31       |\n","|    explained_variance   | 0.367       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0896      |\n","|    n_updates            | 270         |\n","|    policy_gradient_loss | -0.00538    |\n","|    std                  | 0.91        |\n","|    value_loss           | 0.139       |\n","-----------------------------------------\n","BipedalWalker - Step: 130001 | Mean Reward: -51.73 ± 6.00\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.19e+03 |\n","|    ep_rew_mean     | -91.4    |\n","| time/              |          |\n","|    fps             | 2004     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 237568   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.19e+03     |\n","|    ep_rew_mean          | -91          |\n","| time/                   |              |\n","|    fps                  | 1403         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 245760       |\n","| train/                  |              |\n","|    approx_kl            | 0.0049235784 |\n","|    clip_fraction        | 0.0445       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.22        |\n","|    explained_variance   | 0.255        |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0585       |\n","|    n_updates            | 290          |\n","|    policy_gradient_loss | -0.00412     |\n","|    std                  | 0.889        |\n","|    value_loss           | 0.105        |\n","------------------------------------------\n","BipedalWalker - Step: 140001 | Mean Reward: -49.06 ± 1.61\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.24e+03 |\n","|    ep_rew_mean     | -89.5    |\n","| time/              |          |\n","|    fps             | 1955     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 253952   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.3e+03     |\n","|    ep_rew_mean          | -87.3       |\n","| time/                   |             |\n","|    fps                  | 1481        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 262144      |\n","| train/                  |             |\n","|    approx_kl            | 0.006601543 |\n","|    clip_fraction        | 0.0824      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.14       |\n","|    explained_variance   | 0.449       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.022       |\n","|    n_updates            | 310         |\n","|    policy_gradient_loss | -0.00802    |\n","|    std                  | 0.87        |\n","|    value_loss           | 0.123       |\n","-----------------------------------------\n","BipedalWalker - Step: 150001 | Mean Reward: -40.89 ± 0.72\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.31e+03 |\n","|    ep_rew_mean     | -86.2    |\n","| time/              |          |\n","|    fps             | 1538     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 270336   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.36e+03     |\n","|    ep_rew_mean          | -84.3        |\n","| time/                   |              |\n","|    fps                  | 1388         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 278528       |\n","| train/                  |              |\n","|    approx_kl            | 0.0047182497 |\n","|    clip_fraction        | 0.0577       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.09        |\n","|    explained_variance   | 0.46         |\n","|    learning_rate        | 0.0001       |\n","|    loss                 | 0.0807       |\n","|    n_updates            | 330          |\n","|    policy_gradient_loss | -0.00607     |\n","|    std                  | 0.861        |\n","|    value_loss           | 0.149        |\n","------------------------------------------\n","BipedalWalker - Step: 160001 | Mean Reward: -41.58 ± 7.33\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.36e+03 |\n","|    ep_rew_mean     | -82.6    |\n","| time/              |          |\n","|    fps             | 2058     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 3        |\n","|    total_timesteps | 286720   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.4e+03     |\n","|    ep_rew_mean          | -80         |\n","| time/                   |             |\n","|    fps                  | 1410        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 294912      |\n","| train/                  |             |\n","|    approx_kl            | 0.005578568 |\n","|    clip_fraction        | 0.0739      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.03       |\n","|    explained_variance   | 0.631       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.0911      |\n","|    n_updates            | 350         |\n","|    policy_gradient_loss | -0.00713    |\n","|    std                  | 0.85        |\n","|    value_loss           | 0.163       |\n","-----------------------------------------\n","BipedalWalker - Step: 170001 | Mean Reward: -0.30 ± 57.35\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.4e+03  |\n","|    ep_rew_mean     | -77.3    |\n","| time/              |          |\n","|    fps             | 2045     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 303104   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.37e+03    |\n","|    ep_rew_mean          | -75.5       |\n","| time/                   |             |\n","|    fps                  | 1513        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 311296      |\n","| train/                  |             |\n","|    approx_kl            | 0.006313815 |\n","|    clip_fraction        | 0.0826      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.98       |\n","|    explained_variance   | 0.76        |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.104       |\n","|    n_updates            | 370         |\n","|    policy_gradient_loss | -0.00735    |\n","|    std                  | 0.837       |\n","|    value_loss           | 0.261       |\n","-----------------------------------------\n","BipedalWalker - Step: 180001 | Mean Reward: 67.66 ± 15.70\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR1e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.38e+03 |\n","|    ep_rew_mean     | -73.2    |\n","| time/              |          |\n","|    fps             | 1571     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 319488   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.42e+03    |\n","|    ep_rew_mean          | -69.9       |\n","| time/                   |             |\n","|    fps                  | 1425        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 327680      |\n","| train/                  |             |\n","|    approx_kl            | 0.004942797 |\n","|    clip_fraction        | 0.0518      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.95       |\n","|    explained_variance   | 0.853       |\n","|    learning_rate        | 0.0001      |\n","|    loss                 | 0.246       |\n","|    n_updates            | 390         |\n","|    policy_gradient_loss | -0.00419    |\n","|    std                  | 0.834       |\n","|    value_loss           | 2.56        |\n","-----------------------------------------\n","BipedalWalker - Step: 190001 | Mean Reward: 42.20 ± 74.13\n"]}]},{"cell_type":"markdown","source":["#8. PPO Bipedal\n","env = 4, batch size= 128, lr = 3e-4"],"metadata":{"id":"HeXNV96zEPoe"}},{"cell_type":"code","source":["def train_bipedalwalker8():\n","    # Create environment and model for BipedalWalker\n","    env_bipedal = make_vec_env(\"BipedalWalker-v3\", n_envs=4)\n","    model_b = PPO(\"MlpPolicy\", env_bipedal, verbose=1, tensorboard_log=\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/\",\n","                n_steps=2048, batch_size=128, gae_lambda=0.95, gamma=0.99,\n","                n_epochs=10, ent_coef=0.0, learning_rate=3e-4, clip_range=0.18)\n","\n","    eval_freq = 10000  # Evaluate every 10,000 steps\n","    n_eval_episodes = 10  # Number of episodes per evaluation\n","\n","    for step in range(1, 200001, eval_freq):\n","        # Train the model\n","        model_b.learn(total_timesteps=eval_freq, reset_num_timesteps=False)\n","\n","        # Evaluate the model on BipedalWalker\n","        mean_reward, std_reward = evaluate_model(model_b, env_bipedal, n_eval_episodes)\n","\n","        # Print evaluation results for BipedalWalker\n","        print(f\"BipedalWalker - Step: {step} | Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n","\n","    # Save the trained model for BipedalWalker\n","    model_b.save(\"/content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4\")"],"metadata":{"id":"zzmzDXapEc_P","executionInfo":{"status":"ok","timestamp":1740514603415,"user_tz":-120,"elapsed":12,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    train_bipedalwalker8()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nXL7fR2EnL5","executionInfo":{"status":"ok","timestamp":1740515013106,"user_tz":-120,"elapsed":409686,"user":{"displayName":"Rana Choker","userId":"13341093079406228038"}},"outputId":"0e729e22-bf8c-4662-d333-bc8d9f78f634"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 184      |\n","|    ep_rew_mean     | -110     |\n","| time/              |          |\n","|    fps             | 1958     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 8192     |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 386          |\n","|    ep_rew_mean          | -110         |\n","| time/                   |              |\n","|    fps                  | 1369         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 16384        |\n","| train/                  |              |\n","|    approx_kl            | 0.0049158335 |\n","|    clip_fraction        | 0.0499       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.67        |\n","|    explained_variance   | 0.00269      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 146          |\n","|    n_updates            | 10           |\n","|    policy_gradient_loss | -0.00473     |\n","|    std                  | 0.999        |\n","|    value_loss           | 221          |\n","------------------------------------------\n","BipedalWalker - Step: 1 | Mean Reward: -84.46 ± 27.42\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 410      |\n","|    ep_rew_mean     | -108     |\n","| time/              |          |\n","|    fps             | 1968     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 24576    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 471          |\n","|    ep_rew_mean          | -109         |\n","| time/                   |              |\n","|    fps                  | 1393         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 32768        |\n","| train/                  |              |\n","|    approx_kl            | 0.0047535896 |\n","|    clip_fraction        | 0.0476       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.72        |\n","|    explained_variance   | 0.42         |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 31.6         |\n","|    n_updates            | 30           |\n","|    policy_gradient_loss | -0.00382     |\n","|    std                  | 1.01         |\n","|    value_loss           | 61.4         |\n","------------------------------------------\n","BipedalWalker - Step: 10001 | Mean Reward: -104.69 ± 0.05\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 488      |\n","|    ep_rew_mean     | -108     |\n","| time/              |          |\n","|    fps             | 1637     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 40960    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 480          |\n","|    ep_rew_mean          | -108         |\n","| time/                   |              |\n","|    fps                  | 1366         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 49152        |\n","| train/                  |              |\n","|    approx_kl            | 0.0035109008 |\n","|    clip_fraction        | 0.0204       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.65        |\n","|    explained_variance   | 0.542        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 6.42         |\n","|    n_updates            | 50           |\n","|    policy_gradient_loss | -0.00337     |\n","|    std                  | 0.994        |\n","|    value_loss           | 34           |\n","------------------------------------------\n","BipedalWalker - Step: 20001 | Mean Reward: -113.54 ± 0.14\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 504      |\n","|    ep_rew_mean     | -108     |\n","| time/              |          |\n","|    fps             | 1532     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 57344    |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 566          |\n","|    ep_rew_mean          | -107         |\n","| time/                   |              |\n","|    fps                  | 1382         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 65536        |\n","| train/                  |              |\n","|    approx_kl            | 0.0031378057 |\n","|    clip_fraction        | 0.0133       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.65        |\n","|    explained_variance   | 0.727        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 13.5         |\n","|    n_updates            | 70           |\n","|    policy_gradient_loss | -0.00402     |\n","|    std                  | 0.995        |\n","|    value_loss           | 32.7         |\n","------------------------------------------\n","BipedalWalker - Step: 30001 | Mean Reward: -103.37 ± 0.45\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 519      |\n","|    ep_rew_mean     | -106     |\n","| time/              |          |\n","|    fps             | 1513     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 73728    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 564         |\n","|    ep_rew_mean          | -106        |\n","| time/                   |             |\n","|    fps                  | 1353        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 81920       |\n","| train/                  |             |\n","|    approx_kl            | 0.005086594 |\n","|    clip_fraction        | 0.0486      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.63       |\n","|    explained_variance   | 0.853       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.02        |\n","|    n_updates            | 90          |\n","|    policy_gradient_loss | -0.00417    |\n","|    std                  | 0.989       |\n","|    value_loss           | 28.7        |\n","-----------------------------------------\n","BipedalWalker - Step: 40001 | Mean Reward: -54.59 ± 0.21\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 594      |\n","|    ep_rew_mean     | -106     |\n","| time/              |          |\n","|    fps             | 1743     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 90112    |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 592         |\n","|    ep_rew_mean          | -106        |\n","| time/                   |             |\n","|    fps                  | 1368        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 98304       |\n","| train/                  |             |\n","|    approx_kl            | 0.008709585 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.6        |\n","|    explained_variance   | 0.771       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.08        |\n","|    n_updates            | 110         |\n","|    policy_gradient_loss | -0.00353    |\n","|    std                  | 0.977       |\n","|    value_loss           | 6.14        |\n","-----------------------------------------\n","BipedalWalker - Step: 50001 | Mean Reward: -64.83 ± 0.12\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 623      |\n","|    ep_rew_mean     | -106     |\n","| time/              |          |\n","|    fps             | 1981     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 106496   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 654          |\n","|    ep_rew_mean          | -105         |\n","| time/                   |              |\n","|    fps                  | 1378         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 114688       |\n","| train/                  |              |\n","|    approx_kl            | 0.0071578417 |\n","|    clip_fraction        | 0.093        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.53        |\n","|    explained_variance   | 0.642        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 2.38         |\n","|    n_updates            | 130          |\n","|    policy_gradient_loss | -0.00557     |\n","|    std                  | 0.959        |\n","|    value_loss           | 11.3         |\n","------------------------------------------\n","BipedalWalker - Step: 60001 | Mean Reward: -71.74 ± 0.04\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 668      |\n","|    ep_rew_mean     | -105     |\n","| time/              |          |\n","|    fps             | 1984     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 122880   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 699          |\n","|    ep_rew_mean          | -105         |\n","| time/                   |              |\n","|    fps                  | 1368         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 131072       |\n","| train/                  |              |\n","|    approx_kl            | 0.0045992937 |\n","|    clip_fraction        | 0.0689       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.43        |\n","|    explained_variance   | 0.903        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.03         |\n","|    n_updates            | 150          |\n","|    policy_gradient_loss | -0.00255     |\n","|    std                  | 0.94         |\n","|    value_loss           | 3.1          |\n","------------------------------------------\n","BipedalWalker - Step: 70001 | Mean Reward: -75.67 ± 0.46\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 759      |\n","|    ep_rew_mean     | -104     |\n","| time/              |          |\n","|    fps             | 1778     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 139264   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 804          |\n","|    ep_rew_mean          | -103         |\n","| time/                   |              |\n","|    fps                  | 1467         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 147456       |\n","| train/                  |              |\n","|    approx_kl            | 0.0076202927 |\n","|    clip_fraction        | 0.0796       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.34        |\n","|    explained_variance   | 0.946        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.561        |\n","|    n_updates            | 170          |\n","|    policy_gradient_loss | -0.00503     |\n","|    std                  | 0.916        |\n","|    value_loss           | 2.12         |\n","------------------------------------------\n","BipedalWalker - Step: 80001 | Mean Reward: -81.74 ± 12.79\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 881      |\n","|    ep_rew_mean     | -101     |\n","| time/              |          |\n","|    fps             | 1576     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 155648   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 896         |\n","|    ep_rew_mean          | -100        |\n","| time/                   |             |\n","|    fps                  | 1403        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 163840      |\n","| train/                  |             |\n","|    approx_kl            | 0.008744966 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.24       |\n","|    explained_variance   | 0.483       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.202       |\n","|    n_updates            | 190         |\n","|    policy_gradient_loss | -0.00756    |\n","|    std                  | 0.891       |\n","|    value_loss           | 0.515       |\n","-----------------------------------------\n","BipedalWalker - Step: 90001 | Mean Reward: -116.62 ± 0.18\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 929      |\n","|    ep_rew_mean     | -98.6    |\n","| time/              |          |\n","|    fps             | 1535     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 172032   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 944          |\n","|    ep_rew_mean          | -97.4        |\n","| time/                   |              |\n","|    fps                  | 1397         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 180224       |\n","| train/                  |              |\n","|    approx_kl            | 0.0047276267 |\n","|    clip_fraction        | 0.0363       |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -5.18        |\n","|    explained_variance   | 0.798        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.52         |\n","|    n_updates            | 210          |\n","|    policy_gradient_loss | -0.00294     |\n","|    std                  | 0.885        |\n","|    value_loss           | 4.05         |\n","------------------------------------------\n","BipedalWalker - Step: 100001 | Mean Reward: -72.16 ± 5.45\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1e+03    |\n","|    ep_rew_mean     | -95.8    |\n","| time/              |          |\n","|    fps             | 2086     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 3        |\n","|    total_timesteps | 188416   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.07e+03    |\n","|    ep_rew_mean          | -94.2       |\n","| time/                   |             |\n","|    fps                  | 1449        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 196608      |\n","| train/                  |             |\n","|    approx_kl            | 0.007442042 |\n","|    clip_fraction        | 0.107       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.16       |\n","|    explained_variance   | 0.371       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.176       |\n","|    n_updates            | 230         |\n","|    policy_gradient_loss | -0.00418    |\n","|    std                  | 0.879       |\n","|    value_loss           | 2.3         |\n","-----------------------------------------\n","BipedalWalker - Step: 110001 | Mean Reward: -45.41 ± 0.63\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.13e+03 |\n","|    ep_rew_mean     | -92.3    |\n","| time/              |          |\n","|    fps             | 2089     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 3        |\n","|    total_timesteps | 204800   |\n","---------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.14e+03   |\n","|    ep_rew_mean          | -90.5      |\n","| time/                   |            |\n","|    fps                  | 1505       |\n","|    iterations           | 2          |\n","|    time_elapsed         | 10         |\n","|    total_timesteps      | 212992     |\n","| train/                  |            |\n","|    approx_kl            | 0.00660032 |\n","|    clip_fraction        | 0.0644     |\n","|    clip_range           | 0.18       |\n","|    entropy_loss         | -5.1       |\n","|    explained_variance   | 0.531      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.379      |\n","|    n_updates            | 250        |\n","|    policy_gradient_loss | -0.00389   |\n","|    std                  | 0.865      |\n","|    value_loss           | 2.23       |\n","----------------------------------------\n","BipedalWalker - Step: 120001 | Mean Reward: -43.18 ± 6.25\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.19e+03 |\n","|    ep_rew_mean     | -88.3    |\n","| time/              |          |\n","|    fps             | 1565     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 221184   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.22e+03    |\n","|    ep_rew_mean          | -86         |\n","| time/                   |             |\n","|    fps                  | 1406        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 229376      |\n","| train/                  |             |\n","|    approx_kl            | 0.008954275 |\n","|    clip_fraction        | 0.113       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -5.03       |\n","|    explained_variance   | -0.385      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.4         |\n","|    n_updates            | 270         |\n","|    policy_gradient_loss | -0.00368    |\n","|    std                  | 0.847       |\n","|    value_loss           | 0.691       |\n","-----------------------------------------\n","BipedalWalker - Step: 130001 | Mean Reward: -43.63 ± 2.94\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.25e+03 |\n","|    ep_rew_mean     | -83.3    |\n","| time/              |          |\n","|    fps             | 2065     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 3        |\n","|    total_timesteps | 237568   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.26e+03    |\n","|    ep_rew_mean          | -80.5       |\n","| time/                   |             |\n","|    fps                  | 1418        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 245760      |\n","| train/                  |             |\n","|    approx_kl            | 0.009017974 |\n","|    clip_fraction        | 0.12        |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.94       |\n","|    explained_variance   | 0.866       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0912      |\n","|    n_updates            | 290         |\n","|    policy_gradient_loss | -0.00556    |\n","|    std                  | 0.83        |\n","|    value_loss           | 0.804       |\n","-----------------------------------------\n","BipedalWalker - Step: 140001 | Mean Reward: 5.55 ± 27.96\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.26e+03 |\n","|    ep_rew_mean     | -77.2    |\n","| time/              |          |\n","|    fps             | 2087     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 3        |\n","|    total_timesteps | 253952   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.31e+03     |\n","|    ep_rew_mean          | -73.2        |\n","| time/                   |              |\n","|    fps                  | 1539         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 10           |\n","|    total_timesteps      | 262144       |\n","| train/                  |              |\n","|    approx_kl            | 0.0073906155 |\n","|    clip_fraction        | 0.127        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.83        |\n","|    explained_variance   | 0.663        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.105        |\n","|    n_updates            | 310          |\n","|    policy_gradient_loss | -0.00908     |\n","|    std                  | 0.806        |\n","|    value_loss           | 0.224        |\n","------------------------------------------\n","BipedalWalker - Step: 150001 | Mean Reward: 72.82 ± 12.38\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.34e+03 |\n","|    ep_rew_mean     | -68.6    |\n","| time/              |          |\n","|    fps             | 1568     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 270336   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.33e+03     |\n","|    ep_rew_mean          | -64.5        |\n","| time/                   |              |\n","|    fps                  | 1427         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 278528       |\n","| train/                  |              |\n","|    approx_kl            | 0.0069895913 |\n","|    clip_fraction        | 0.112        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.74        |\n","|    explained_variance   | 0.84         |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.108        |\n","|    n_updates            | 330          |\n","|    policy_gradient_loss | -0.00548     |\n","|    std                  | 0.788        |\n","|    value_loss           | 0.553        |\n","------------------------------------------\n","BipedalWalker - Step: 160001 | Mean Reward: -95.79 ± 0.10\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.33e+03 |\n","|    ep_rew_mean     | -60      |\n","| time/              |          |\n","|    fps             | 1632     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 286720   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.37e+03    |\n","|    ep_rew_mean          | -54.7       |\n","| time/                   |             |\n","|    fps                  | 1427        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 294912      |\n","| train/                  |             |\n","|    approx_kl            | 0.005900978 |\n","|    clip_fraction        | 0.0808      |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.68       |\n","|    explained_variance   | 0.879       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.898       |\n","|    n_updates            | 350         |\n","|    policy_gradient_loss | -0.00398    |\n","|    std                  | 0.778       |\n","|    value_loss           | 1.62        |\n","-----------------------------------------\n","BipedalWalker - Step: 170001 | Mean Reward: 83.85 ± 59.80\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.39e+03 |\n","|    ep_rew_mean     | -47.2    |\n","| time/              |          |\n","|    fps             | 2025     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 303104   |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.39e+03    |\n","|    ep_rew_mean          | -41.8       |\n","| time/                   |             |\n","|    fps                  | 1402        |\n","|    iterations           | 2           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 311296      |\n","| train/                  |             |\n","|    approx_kl            | 0.008544548 |\n","|    clip_fraction        | 0.119       |\n","|    clip_range           | 0.18        |\n","|    entropy_loss         | -4.57       |\n","|    explained_variance   | 0.813       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.143       |\n","|    n_updates            | 370         |\n","|    policy_gradient_loss | -0.00591    |\n","|    std                  | 0.755       |\n","|    value_loss           | 0.298       |\n","-----------------------------------------\n","BipedalWalker - Step: 180001 | Mean Reward: -93.98 ± 0.77\n","Logging to /content/drive/MyDrive/RL_models/ppo_bipedalwalkerEnv4B128LR3e-4/PPO_0\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 1.39e+03 |\n","|    ep_rew_mean     | -35.8    |\n","| time/              |          |\n","|    fps             | 1835     |\n","|    iterations      | 1        |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 319488   |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 1.39e+03     |\n","|    ep_rew_mean          | -29.7        |\n","| time/                   |              |\n","|    fps                  | 1411         |\n","|    iterations           | 2            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 327680       |\n","| train/                  |              |\n","|    approx_kl            | 0.0062100366 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.18         |\n","|    entropy_loss         | -4.49        |\n","|    explained_variance   | 0.514        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.588        |\n","|    n_updates            | 390          |\n","|    policy_gradient_loss | -0.0049      |\n","|    std                  | 0.742        |\n","|    value_loss           | 2.01         |\n","------------------------------------------\n","BipedalWalker - Step: 190001 | Mean Reward: 57.36 ± 93.17\n"]}]}]}